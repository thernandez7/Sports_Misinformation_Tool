{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72517dfa-1720-49e6-b55a-f15cdf181498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git add .\n",
    "# !git commit -m \"ran hueristic features on euro dataset and updates RSS collection\"\n",
    "# !git pull\n",
    "# !git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83bc5e79-db2d-4764-81e0-c13cc4ca8fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8446acb8-a93e-418e-a0ec-3ef05f7389b8",
   "metadata": {},
   "source": [
    "## Sports Misinformation Classification Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc105918-5fb3-432a-b47f-f3717be38100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82e525a-1cf9-47be-8298-238a25ce2667",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Connect to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fde6c4b-1920-4bb2-846f-61455f869aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database successfully!\n"
     ]
    }
   ],
   "source": [
    "username = 'root'  \n",
    "password = 'password'\n",
    "host = 'localhost' \n",
    "database = 'sports_news_db'\n",
    "\n",
    "connection = mysql.connector.connect(\n",
    "    host=host,\n",
    "    user=username,\n",
    "    password=password,\n",
    "    database=database\n",
    ")\n",
    "\n",
    "if connection.is_connected():\n",
    "    print(\"Connected to the database successfully!\")\n",
    "\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{username}:{password}@{host}/{database}\", echo=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70ed146f-9fe7-4370-a3d2-898811f3e183",
   "metadata": {},
   "source": [
    "#### Use this format to insert into the database\n",
    "\n",
    "INSERT INTO articles (team_or_player, source, publication_date, content, trust_score, classification, link)\n",
    "VALUES\n",
    "('New York Yankees, Los Angeles Lakers', 'ESPN', '2024-10-27', 'Yankees article content example.', 85.00, 'real', 'https://example.com/article1'),\n",
    "('Los Angeles Lakers', 'Twitter', '2024-10-27', 'Lakers article content example.', 60.00, 'fake', 'https://example.com/article2');\n",
    "\n",
    "\n",
    "#### Table fields \n",
    "Table Name: articles\n",
    "\n",
    "Fields: \n",
    "\n",
    "     id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "     \n",
    "     team_or_player VARCHAR(500), (This will be the article title that we can query- usually includes teams or names in it)\n",
    "     \n",
    "     source VARCHAR(200),\n",
    "     \n",
    "     publication_date DATE,\n",
    "     \n",
    "     content TEXT,\n",
    "     \n",
    "     trust_score DECIMAL(5, 2), \n",
    "     \n",
    "     classification ENUM('credible', 'uncredible', 'unknown') DEFAULT 'unknown',\n",
    "\n",
    "     link VARCHAR(255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359168cc-f336-4972-b580-38575b45a13e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Simulate Tool Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9687e8d1-8c66-4df1-80cf-4bcf6f33805c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the team or player's name:  x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No articles found for x.\n"
     ]
    }
   ],
   "source": [
    "team_or_player = input(\"Enter the team or player's name: \")\n",
    "\n",
    "query = f\"SELECT * FROM articles WHERE team_or_player LIKE '%{team_or_player}%'\" #search for entered name/team in the title \n",
    "df_result = pd.read_sql(query, con=engine, params={'team_or_player': team_or_player})\n",
    "\n",
    "#get results\n",
    "if not df_result.empty:\n",
    "    print(f\"Articles related to {team_or_player}:\")\n",
    "    display(df_result)\n",
    "else:\n",
    "    print(f\"No articles found for {team_or_player}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cee353-47af-48c3-a0eb-cf19ddcbc995",
   "metadata": {},
   "source": [
    "### Get Article Entries from RSS Feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06ea0136-a9b1-4fac-94f6-1949deec9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reddit API - collected 1876 posts\n",
    "# import csv\n",
    "# import praw\n",
    "# from datetime import datetime\n",
    "\n",
    "\n",
    "# reddit = praw.Reddit(\n",
    "#     client_id='w-kwRyPigyjYeG9DOiDc8g', \n",
    "#     client_secret='ZeDsvNH2YlpVH7F9wEWPkt5wkjLzqA',  \n",
    "#     user_agent='sports_misinfo_script'  \n",
    "# )\n",
    "\n",
    "# subreddit = reddit.subreddit('sports+fantasyfootball') #2 subreddits \n",
    "\n",
    "# recent_posts = []\n",
    "# for post in subreddit.new(limit= 5000):\n",
    "#     created_date = datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "#     recent_posts.append({\n",
    "#         'title': post.title,\n",
    "#         'score': post.score,\n",
    "#         'url': post.url,\n",
    "#         'id': post.id,\n",
    "#         'author': str(post.author),\n",
    "#         'text': post.selftext,\n",
    "#         'created_date': created_date,\n",
    "#         'num_comments': post.num_comments,\n",
    "#         'subreddit': post.subreddit.display_name,\n",
    "\n",
    "#     })\n",
    "\n",
    "# print(f\"Fetched {len(recent_posts)} posts\")\n",
    "\n",
    "# for i, post in enumerate(recent_posts[:5]):\n",
    "#     print(f\"{i+1}. Title: {post['title']} | Score: {post['score']} | URL: {post['url']}\")\n",
    "\n",
    "\n",
    "# #-------------------------------\n",
    "# #Save the data to a csv file\n",
    "# fieldnames = ['title', 'score', 'url', 'id', 'author', 'text', 'created_date', 'num_comments', 'subreddit']\n",
    "\n",
    "# with open('recent_sports_reddit_posts.csv', mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "#     writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "#     writer.writeheader()\n",
    "\n",
    "#     for post in recent_posts:\n",
    "#         writer.writerow(post)\n",
    "\n",
    "# print(\"Data successfully saved to 'recent_sports_reddit_posts.csv'\")\n",
    "# print(\"Data saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "334d2970-7ee0-47e1-80d9-e01ae15fa4a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape content from https://www.espn.com/college-football/story/_/id/42447741/oregon-remains-unanimous-no-1-ap-top-25-notre-dame-6: 403 Client Error: Forbidden for url: https://www.espn.com/college-football/story/_/id/42447741/oregon-remains-unanimous-no-1-ap-top-25-notre-dame-6\n",
      "Failed to scrape content from https://www.espn.com/nba/story/_/id/42447996/hornets-lamelo-ball-fined-100k-using-anti-gay-term: 403 Client Error: Forbidden for url: https://www.espn.com/nba/story/_/id/42447996/hornets-lamelo-ball-fined-100k-using-anti-gay-term\n",
      "Failed to scrape content from https://www.espn.com/college-football/story/_/id/42447431/sources-top-2025-qb-julian-lewis-decommits-usc: 403 Client Error: Forbidden for url: https://www.espn.com/college-football/story/_/id/42447431/sources-top-2025-qb-julian-lewis-decommits-usc\n",
      "Failed to scrape content from https://www.espn.com/nba/story/_/id/42449490/celtics-celebrate-nba-title-white-house-thursday: 403 Client Error: Forbidden for url: https://www.espn.com/nba/story/_/id/42449490/celtics-celebrate-nba-title-white-house-thursday\n",
      "Failed to scrape content from https://www.espn.com/golf/story/_/id/42448704/new-father-rafael-campos-emotional-first-pga-tour-win: 403 Client Error: Forbidden for url: https://www.espn.com/golf/story/_/id/42448704/new-father-rafael-campos-emotional-first-pga-tour-win\n",
      "Failed to scrape content from https://www.espn.com/nba/story/_/id/42445812/reports-nba-air-espn-abc-next-season: 403 Client Error: Forbidden for url: https://www.espn.com/nba/story/_/id/42445812/reports-nba-air-espn-abc-next-season\n",
      "Failed to scrape content from https://www.espn.com/college-football/story/_/id/42445530/temple-fires-coach-stan-drayton-day-ot-victory: 403 Client Error: Forbidden for url: https://www.espn.com/college-football/story/_/id/42445530/temple-fires-coach-stan-drayton-day-ot-victory\n",
      "Failed to scrape content from https://www.espn.com/college-sports/recruiting/football/story/_/id/42448614/anthony-rogers-no-97-recruit-2025-decommits-alabama: 403 Client Error: Forbidden for url: https://www.espn.com/college-sports/recruiting/football/story/_/id/42448614/anthony-rogers-no-97-recruit-2025-decommits-alabama\n",
      "Failed to scrape content from https://www.espn.com/nfl/story/_/id/42365446/can-chiefs-go-undefeated-record-winning-ugly-roster-flaws-super-bowl-2024-mahomes: 403 Client Error: Forbidden for url: https://www.espn.com/nfl/story/_/id/42365446/can-chiefs-go-undefeated-record-winning-ugly-roster-flaws-super-bowl-2024-mahomes\n",
      "Failed to scrape content from https://www.espn.com/nfl/story/_/id/42368149/buffalo-bills-player-led-playcalling-building-team-confidence-undefeated-kansas-city-chiefs: 403 Client Error: Forbidden for url: https://www.espn.com/nfl/story/_/id/42368149/buffalo-bills-player-led-playcalling-building-team-confidence-undefeated-kansas-city-chiefs\n",
      "Failed to scrape content from https://www.espn.com/college-football/story/_/page/gamedayfinal111624/college-football-week-12-highlights-top-plays-games-takeaways-2024: 403 Client Error: Forbidden for url: https://www.espn.com/college-football/story/_/page/gamedayfinal111624/college-football-week-12-highlights-top-plays-games-takeaways-2024\n",
      "Failed to scrape content from https://www.espn.com/college-football/story/_/id/42393651/2024-college-football-power-rankings-week-12-top-25-teams: 403 Client Error: Forbidden for url: https://www.espn.com/college-football/story/_/id/42393651/2024-college-football-power-rankings-week-12-top-25-teams\n",
      "Failed to scrape content from https://www.espn.com/wnba/story/_/id/42370750/wnba-mock-draft-2025-paige-bueckers-draft-lottery-los-angeles-sparks-dallas-wings-washington-mystics-chicago-sky: 403 Client Error: Forbidden for url: https://www.espn.com/wnba/story/_/id/42370750/wnba-mock-draft-2025-paige-bueckers-draft-lottery-los-angeles-sparks-dallas-wings-washington-mystics-chicago-sky\n",
      "Failed to scrape content from https://www.espn.com/nhl/story/_/id/42390646/nhl-power-rankings-2024-2025-best-teams-standings-fantasy-hockey: 403 Client Error: Forbidden for url: https://www.espn.com/nhl/story/_/id/42390646/nhl-power-rankings-2024-2025-best-teams-standings-fantasy-hockey\n",
      "Failed to scrape content from https://www.espn.com/mens-college-basketball/story/_/id/42391361/judging-5-early-season-mens-college-basketball-overreactions-kentucky-flagg-arkansas: 403 Client Error: Forbidden for url: https://www.espn.com/mens-college-basketball/story/_/id/42391361/judging-5-early-season-mens-college-basketball-overreactions-kentucky-flagg-arkansas\n",
      "Failed to scrape content from https://www.espn.com/mens-college-basketball/story/_/id/42371393/mens-college-basketball-power-rankings-2024-25-kansas-gonzaga-auburn-kentucky: 403 Client Error: Forbidden for url: https://www.espn.com/mens-college-basketball/story/_/id/42371393/mens-college-basketball-power-rankings-2024-25-kansas-gonzaga-auburn-kentucky\n",
      "Failed to scrape content from https://www.espn.com/boxing/story/_/id/42419370/after-mike-tyson-jake-paul-surprised-see-boxing-spectacle-again: 403 Client Error: Forbidden for url: https://www.espn.com/boxing/story/_/id/42419370/after-mike-tyson-jake-paul-surprised-see-boxing-spectacle-again\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42428167/washington-spirit-gotham-fc-shootout-nwsl-semifinal-championship: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42428167/washington-spirit-gotham-fc-shootout-nwsl-semifinal-championship\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42430199/usmnt-tim-ream-tim-weah-back-copa-america-red-card: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42430199/usmnt-tim-ream-tim-weah-back-copa-america-red-card\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42423254/cristiano-ronaldo-makes-retirement-hint-record-portugal-win: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42423254/cristiano-ronaldo-makes-retirement-hint-record-portugal-win\n",
      "Failed to scrape content from https://www.espn.com/soccer/report/_/gameId/699011: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/report/_/gameId/699011\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42433979/uswnt-lily-yohannes-tough-week-netherlands-ajax-coach: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42433979/uswnt-lily-yohannes-tough-week-netherlands-ajax-coach\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42425280/bayern-munich-extend-mala-grohs-contract-cancer-diagnosis: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42425280/bayern-munich-extend-mala-grohs-contract-cancer-diagnosis\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42431109/hungary-assistant-coach-adam-szalai-netherlands-nations-league: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42431109/hungary-assistant-coach-adam-szalai-netherlands-nations-league\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42427478/man-united-confirm-ruben-amorim-coaching-staff: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42427478/man-united-confirm-ruben-amorim-coaching-staff\n",
      "Failed to scrape content from https://www.espn.com/college-football/story/_/id/42430837/college-football-trolls-week-12: 403 Client Error: Forbidden for url: https://www.espn.com/college-football/story/_/id/42430837/college-football-trolls-week-12\n",
      "Failed to scrape content from https://www.espn.com/college-football/story/_/id/42433526/colorado-buffaloes-peggy-coppom-happy-birthday-100-birthday: 403 Client Error: Forbidden for url: https://www.espn.com/college-football/story/_/id/42433526/colorado-buffaloes-peggy-coppom-happy-birthday-100-birthday\n",
      "Failed to scrape content from https://www.espn.com/espn/betting/story/_/id/42376299/2024-nfl-season-betting-odds-daniel-dopp-liz-loza-football-props-pop-week-11: 403 Client Error: Forbidden for url: https://www.espn.com/espn/betting/story/_/id/42376299/2024-nfl-season-betting-odds-daniel-dopp-liz-loza-football-props-pop-week-11\n",
      "Failed to scrape content from https://www.espn.com/espn/betting/story/_/id/42394173/2024-nba-betting-futures-rookie-year-jared-mccain-zaccharie-risacher-zach-edey: 403 Client Error: Forbidden for url: https://www.espn.com/espn/betting/story/_/id/42394173/2024-nba-betting-futures-rookie-year-jared-mccain-zaccharie-risacher-zach-edey\n",
      "Failed to scrape content from https://www.espn.com/boxing/story/_/id/42422294/katie-taylor-amanda-serrano-jake-paul-mike-tyson-boxing: 403 Client Error: Forbidden for url: https://www.espn.com/boxing/story/_/id/42422294/katie-taylor-amanda-serrano-jake-paul-mike-tyson-boxing\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42388116/premier-league-manchester-city-chelsea-arsenal-tottenham-manchester-united-january-transfer: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42388116/premier-league-manchester-city-chelsea-arsenal-tottenham-manchester-united-january-transfer\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42431717/soccer-transfer-rumors-news-lafc-circle-griezmann-eyes-atleti-exit: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42431717/soccer-transfer-rumors-news-lafc-circle-griezmann-eyes-atleti-exit\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#add urls to this list to parse\n",
    "url_list = [\n",
    "        #MBFC High Credibility\n",
    "        \"https://www.espn.com/espn/rss/news\", #ESPN top headlines\n",
    "        \"https://deadspin.com/rss/\", #Deadspin \n",
    "        \"https://feeds.abcnews.com/abcnews/sportsheadlines\" #ABC news- espn sports\n",
    "        \"https://rss.nytimes.com/services/xml/rss/nyt/Sports.xml\", #NYT\n",
    "        \"https://www.nytimes.com/athletic/rss/news/\"#The Athletic- acquired by NYT\n",
    "        \"https://www.mlb.com/feeds/news/rss.xml\", #MLB news \n",
    "        \"https://www.reutersagency.com/feed/?best-topics=sports&post_type=best\", # Reuters\n",
    "        \"https://sports.yahoo.com/rss/\", #Yahoo News\n",
    "        \"https://www.cbssports.com/rss/headlines/\" #CBS sports general headlines \n",
    "\n",
    "        #MBFC questionable sources or medium credibility\n",
    "        \"https://notthebee.com/feed\", #not the bee\n",
    "        \"https://uproxx.com/sports/feed/\", #uproxx   \n",
    "        \"https://www.vibe.com/c/news/sports/feed/\", #The vibe - medium cred \n",
    "        \"https://moxie.foxnews.com/google-publisher/sports.xml\", #fox news \n",
    "        \"https://nypost.com/sports/feed/\" #NY post - medium cred\n",
    "\n",
    "        #NO MBFC RATING\n",
    "        \"https://www.sportscollectorsdaily.com/feed/\", #Sports Collectors Daily \n",
    "        \"https://news.sportslogos.net/feed/\", #SportsLogos.Net\n",
    "        \"https://www.sportingnews.com/us/rss\", #sportingnews.com\n",
    "        \"https://www.sportskeeda.com/feed\",#sportskeeda.com\n",
    "        \"https://sportsweez.com/feed/\", #sportsweez\n",
    "        \"https://sportsbrackets.net/feed/\", #sportsbracket\n",
    "        \"https://21sports.com/feed/\", # 21 sports\n",
    "        \"https://www.essentiallysports.com/feed/\", #essentiallysports\n",
    "        \"https://boxingnewsonline.net/feed/\", #boxing news\n",
    "        \"https://www.thecoldwire.com/feed/\", #the cold wire\n",
    "        \"https://sportsdark.com/feed/\", #sports dark\n",
    "]\n",
    "    \n",
    "entries = [] #list of dictionaries\n",
    "\n",
    "#--------------DEFINE FUNCTION TO SCRAPE-------------------\n",
    "def scrape_article_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        #extract content from common tags\n",
    "        article_body = (\n",
    "            soup.find(\"article\") or\n",
    "            soup.find(\"div\", {\"class\": \"post-content\"}) or\n",
    "            soup.find(\"div\", class_=\"article-body\") or\n",
    "            soup.find(\"div\", class_=\"article-content\") or\n",
    "            soup.find(\"section\", class_=\"article-section\") or\n",
    "            soup.find(\"div\", class_=\"main-content\") or\n",
    "            soup.find(\"div\", class_=\"content-body\")\n",
    "        )\n",
    "        \n",
    "        if article_body:\n",
    "            paragraphs = article_body.find_all(\"p\")\n",
    "        else:\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "\n",
    "        #join all paragraphs into a single string\n",
    "        article_content = \" \".join(p.get_text() for p in paragraphs)\n",
    "        return article_content.strip() if article_content else \"No content found\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape content from {url}: {e}\")\n",
    "        return \"Failed to fetch content\"\n",
    "\n",
    "\n",
    "#run the function to collect feeds and scrape\n",
    "for url in url_list:\n",
    "    feed = feedparser.parse(url)\n",
    "    #feed_title= feed.feed.title\n",
    "    feed_title = getattr(feed.feed, \"title\", None)\n",
    "\n",
    "    \n",
    "    for entry in feed.entries:\n",
    "        entry_title = getattr(entry, \"title\", None)\n",
    "        entry_link = getattr(entry, \"link\", None)\n",
    "        entry_published_date = getattr(entry, \"published\", None)\n",
    "        entry_summary = getattr(entry, \"summary\", None)\n",
    "        entry_content = scrape_article_content(entry_link) #scrape\n",
    "        \n",
    "        entries.append({\n",
    "            \"feed_title\": feed_title,\n",
    "            \"entry_title\": entry_title,\n",
    "            \"entry_link\": entry_link,\n",
    "            \"entry_published_date\": entry_published_date,\n",
    "            \"entry_summary\": entry_summary,\n",
    "            \"entry_content\": entry_content,\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(entries)\n",
    "#print(df)\n",
    "\n",
    "df.to_csv('RSS_sports_feeds_11-17.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b17ce3-4167-4ee2-b75b-0e30f8d21697",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Attempt to Scrape Links without RSS Feeds- not very effective "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f9a3d96-4201-4150-8ecb-b14e133f886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "\n",
    "# uncredible_urls = [\n",
    "#     \"https://www.tellerreport.com/sports\",\n",
    "#     \"https://www.newsbreak.com/mountain-view-ca-sports\",\n",
    "#     \"https://newsrnd.com/sports\",\n",
    "#     \"https://baltimorecitywire.com/stories/tag/53-sports\"\n",
    "# ]\n",
    "\n",
    "# # Function to scrape article details\n",
    "# def scrape_article(url):\n",
    "#     try:\n",
    "#         response = requests.get(url, timeout=10)\n",
    "#         response.raise_for_status()\n",
    "#         soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "#         # Extract the article title\n",
    "#         title = soup.find(\"h1\").get_text() if soup.find(\"h1\") else soup.title.get_text()\n",
    "\n",
    "#         # Extract the publication date (common in <time> or meta tags)\n",
    "#         date = soup.find(\"time\")\n",
    "#         if date:\n",
    "#             publication_date = date.get(\"datetime\") or date.get_text()\n",
    "#         else:\n",
    "#             date_meta = soup.find(\"meta\", {\"name\": \"article:published_time\"})\n",
    "#             publication_date = date_meta[\"content\"] if date_meta else \"No date found\"\n",
    "\n",
    "#         # Extract the article content\n",
    "#         content_container = (\n",
    "#             soup.find(\"article\") or\n",
    "#             soup.find(\"div\", class_=[\"post-content\", \"article-body\", \"article-content\", \"content-body\"])\n",
    "#         )\n",
    "#         if content_container:\n",
    "#             paragraphs = content_container.find_all(\"p\")\n",
    "#         else:\n",
    "#             paragraphs = soup.find_all(\"p\")\n",
    "\n",
    "#         content = \"\\n\".join(p.get_text() for p in paragraphs)\n",
    "\n",
    "#         return {\n",
    "#             \"Title\": title.strip(),\n",
    "#             \"Publication Date\": publication_date.strip(),\n",
    "#             \"Content\": content.strip()[:500] + \"...\",\n",
    "#             \"Link\": url\n",
    "#         }\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to scrape {url}: {e}\")\n",
    "#         return None\n",
    "\n",
    "# articles = []\n",
    "# for url in uncredible_urls:\n",
    "#     print(f\"Scraping: {url}\")\n",
    "#     article_details = scrape_article(url)\n",
    "#     if article_details:\n",
    "#         articles.append(article_details)\n",
    "\n",
    "# for article in articles:\n",
    "#     print(\"\\n--- Article ---\")\n",
    "#     print(f\"Title: {article['Title']}\")\n",
    "#     print(f\"Date: {article['Publication Date']}\")\n",
    "#     print(f\"Content Preview: {article['Content']}\")\n",
    "#     print(f\"Link: {article['Link']}\\n\")\n",
    "\n",
    "# df = pd.DataFrame(articles)\n",
    "# output_file = \"scraped_uncredible_articles.csv\"\n",
    "# df.to_csv(output_file, index=False)\n",
    "# print(f\"\\nResults saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f53448-164e-46d8-a16d-02c3dbaa7d0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Format DF to match DB table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b6b1163a-ce9f-47f0-9c15-d7e5cceaf6e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tizia\\Anaconda4\\Lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "df['publication_date'] = df['entry_published_date'].apply(lambda x: parser.parse(x).date()) #parse different date formats to date object format\n",
    "dbdf = pd.read_csv('RSS_sports_feeds.csv')\n",
    "dbdf['team_or_player'] = df['entry_title']\n",
    "dbdf['source'] = df['feed_title']\n",
    "dbdf['publication_date'] = df['publication_date'] \n",
    "dbdf['content'] = df['entry_summary']\n",
    "dbdf['trust_score'] = 0.00  #default\n",
    "dbdf['classification'] = 'unknown' #default\n",
    "dbdf['link'] = df['entry_link']\n",
    "\n",
    "#make a new df in the format of the DB table for easy inserting\n",
    "sports_DB_df = dbdf[['team_or_player', 'source', 'publication_date', 'content', 'trust_score', 'classification', 'link']]\n",
    "#save to a new CSV \n",
    "sports_DB_df.to_csv('formatted_sports_posts_for_DB.csv', index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e65e92b-0473-40ed-9279-7d6f643fe6f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Examine 2 Lists of True and False Headlines to Discover Features for Heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60f4d51a-90aa-4cf2-8699-ad47dcbd5e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Lists\n",
    "false_headlines=[\n",
    "    \"BREAKING: Fox Sports Cancels ALL NFL Broadcasts \\\"Until Players Respect The Flag\\\"\",\n",
    "    \"Rugby Safer Than American Football \\\"For Health Reasons\\\": Biden\",\n",
    "    \"First woman to medal in six straight Olympics. Media and sponsors ignore her because she is outspoken pro-2A.\",\n",
    "    \"Novak Djokovic becomes the first professional athlete in history to be banned from a major sporting competition for not taking drugs\",\n",
    "    \"BREAKING: ESPN has fired Shannon Sharpe, per @ESPNNBA\",\n",
    "    \"The Minnesota Vikings have denounced Tim Walz: \\\"We don’t suppᴏrt his values.\\\"\",\n",
    "    \"'BREAKING: WNBA referees disqualify two players under league’s new \\\"no anthem kneeling\\\" rule\",\n",
    "    \"Nike announces termination of contract with Brittney Griner after \\\"strong backlash\\\" from online community: \\\"We need more athletes like Riley Gaines and less woke Brittney Griner!\\\"\",\n",
    "    \"KNEELING: After the University of Texas, all students who knelt during the national anthem were rounded up and REMOVED FROM SCHOLARSHIPS\",\n",
    "    \"Travis Kelce kneels during national anthem fined $10 million and thrown out of the game.\",\n",
    "    \"The NFL will now use facial recognition at every stadium to verify the identity of everyone at the game.\",\n",
    "    \"Mike Tyson says he’s willing to box Olympic DUDE with all proceeds to go to a battered women’s charity.\",\n",
    "    \"After winning silver, Yusef stood emotionless on the Olympic podium and declared, \\\"Sharon, if you’re watching this, I want my dog back.\\\"\",\n",
    "    \"Miami Dolphins QB Tua Tagovailoa will be sitting front row tonight in Doral for the Trump speech\",\n",
    "    \"BREAKING: The WNBA organizers have officially announced an investigation into the referees in all of Caitlin Clark's games for ignoring all dirty actions by her opponents against her\",\n",
    "    \"Chiefs' Coach Andy Reid \\\"fires 3 top players for anthem kneeling.\\\"\",\n",
    "    \"BREAKING: Caitlin Clark Rejects $400 Million Deal From Nike, \\\"Not With That Kaepernick Clown,\\\"\",\n",
    "    \"At Euro 2020, UEFA (European Football Association) ordered all team captains to wear \\\"OneLove\\\" bands. The band was used as a symbol of LGBTQ. But, Portugal captain Cristiano Ronaldo was the only European captain who did not wear the band.\",\n",
    "    \"Golden State Warriors refuse to visit White House after winning NBA title: reports\",\n",
    "    \"Taylor Swift faces a 10-game NFL ban following controversial political involvement - fans in uproar!\"\n",
    "]\n",
    "true_headlines=[\n",
    "    \"Did that really happen? Barbados v Grenada and a deliberate own goal\",\n",
    "    \"Breakdancing Will Not Be Busting A Move In 2028 Olympics\",\n",
    "    \"Breaking Will Not Be in The 2028 Los Angeles Olympics—What’s Next?\",\n",
    "    \"Braves Superstar Sets Atlanta-Era Record in 1st Inning\",\n",
    "    \"Only 20 schools in the Football Bowl Subdivision have athletic departments with revenue exceeding expenses\",\n",
    "    \"LATEST: Mitchell Stadium named America’s Best High School Football Stadium\",\n",
    "    \"Homes of Patrick Mahomes, Travis Kelce burglarized last month\",\n",
    "    \"Gregg Popovich recovering from mild stroke, no timeline for return set\",\n",
    "    \"Patrick Queen: I wasn’t wanted back with Ravens, it was definitely kind of upsetting\",\n",
    "    \"Odell Beckham Jr. boasts about taking 2021 Rams salary in Bitcoin National Football League Odell Beckham Jr. boasts about taking 2021 Rams salary in Bitcoin\",\n",
    "    \"Ecuador international soccer player Marco Angulo dies aged 22 following car crash\",\n",
    "    \"Shohei Ohtani Baseball Worth $4 Million Lands in Globally-Recognized Skyscraper\",\n",
    "    \"Kobe Bryant is the only person to have won both an Olympic medal and an Oscar\",\n",
    "    \"Bucks fan predicted Milwaukee-Phoenix NBA Finals all the way back in 2016\",\n",
    "    \"Chiefs kicker Butker congratulates women graduates and says most are more excited about motherhood\",\n",
    "    \"Kansas City Chiefs player faces backlash for graduation speech criticizing working women, calling Pride a \\\"deadly sin\\\"\", \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d48e1a6-9c29-4a2b-a38a-d0f53e5ed469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textstat\n",
    "from transformers import pipeline\n",
    "import string\n",
    "from textstat import flesch_reading_ease\n",
    "import textstat\n",
    "import re\n",
    "\n",
    "\n",
    "#functions for each feature we are considering\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def char_count(text):\n",
    "    return len(text)\n",
    "\n",
    "def avg_word_length(text):\n",
    "    words = text.split()\n",
    "    return sum(len(word) for word in words) / len(words) if words else 0\n",
    "\n",
    "def all_capitalized_word_count(text):\n",
    "    return sum(1 for word in text.split() if word.isupper())\n",
    "\n",
    "def punctuation_count(text):\n",
    "    return sum(1 for char in text if char in string.punctuation)\n",
    "\n",
    "def exclamation_count(text):\n",
    "    return text.count(\"!\")\n",
    "\n",
    "def question_mark_count(text):\n",
    "    return text.count(\"?\")\n",
    "\n",
    "def readability_score(text): #high score = easier readability\n",
    "    return flesch_reading_ease(text)\n",
    "\n",
    "def smog_index(text): #readability based on the number of complex words (3+ syllables)\n",
    "    return textstat.smog_index(text)\n",
    "\n",
    "def count_quotation_pairs(text): \n",
    "    double_quotes = re.findall(r'\"[^\"]+\"', text) #Finds all sections of text in double quotes\n",
    "    return len(double_quotes)\n",
    "\n",
    "def variety_of_vocabularity(text):\n",
    "    words = text.split()\n",
    "    unique_words = set(words)\n",
    "    return len(unique_words) / len(words) if words else 0\n",
    "\n",
    "\n",
    "sensational_words = [\"shocking\", \"amazing\", \"unbelievable\", \"you won’t believe\", \"incredible\", \"stunning\",\\\n",
    "                    \"astounding\", \"breathtaking\", \"outstanding\", \"thrilling\", \"must see\", \"terrible\", \"awful\",\\\n",
    "                     \"fantastic\", \"horrible\", \"remarkable\"]\n",
    "def count_sensational_words(text):\n",
    "    text = text.lower()\n",
    "    return sum(1 for word in sensational_words if word in text)\n",
    "\n",
    "\n",
    "absolute_words = [\"always\", \"never\", \"clearly\", \"obviously\", \"definitely\", \"everyone\", \"nobody\", \"all\",\\\n",
    "              \"none\", \"blatantly\", \"undoubtedly\", \"must\", \"should\"]\n",
    "def count_absolute_words(text):\n",
    "    words = text.lower().split()\n",
    "    return sum(1 for word in words if word in absolute_words)\n",
    "\n",
    "\n",
    "factuality_words = [\"proven\", \"irrefutable\", \"unarguable\", \"unquestionably\", \"certainly\", \"definitely\",\\\n",
    "                    \"undeniable\"]\n",
    "def count_factuality_words(text):\n",
    "    words = text.lower().split()\n",
    "    return sum(1 for word in words if word in factuality_words)\n",
    "\n",
    "\n",
    "subjective_words = [\"believe\", \"think\", \"feel\", \"prefer\", \"seems\", \"wish\"]\n",
    "def subjectivity_score(text):\n",
    "    words = text.lower().split()\n",
    "    subjective_count = sum(1 for word in words if word in subjective_words)\n",
    "    return subjective_count / len(words) if len(words) > 0 else 0\n",
    "\n",
    "objective_words = [\"reported\", \"measured\", \"confirmed\", \"analyzed\", \"observed\", \"recorded\",\\\n",
    "                   \"found\", \"documented\", \"verified\", \"tested\", \"studied\", \"calculated\", \"noted\",\\\n",
    "                   \"established\", \"evidence\", \"fact\", \"data\", \"statistics\", \"demonstrated\", \"shown\",\\\n",
    "                   \"results\", \"result\", \"evidence-based\", \"peer-reviewed\", \"sampled\", \"quantified\",\\\n",
    "                   \"evaluated\", \"experimented\", \"investigated\"]\n",
    "def objective_score(text):\n",
    "    words = text.lower().split()\n",
    "    objective_count = sum(1 for word in words if word in objective_words)\n",
    "    return objective_count / len(words) if len(words) > 0 else 0\n",
    "\n",
    "\n",
    "def count_numerics(text):\n",
    "    numerical_count = len(re.findall(r'\\d+', text)) \n",
    "    return numerical_count\n",
    "\n",
    "\n",
    "pipe_finnews = pipeline(\"text-classification\", model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
    "def sentiment(text):\n",
    "    result = pipe_finnews(text)\n",
    "    return result[0][\"label\"] \n",
    "    \n",
    "    \n",
    "#add in source credibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321abb20-959e-47bc-a85e-169de141ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply these feature functions to the headlines to compare\n",
    "features = [\n",
    "    word_count, char_count, avg_word_length, all_capitalized_word_count,\n",
    "    punctuation_count, exclamation_count, question_mark_count, readability_score,\n",
    "    smog_index, count_quotation_pairs, variety_of_vocabularity,\n",
    "    count_sensational_words, count_absolute_words, count_factuality_words,\n",
    "    subjectivity_score, objective_score, count_numerics, sentiment\n",
    "]\n",
    "\n",
    "def compute_features(headlines):\n",
    "    results = []\n",
    "    for headline in headlines:\n",
    "        data = {\n",
    "            \"headline\": headline,\n",
    "            \"word_count\": word_count(headline),\n",
    "            \"char_count\": char_count(headline),\n",
    "            \"avg_word_length\": avg_word_length(headline),\n",
    "            \"all_capitalized_word_count\": all_capitalized_word_count(headline),\n",
    "            \"punctuation_count\": punctuation_count(headline),\n",
    "            \"exclamation_count\": exclamation_count(headline),\n",
    "            \"question_mark_count\": question_mark_count(headline),\n",
    "            \"readability_score\": readability_score(headline),\n",
    "            \"smog_index\": smog_index(headline),\n",
    "            \"quotation_count\": count_quotation_pairs(headline),\n",
    "            \"vocab_variety\": variety_of_vocabularity(headline),\n",
    "            \"sensational_words_count\": count_sensational_words(headline),\n",
    "            \"absolute_words_count\": count_absolute_words(headline),\n",
    "            \"factuality_words_count\": count_factuality_words(headline),\n",
    "            \"subjectivity_score\": subjectivity_score(headline),\n",
    "            \"objective_score\": objective_score(headline),\n",
    "            \"count_numerics\": count_numerics(headline),\n",
    "            \"sentiment\": sentiment(headline)\n",
    "        }\n",
    "        results.append(data)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Compute features for true and false headlines\n",
    "true_features_df = compute_features(true_headlines)\n",
    "false_features_df = compute_features(false_headlines)\n",
    "\n",
    "true_features_df[\"label\"] = \"True\"\n",
    "false_features_df[\"label\"] = \"False\"\n",
    "\n",
    "combined_df = pd.concat([true_features_df, false_features_df], ignore_index=True)\n",
    "combined_df.to_csv(\"headlines_initial_hueristic_exploration.csv\")\n",
    "print(\"Data saved to 'headlines_initial_hueristic_exploration.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7a47f9-7733-4440-a768-65fd38167322",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Run the Heuristics on the European Soccer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b923503-bb70-46cc-ad0c-a13fb1dbdd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_euro_df = pd.read_csv('real_european.csv')\n",
    "fake_euro_df = pd.read_csv('fake_european.csv')\n",
    "\n",
    "real_euro_df[\"label\"] = \"True\"\n",
    "fake_euro_df[\"label\"] = \"False\"\n",
    "\n",
    "real_tweets= real_euro_df[\"tweet\"].dropna()\n",
    "fake_tweets= fake_euro_df[\"tweet\"].dropna()\n",
    "\n",
    "# Compute features for true and false tweets\n",
    "euro_true_features_df = compute_features(real_tweets)\n",
    "euro_false_features_df = compute_features(fake_tweets)\n",
    "\n",
    "euro_true_features_df[\"label\"] = \"True\"\n",
    "euro_false_features_df[\"label\"] = \"False\"\n",
    "\n",
    "euro_combined_df = pd.concat([euro_true_features_df, euro_false_features_df], ignore_index=True)\n",
    "euro_combined_df.to_csv(\"Euro_tweets_initial_hueristic_exploration.csv\")\n",
    "print(\"Data saved to 'Euro_tweets_initial_hueristic_exploration.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c282d88e-3a8a-4283-af31-f011ec789135",
   "metadata": {},
   "source": [
    "### Define labeling heuristic and trust score to update sports_DB_df with ground truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ff2a5014-fff8-4491-a937-cadbd2b697ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ace_tools in c:\\users\\tizia\\anaconda4\\lib\\site-packages (0.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
