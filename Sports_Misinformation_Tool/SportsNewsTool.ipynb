{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "72517dfa-1720-49e6-b55a-f15cdf181498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: in the working copy of 'Sports_Misinformation_Tool/SportsNewsTool.ipynb', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Sports_Misinformation_Tool/testing_labeling_on_euro_fake.csv', LF will be replaced by CRLF the next time Git touches it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 0ddd89c] labeling on headlines to evaluate labeling\n",
      " 5 files changed, 264 insertions(+), 13 deletions(-)\n",
      " create mode 100644 Sports_Misinformation_Tool/.ipynb_checkpoints/testing_labeling_on_headlines_fake-checkpoint.csv\n",
      " create mode 100644 Sports_Misinformation_Tool/.ipynb_checkpoints/testing_labeling_on_headlines_true-checkpoint.csv\n",
      " create mode 100644 Sports_Misinformation_Tool/testing_labeling_on_headlines_fake.csv\n",
      " create mode 100644 Sports_Misinformation_Tool/testing_labeling_on_headlines_true.csv\n",
      "Already up to date.\n",
      "branch 'main' set up to track 'origin/main'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To github.com:thernandez7/Sports_Misinformation_Tool.git\n",
      "   b2bb8a9..0ddd89c  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git commit -m \"labeling on headlines to evaluate labeling\"\n",
    "!git pull\n",
    "!git push -u origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8446acb8-a93e-418e-a0ec-3ef05f7389b8",
   "metadata": {},
   "source": [
    "## Sports Misinformation Classification Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc105918-5fb3-432a-b47f-f3717be38100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82e525a-1cf9-47be-8298-238a25ce2667",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Connect to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fde6c4b-1920-4bb2-846f-61455f869aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database successfully!\n"
     ]
    }
   ],
   "source": [
    "username = 'root'  \n",
    "password = 'password'\n",
    "host = 'localhost' \n",
    "database = 'sports_news_db'\n",
    "\n",
    "connection = mysql.connector.connect(\n",
    "    host=host,\n",
    "    user=username,\n",
    "    password=password,\n",
    "    database=database\n",
    ")\n",
    "\n",
    "if connection.is_connected():\n",
    "    print(\"Connected to the database successfully!\")\n",
    "\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{username}:{password}@{host}/{database}\", echo=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70ed146f-9fe7-4370-a3d2-898811f3e183",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Use this format to insert into the database\n",
    "\n",
    "INSERT INTO articles (team_or_player, source, publication_date, content, trust_score, classification, link)\n",
    "VALUES\n",
    "('New York Yankees, Los Angeles Lakers', 'ESPN', '2024-10-27', 'Yankees article content example.', 85.00, 'real', 'https://example.com/article1'),\n",
    "('Los Angeles Lakers', 'Twitter', '2024-10-27', 'Lakers article content example.', 60.00, 'fake', 'https://example.com/article2');\n",
    "\n",
    "\n",
    "#### Table fields \n",
    "Table Name: articles\n",
    "\n",
    "Fields: \n",
    "\n",
    "     id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "     \n",
    "     team_or_player VARCHAR(500), (This will be the article title that we can query- usually includes teams or names in it)\n",
    "     \n",
    "     source VARCHAR(200),\n",
    "     \n",
    "     publication_date DATE,\n",
    "     \n",
    "     content TEXT,\n",
    "     \n",
    "     trust_score DECIMAL(5, 2), \n",
    "     \n",
    "     classification ENUM('credible', 'uncredible', 'unknown') DEFAULT 'unknown',\n",
    "\n",
    "     link VARCHAR(255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359168cc-f336-4972-b580-38575b45a13e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Simulate Tool Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9687e8d1-8c66-4df1-80cf-4bcf6f33805c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the team or player's name:  patriots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No articles found for patriots.\n"
     ]
    }
   ],
   "source": [
    "team_or_player = input(\"Enter the team or player's name: \")\n",
    "\n",
    "query = f\"SELECT * FROM articles WHERE team_or_player LIKE '%{team_or_player}%'\" #search for entered name/team in the title \n",
    "df_result = pd.read_sql(query, con=engine, params={'team_or_player': team_or_player})\n",
    "\n",
    "#get results\n",
    "if not df_result.empty:\n",
    "    print(f\"Articles related to {team_or_player}:\")\n",
    "    display(df_result)\n",
    "else:\n",
    "    print(f\"No articles found for {team_or_player}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cee353-47af-48c3-a0eb-cf19ddcbc995",
   "metadata": {},
   "source": [
    "### Get Article Entries from RSS Feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06ea0136-a9b1-4fac-94f6-1949deec9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reddit API - collected 1876 posts\n",
    "# import csv\n",
    "# import praw\n",
    "# from datetime import datetime\n",
    "\n",
    "\n",
    "# reddit = praw.Reddit(\n",
    "#     client_id='w-kwRyPigyjYeG9DOiDc8g', \n",
    "#     client_secret='ZeDsvNH2YlpVH7F9wEWPkt5wkjLzqA',  \n",
    "#     user_agent='sports_misinfo_script'  \n",
    "# )\n",
    "\n",
    "# subreddit = reddit.subreddit('sports+fantasyfootball') #2 subreddits \n",
    "\n",
    "# recent_posts = []\n",
    "# for post in subreddit.new(limit= 5000):\n",
    "#     created_date = datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "#     recent_posts.append({\n",
    "#         'title': post.title,\n",
    "#         'score': post.score,\n",
    "#         'url': post.url,\n",
    "#         'id': post.id,\n",
    "#         'author': str(post.author),\n",
    "#         'text': post.selftext,\n",
    "#         'created_date': created_date,\n",
    "#         'num_comments': post.num_comments,\n",
    "#         'subreddit': post.subreddit.display_name,\n",
    "\n",
    "#     })\n",
    "\n",
    "# print(f\"Fetched {len(recent_posts)} posts\")\n",
    "\n",
    "# for i, post in enumerate(recent_posts[:5]):\n",
    "#     print(f\"{i+1}. Title: {post['title']} | Score: {post['score']} | URL: {post['url']}\")\n",
    "\n",
    "\n",
    "# #-------------------------------\n",
    "# #Save the data to a csv file\n",
    "# fieldnames = ['title', 'score', 'url', 'id', 'author', 'text', 'created_date', 'num_comments', 'subreddit']\n",
    "\n",
    "# with open('recent_sports_reddit_posts.csv', mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "#     writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "#     writer.writeheader()\n",
    "\n",
    "#     for post in recent_posts:\n",
    "#         writer.writerow(post)\n",
    "\n",
    "# print(\"Data successfully saved to 'recent_sports_reddit_posts.csv'\")\n",
    "# print(\"Data saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "334d2970-7ee0-47e1-80d9-e01ae15fa4a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape content from https://www.espn.com/nfl/game/_/gameId/401671875/steelers-browns: 403 Client Error: Forbidden for url: https://www.espn.com/nfl/game/_/gameId/401671875/steelers-browns\n",
      "Failed to scrape content from https://www.espn.com/mlb/story/_/id/42530953/dodgers-shohei-ohtani-yankees-aaron-judge-unanimous-mvps: 403 Client Error: Forbidden for url: https://www.espn.com/mlb/story/_/id/42530953/dodgers-shohei-ohtani-yankees-aaron-judge-unanimous-mvps\n",
      "Failed to scrape content from https://www.espn.com/college-sports/recruiting/football/story/_/id/42531006/bryce-underwood-no-1-recruit-2025-flips-michigan-lsu: 403 Client Error: Forbidden for url: https://www.espn.com/college-sports/recruiting/football/story/_/id/42531006/bryce-underwood-no-1-recruit-2025-flips-michigan-lsu\n",
      "Failed to scrape content from https://www.espn.com/nfl/story/_/id/42529180/sources-hof-committee-passes-patriots-robert-kraft-again: 403 Client Error: Forbidden for url: https://www.espn.com/nfl/story/_/id/42529180/sources-hof-committee-passes-patriots-robert-kraft-again\n",
      "Failed to scrape content from https://www.espn.com/nfl/story/_/id/42529597/giants-qb-daniel-jones-regretful-losses-processing-future: 403 Client Error: Forbidden for url: https://www.espn.com/nfl/story/_/id/42529597/giants-qb-daniel-jones-regretful-losses-processing-future\n",
      "Failed to scrape content from https://www.espn.com/nba/story/_/id/42530352/76ers-tests-show-no-structural-damage-paul-george-knee: 403 Client Error: Forbidden for url: https://www.espn.com/nba/story/_/id/42530352/76ers-tests-show-no-structural-damage-paul-george-knee\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42530907/caitlin-clark-part-cincinnati-group-nwsl-expansion-bid: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42530907/caitlin-clark-part-cincinnati-group-nwsl-expansion-bid\n",
      "Failed to scrape content from https://www.espn.com/mlb/story/_/id/42530207/st-petersburg-approves-23m-repair-tropicana-field-roof: 403 Client Error: Forbidden for url: https://www.espn.com/mlb/story/_/id/42530207/st-petersburg-approves-23m-repair-tropicana-field-roof\n",
      "Failed to scrape content from https://www.espn.com/nhl/story/_/id/42530455/capitals-star-alex-ovechkin-4-6-weeks-broken-fibula: 403 Client Error: Forbidden for url: https://www.espn.com/nhl/story/_/id/42530455/capitals-star-alex-ovechkin-4-6-weeks-broken-fibula\n",
      "Failed to scrape content from https://www.espn.com/mlb/story/_/id/42372887/2024-mlb-awards-mvp-cy-young-rookie-manager-year-predictions-results-analysis: 403 Client Error: Forbidden for url: https://www.espn.com/mlb/story/_/id/42372887/2024-mlb-awards-mvp-cy-young-rookie-manager-year-predictions-results-analysis\n",
      "Failed to scrape content from https://www.espn.com/nba/story/_/id/42509302/how-chris-paul-victor-wembanyama-evolving-together: 403 Client Error: Forbidden for url: https://www.espn.com/nba/story/_/id/42509302/how-chris-paul-victor-wembanyama-evolving-together\n",
      "Failed to scrape content from https://www.espn.com/womens-college-basketball/story/_/id/42507416/juju-watkins-usc-next-caitlin-clark-women-college-basketball-2024-25: 403 Client Error: Forbidden for url: https://www.espn.com/womens-college-basketball/story/_/id/42507416/juju-watkins-usc-next-caitlin-clark-women-college-basketball-2024-25\n",
      "Failed to scrape content from https://www.espn.com/nhl/story/_/id/42531693/nhl-alex-ovechkin-injury-goal-scoring-record-wayne-gretzky-capitals-playoffs: 403 Client Error: Forbidden for url: https://www.espn.com/nhl/story/_/id/42531693/nhl-alex-ovechkin-injury-goal-scoring-record-wayne-gretzky-capitals-playoffs\n",
      "Failed to scrape content from https://www.espn.com/college-football/story/_/page/gamedaykickoff112124/2024-week-13-college-football-game-day-kickoff: 403 Client Error: Forbidden for url: https://www.espn.com/college-football/story/_/page/gamedaykickoff112124/2024-week-13-college-football-game-day-kickoff\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42485806/manchester-united-ruben-amorim-predictions-team-formation-transfers-projections: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42485806/manchester-united-ruben-amorim-predictions-team-formation-transfers-projections\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42504471/premier-league-surprise-disappointing-players-far-salah-haaland-palmer-caicedo-sterling-rashford: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42504471/premier-league-surprise-disappointing-players-far-salah-haaland-palmer-caicedo-sterling-rashford\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42527744/pep-guardiola-signs-manchester-city-extension-2027: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42527744/pep-guardiola-signs-manchester-city-extension-2027\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42526490/messi-miami-teammate-gressel-said-star-accept-ties: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42526490/messi-miami-teammate-gressel-said-star-accept-ties\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42527236/spirits-croix-bethune-nwsl-awards-age-doesnt-matter: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42527236/spirits-croix-bethune-nwsl-awards-age-doesnt-matter\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42523638/alexia-putellas-200-barcelona-goals-uwcl: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42523638/alexia-putellas-200-barcelona-goals-uwcl\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42519846/football-clubs-earn-125m-money-owed-player-transfers: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42519846/football-clubs-earn-125m-money-owed-player-transfers\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42522000/chelsea-reece-james-suffers-new-hamstring-injury: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42522000/chelsea-reece-james-suffers-new-hamstring-injury\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42520241/son-heung-min-lauds-palestine-spirit-example-follow: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42520241/son-heung-min-lauds-palestine-spirit-example-follow\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42523272/ed-sheeran-helped-sign-ipswich-player-taylor-swift-show: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42523272/ed-sheeran-helped-sign-ipswich-player-taylor-swift-show\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42530768/soccer-transfer-rumors-news-manchester-united-exploring-randal-kolo-muani-loan: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42530768/soccer-transfer-rumors-news-manchester-united-exploring-randal-kolo-muani-loan\n",
      "Failed to scrape content from https://www.espn.com/mens-college-basketball/story/_/id/42510695/mens-college-basketball-power-rankings-2024-25-marquette-wisconsin: 403 Client Error: Forbidden for url: https://www.espn.com/mens-college-basketball/story/_/id/42510695/mens-college-basketball-power-rankings-2024-25-marquette-wisconsin\n",
      "Failed to scrape content from https://www.espn.com/womens-college-basketball/story/_/id/42504866/geno-auriemma-uconn-all-ncaa-coaching-wins-record-legacy-changed-lives: 403 Client Error: Forbidden for url: https://www.espn.com/womens-college-basketball/story/_/id/42504866/geno-auriemma-uconn-all-ncaa-coaching-wins-record-legacy-changed-lives\n",
      "Failed to scrape content from https://www.espn.com/college-football/story/_/id/42487935/curt-cignetti-google-indiana-hoosiers-college-football-playoff: 403 Client Error: Forbidden for url: https://www.espn.com/college-football/story/_/id/42487935/curt-cignetti-google-indiana-hoosiers-college-football-playoff\n",
      "Failed to scrape content from https://www.espn.com/nfl/story/_/id/42511163/nfl-packers-jordan-love-49ers-playoff-loss-rematch: 403 Client Error: Forbidden for url: https://www.espn.com/nfl/story/_/id/42511163/nfl-packers-jordan-love-49ers-playoff-loss-rematch\n",
      "Failed to scrape content from https://www.espn.com/womens-college-basketball/story/_/id/42526423/uconn-huskies-chris-dailey-geno-auriemma-paige-bueckers-azzi-fudd-wig-hairstyle: 403 Client Error: Forbidden for url: https://www.espn.com/womens-college-basketball/story/_/id/42526423/uconn-huskies-chris-dailey-geno-auriemma-paige-bueckers-azzi-fudd-wig-hairstyle\n",
      "Failed to scrape content from https://www.espn.com/nfl/story/_/id/42512446/saquon-barkley-hurdle-madden-25-update: 403 Client Error: Forbidden for url: https://www.espn.com/nfl/story/_/id/42512446/saquon-barkley-hurdle-madden-25-update\n",
      "Failed to scrape content from https://www.espn.com/racing/f1/story/_/id/42528514/already-commercial-success-f1-vegas-gp-deliver-more: 403 Client Error: Forbidden for url: https://www.espn.com/racing/f1/story/_/id/42528514/already-commercial-success-f1-vegas-gp-deliver-more\n",
      "Failed to scrape content from https://www.espn.com/f1/story/_/id/42504384/las-vegas-grand-prix-s-stats-preview: 403 Client Error: Forbidden for url: https://www.espn.com/f1/story/_/id/42504384/las-vegas-grand-prix-s-stats-preview\n",
      "Failed to scrape content from https://www.espn.com/boxing/story/_/id/42487901/women-boxing-pound-pound-rankings-three-undisputed-champs-top-10: 403 Client Error: Forbidden for url: https://www.espn.com/boxing/story/_/id/42487901/women-boxing-pound-pound-rankings-three-undisputed-champs-top-10\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42507374/bayer-leverkusen-lost-won-last-season-bundesliga-title: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42507374/bayer-leverkusen-lost-won-last-season-bundesliga-title\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42520543/can-nwsl-ever-compete-mls-nhl-nfl-nba: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42520543/can-nwsl-ever-compete-mls-nhl-nfl-nba\n",
      "Failed to scrape content from https://www.espn.com/espn/betting/story/_/id/42514554/2024-nba-betting-futures-bet-most-improved-player-race-jalen-williams-jalen-johnson: 403 Client Error: Forbidden for url: https://www.espn.com/espn/betting/story/_/id/42514554/2024-nba-betting-futures-bet-most-improved-player-race-jalen-williams-jalen-johnson\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#add urls to this list to parse\n",
    "url_list = [\n",
    "        # #MBFC High Credibility-9 \n",
    "        \"https://www.espn.com/espn/rss/news\", #ESPN top headlines\n",
    "        \"https://deadspin.com/rss/\", #Deadspin \n",
    "        \"https://feeds.abcnews.com/abcnews/sportsheadlines\", #ABC news- espn sports\n",
    "        \"https://rss.nytimes.com/services/xml/rss/nyt/Sports.xml\", #NYT\n",
    "        \"https://www.nytimes.com/athletic/rss/news/\", #The Athletic- acquired by NYT\n",
    "        \"https://www.mlb.com/feeds/news/rss.xml\", #MLB news \n",
    "        \"https://www.reutersagency.com/feed/?best-topics=sports&post_type=best\", # Reuters\n",
    "        \"https://sports.yahoo.com/rss/\", #Yahoo News\n",
    "        \"https://www.cbssports.com/rss/headlines/\", #CBS sports general headlines \n",
    "\n",
    "        #MBFC questionable sources or medium credibility- 5\n",
    "        \"https://notthebee.com/feed\", #not the bee\n",
    "        \"https://uproxx.com/sports/feed/\", #uproxx   \n",
    "        \"https://www.vibe.com/c/news/sports/feed/\", #The vibe - medium cred \n",
    "        \"https://moxie.foxnews.com/google-publisher/sports.xml\", #fox news \n",
    "        \"https://nypost.com/sports/feed/\", #NY post - medium cred\n",
    "\n",
    "        #NO MBFC RATING -11 \n",
    "        \"https://www.sportscollectorsdaily.com/feed/\", #Sports Collectors Daily \n",
    "        \"https://news.sportslogos.net/feed/\", #SportsLogos.Net\n",
    "        \"https://www.sportingnews.com/us/rss\", #sportingnews.com\n",
    "        \"https://www.sportskeeda.com/feed\",#sportskeeda.com\n",
    "        \"https://sportsweez.com/feed/\", #sportsweez\n",
    "        \"https://sportsbrackets.net/feed/\", #sportsbracket\n",
    "        \"https://21sports.com/feed/\", # 21 sports\n",
    "        \"https://www.essentiallysports.com/feed/\", #essentiallysports\n",
    "        \"https://boxingnewsonline.net/feed/\", #boxing news\n",
    "        \"https://www.thecoldwire.com/feed/\", #the cold wire\n",
    "        \"https://sportsdark.com/feed/\", #sports dark\n",
    "         \n",
    "]\n",
    "    \n",
    "entries = [] #list of dictionaries\n",
    "\n",
    "#--------------DEFINE FUNCTION TO SCRAPE-------------------\n",
    "def scrape_article_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        #extract content from common tags\n",
    "        article_body = (\n",
    "            soup.find(\"article\") or\n",
    "            soup.find(\"div\", {\"class\": \"post-content\"}) or\n",
    "            soup.find(\"div\", class_=\"article-body\") or\n",
    "            soup.find(\"div\", class_=\"article-content\") or\n",
    "            soup.find(\"section\", class_=\"article-section\") or\n",
    "            soup.find(\"div\", class_=\"main-content\") or\n",
    "            soup.find(\"div\", class_=\"content-body\")\n",
    "        )\n",
    "        \n",
    "        if article_body:\n",
    "            paragraphs = article_body.find_all(\"p\")\n",
    "        else:\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "\n",
    "        #join all paragraphs into a single string\n",
    "        article_content = \" \".join(p.get_text() for p in paragraphs)\n",
    "        return article_content.strip() if article_content else \"No content found\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape content from {url}: {e}\")\n",
    "        return \"Failed to fetch content\"\n",
    "\n",
    "\n",
    "#run the function to collect feeds and scrape\n",
    "for url in url_list:\n",
    "    feed = feedparser.parse(url)\n",
    "    #feed_title= feed.feed.title\n",
    "    feed_title = getattr(feed.feed, \"title\", None)\n",
    "\n",
    "    \n",
    "    for entry in feed.entries:\n",
    "        entry_title = getattr(entry, \"title\", None)\n",
    "        entry_link = getattr(entry, \"link\", None)\n",
    "        entry_published_date = getattr(entry, \"published\", None)\n",
    "        entry_summary = getattr(entry, \"summary\", None)\n",
    "        entry_content = scrape_article_content(entry_link) #scrape\n",
    "        \n",
    "        entries.append({\n",
    "            \"feed_title\": feed_title,\n",
    "            \"entry_title\": entry_title,\n",
    "            \"entry_link\": entry_link,\n",
    "            \"entry_published_date\": entry_published_date,\n",
    "            \"entry_summary\": entry_summary,\n",
    "            \"entry_content\": entry_content,\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(entries)\n",
    "#print(df)\n",
    "\n",
    "df.to_csv('RSS_sports_feeds_11-21.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b17ce3-4167-4ee2-b75b-0e30f8d21697",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Attempt to Scrape Links without RSS Feeds- not very effective "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f9a3d96-4201-4150-8ecb-b14e133f886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "\n",
    "# uncredible_urls = [\n",
    "#     \"https://www.tellerreport.com/sports\",\n",
    "#     \"https://www.newsbreak.com/mountain-view-ca-sports\",\n",
    "#     \"https://newsrnd.com/sports\",\n",
    "#     \"https://baltimorecitywire.com/stories/tag/53-sports\"\n",
    "# ]\n",
    "\n",
    "# # Function to scrape article details\n",
    "# def scrape_article(url):\n",
    "#     try:\n",
    "#         response = requests.get(url, timeout=10)\n",
    "#         response.raise_for_status()\n",
    "#         soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "#         # Extract the article title\n",
    "#         title = soup.find(\"h1\").get_text() if soup.find(\"h1\") else soup.title.get_text()\n",
    "\n",
    "#         # Extract the publication date (common in <time> or meta tags)\n",
    "#         date = soup.find(\"time\")\n",
    "#         if date:\n",
    "#             publication_date = date.get(\"datetime\") or date.get_text()\n",
    "#         else:\n",
    "#             date_meta = soup.find(\"meta\", {\"name\": \"article:published_time\"})\n",
    "#             publication_date = date_meta[\"content\"] if date_meta else \"No date found\"\n",
    "\n",
    "#         # Extract the article content\n",
    "#         content_container = (\n",
    "#             soup.find(\"article\") or\n",
    "#             soup.find(\"div\", class_=[\"post-content\", \"article-body\", \"article-content\", \"content-body\"])\n",
    "#         )\n",
    "#         if content_container:\n",
    "#             paragraphs = content_container.find_all(\"p\")\n",
    "#         else:\n",
    "#             paragraphs = soup.find_all(\"p\")\n",
    "\n",
    "#         content = \"\\n\".join(p.get_text() for p in paragraphs)\n",
    "\n",
    "#         return {\n",
    "#             \"Title\": title.strip(),\n",
    "#             \"Publication Date\": publication_date.strip(),\n",
    "#             \"Content\": content.strip()[:500] + \"...\",\n",
    "#             \"Link\": url\n",
    "#         }\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to scrape {url}: {e}\")\n",
    "#         return None\n",
    "\n",
    "# articles = []\n",
    "# for url in uncredible_urls:\n",
    "#     print(f\"Scraping: {url}\")\n",
    "#     article_details = scrape_article(url)\n",
    "#     if article_details:\n",
    "#         articles.append(article_details)\n",
    "\n",
    "# for article in articles:\n",
    "#     print(\"\\n--- Article ---\")\n",
    "#     print(f\"Title: {article['Title']}\")\n",
    "#     print(f\"Date: {article['Publication Date']}\")\n",
    "#     print(f\"Content Preview: {article['Content']}\")\n",
    "#     print(f\"Link: {article['Link']}\\n\")\n",
    "\n",
    "# df = pd.DataFrame(articles)\n",
    "# output_file = \"scraped_uncredible_articles.csv\"\n",
    "# df.to_csv(output_file, index=False)\n",
    "# print(f\"\\nResults saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dd4fae-b096-4e94-beed-e7c6b237031a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Format DF to match DB table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b6b1163a-ce9f-47f0-9c15-d7e5cceaf6e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tizia\\Anaconda4\\Lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "df['publication_date'] = df['entry_published_date'].apply(lambda x: parser.parse(x).date()) #parse different date formats to date object format\n",
    "dbdf = pd.read_csv('RSS_sports_feeds.csv')\n",
    "dbdf['team_or_player'] = df['entry_title']\n",
    "dbdf['source'] = df['feed_title']\n",
    "dbdf['publication_date'] = df['publication_date'] \n",
    "dbdf['content'] = df['entry_summary']    #entry content if not empty? or keep as summary\n",
    "dbdf['trust_score'] = 0.00               #CHANGE to score column\n",
    "dbdf['classification'] = 'unknown'       #CHANGE to label column \n",
    "dbdf['link'] = df['entry_link']\n",
    "\n",
    "#make a new df in the format of the DB table for easy inserting\n",
    "sports_DB_df = dbdf[['team_or_player', 'source', 'publication_date', 'content', 'trust_score', 'classification', 'link']]\n",
    "#save to a new CSV \n",
    "sports_DB_df.to_csv('formatted_sports_posts_for_DB.csv', index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b59c465-65a2-414d-90d3-7cb99ffb36e9",
   "metadata": {},
   "source": [
    "### Feature Computation Functions - use some for labeling and others for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbc4a29c-616d-475e-bad1-56ae7771036c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\tizia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install textstat\n",
    "from transformers import pipeline\n",
    "import string\n",
    "from textstat import flesch_reading_ease\n",
    "import textstat\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "#functions for each feature we are considering\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def char_count(text):\n",
    "    return len(text)\n",
    "\n",
    "def avg_word_length(text):\n",
    "    words = text.split()\n",
    "    return sum(len(word) for word in words) / len(words) if words else 0\n",
    "\n",
    "def all_capitalized_word_count(text):\n",
    "    return sum(1 for word in text.split() if word.isupper())\n",
    "\n",
    "def punctuation_count(text):\n",
    "    return sum(1 for char in text if char in string.punctuation)\n",
    "\n",
    "def exclamation_count(text):\n",
    "    return text.count(\"!\")\n",
    "\n",
    "def question_mark_count(text):\n",
    "    return text.count(\"?\")\n",
    "\n",
    "def readability_score(text): #high score = easier readability\n",
    "    return flesch_reading_ease(text)\n",
    "\n",
    "def smog_index(text): #readability based on the number of complex words (3+ syllables)\n",
    "    return textstat.smog_index(text)\n",
    "\n",
    "def count_quotation_pairs(text): \n",
    "    double_quotes = re.findall(r'\"[^\"]+\"', text) #Finds all sections of text in double quotes\n",
    "    return len(double_quotes)\n",
    "\n",
    "def variety_of_vocabularity(text):\n",
    "    words = text.split()\n",
    "    unique_words = set(words)\n",
    "    return len(unique_words) / len(words) if words else 0\n",
    "\n",
    "\n",
    "sensational_words = [\"shocking\",\"shock\", \"amazing\", \"unbelievable\", \"you won’t believe\", \"incredible\", \"stunning\",\\\n",
    "                    \"astounding\", \"breathtaking\", \"outstanding\", \"thrilling\", \"must see\", \"terrible\", \"awful\",\\\n",
    "                     \"fantastic\", \"horrible\", \"remarkable\"]\n",
    "def count_sensational_words(text):\n",
    "    text = text.lower()\n",
    "    return sum(1 for word in sensational_words if word in text)\n",
    "\n",
    "\n",
    "absolute_words = [\"always\", \"never\", \"clearly\", \"obviously\", \"definitely\", \"everyone\", \"nobody\", \"all\",\\\n",
    "              \"none\", \"blatantly\", \"undoubtedly\", \"must\", \"should\"]\n",
    "def count_absolute_words(text):\n",
    "    words = text.lower().split()\n",
    "    return sum(1 for word in words if word in absolute_words)\n",
    "\n",
    "\n",
    "factuality_words = [\"proven\", \"irrefutable\", \"unarguable\", \"unquestionably\", \"certainly\", \"definitely\",\\\n",
    "                    \"undeniable\"]\n",
    "def count_factuality_words(text):\n",
    "    words = text.lower().split()\n",
    "    return sum(1 for word in words if word in factuality_words)\n",
    "\n",
    "\n",
    "subjective_words = [\"believe\", \"think\", \"feel\", \"prefer\", \"seems\", \"wish\"]\n",
    "def subjectivity_score(text):\n",
    "    words = text.lower().split()\n",
    "    subjective_count = sum(1 for word in words if word in subjective_words)\n",
    "    return subjective_count / len(words) if len(words) > 0 else 0\n",
    "\n",
    "objective_words = [\"reported\", \"measured\", \"confirmed\", \"analyzed\", \"observed\", \"recorded\",\\\n",
    "                   \"found\", \"documented\", \"verified\", \"tested\", \"studied\", \"calculated\", \"noted\",\\\n",
    "                   \"established\", \"evidence\", \"fact\", \"data\", \"statistics\", \"demonstrated\", \"shown\",\\\n",
    "                   \"results\", \"result\", \"evidence-based\", \"peer-reviewed\", \"sampled\", \"quantified\",\\\n",
    "                   \"evaluated\", \"experimented\", \"investigated\"]\n",
    "def objective_score(text):\n",
    "    words = text.lower().split()\n",
    "    objective_count = sum(1 for word in words if word in objective_words)\n",
    "    return objective_count / len(words) if len(words) > 0 else 0\n",
    "\n",
    "\n",
    "def count_numerics(text):\n",
    "    numerical_count = len(re.findall(r'\\d+', text)) \n",
    "    return numerical_count\n",
    "\n",
    "\n",
    "pipe_finnews = pipeline(\"text-classification\", model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
    "def sentiment(text):\n",
    "    result = pipe_finnews(text)\n",
    "    return result[0][\"label\"] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c282d88e-3a8a-4283-af31-f011ec789135",
   "metadata": {},
   "source": [
    "### Define Labeling Heuristic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f055a2fa-dd0d-4480-88ff-0744cbe40bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_credibility_score(headline, source_credibility):\n",
    "    # Source credibility weights\n",
    "    SOURCE_CREDIBILITY = {\n",
    "        'high': 1.0,\n",
    "        'unknown': 0,\n",
    "        'medium': -0.5,\n",
    "        'low': -1.0\n",
    "    }\n",
    "    \n",
    "    # Top heuristics and their weights (based on the difference column)\n",
    "    heuristics = {\n",
    "        'smog_index': {\n",
    "            'weight': 9.448,  # From the Difference column\n",
    "            'true_avg': 0.124, # From the True_Avg column\n",
    "            'false_avg': 1.169 # From the False_Avg column\n",
    "        },\n",
    "        'factuality_words_count': {\n",
    "            'weight': 8.200,  \n",
    "            'true_avg': 0.000263,\n",
    "            'false_avg': 0.002158\n",
    "        },\n",
    "        'exclamation_count': {\n",
    "            'weight': 6.326, \n",
    "            'true_avg': 0.0192,\n",
    "            'false_avg': 0.1215\n",
    "        },\n",
    "        'subjectivity_score': {\n",
    "            'weight': 4.308,  \n",
    "            'true_avg': 0.000373,\n",
    "            'false_avg': 0.001606\n",
    "        },\n",
    "        'question_mark_count': {\n",
    "            'weight': 4.285,  \n",
    "            'true_avg': 0.0327,\n",
    "            'false_avg': 0.1403\n",
    "        },\n",
    "        'count_numerics': {\n",
    "            'weight': 3.089,  \n",
    "            'true_avg': 0.7496,\n",
    "            'false_avg': 0.2427\n",
    "        }\n",
    "    }   \n",
    "    \n",
    "    # Calculate metrics for each of the heuristics we're using\n",
    "    metrics = {\n",
    "        'smog_index': smog_index(headline),\n",
    "        'factuality_words_count': count_factuality_words(headline),\n",
    "        'exclamation_count': exclamation_count(headline),\n",
    "        'subjectivity_score': subjectivity_score(headline),\n",
    "        'question_mark_count': question_mark_count(headline),\n",
    "        'count_numerics': count_numerics(headline)\n",
    "    }\n",
    "\n",
    "    # Initialize the credibility score\n",
    "    credibility_score = 0\n",
    "    \n",
    "    # Calculate scores for each heuristic\n",
    "    for metric, values in heuristics.items():\n",
    "        true_avg = values['true_avg']\n",
    "        false_avg = values['false_avg']\n",
    "        weight = values['weight']\n",
    "        current_value = metrics[metric]\n",
    "        \n",
    "        # Calculate which average the current value is closer to\n",
    "        dist_to_true = abs(current_value - true_avg)\n",
    "        dist_to_false = abs(current_value - false_avg)\n",
    "        \n",
    "        # Calculate percentage difference from the midpoint\n",
    "        midpoint = (true_avg + false_avg) / 2            \n",
    "        percent_diff = abs(current_value - midpoint) / midpoint\n",
    "        \n",
    "        # Add or subtract from score based on which average it's closer to\n",
    "        if dist_to_true < dist_to_false:\n",
    "            credibility_score += weight * percent_diff\n",
    "        else:\n",
    "            credibility_score -= weight * percent_diff\n",
    "        \n",
    "        # print(f\"{metric}: {current_value:.4f}\\n\\tWeight: {weight:.2f}\\n\\tPercent Difference: {percent_diff:.4f}\\n\\tCredibility Score: {credibility_score:.2f}\")\n",
    "    \n",
    "    # Add source credibility modifier\n",
    "    if source_credibility: \n",
    "        source_credibility_score = SOURCE_CREDIBILITY.get(source_credibility.lower(), SOURCE_CREDIBILITY['unknown'])\n",
    "        weight = 10\n",
    "        credibility_score += (source_credibility_score * weight)\n",
    "        # print(f\"source_credibility: {source_credibility_score:.2f}\\n\\tWeight: {weight:.2f}\\n\\tCredibility Score: {credibility_score:.2f}\")\n",
    "\n",
    "    return credibility_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237d0293-0598-4772-9e11-a55b7592cd38",
   "metadata": {},
   "source": [
    "### Define Source credibilty of each source and run Labeling Heuristic Function on RSS Feed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e839780d-0227-4742-9909-246246e03b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['www.espn.com - TOP'\n",
      " 'Deadspin > Sports News Without Fear, Favor or Compromise'\n",
      " 'ABC News: Sports' 'NYT > Sports' 'Latest Headlines - The Athletic'\n",
      " 'MLB News'\n",
      " 'Yahoo! Sports - News, Scores, Standings, Rumors, Fantasy Games'\n",
      " 'CBSSports.com Headlines' 'Not the Bee' 'Sports – UPROXX' 'Sports'\n",
      " 'Latest Sports News Today on Fox News'\n",
      " 'NY Post Sports – Latest News, Scores, Stats & Videos'\n",
      " 'Sports Collectors Daily' 'SportsLogos.Net News' 'Sporting News RSS'\n",
      " 'Sports Weez' '21Sports.com' 'EssentiallySports' 'Boxing News'\n",
      " 'The Cold Wire' 'Sportsdark' 'Sports Brackets' 'Sportskeeda'\n",
      " 'FOX Sports Digital']\n",
      "saved\n"
     ]
    }
   ],
   "source": [
    "#------------------------Determine source credibility-------------------------------------------\n",
    "\n",
    "all_articles_df = pd.read_csv('All_RSS_articles.csv')\n",
    "print(all_articles_df[\"feed_title\"].unique())\n",
    "    \n",
    "high_cred= [\n",
    "    'www.espn.com - TOP',\n",
    "    'Deadspin > Sports News Without Fear, Favor or Compromise',\n",
    "    'Yahoo! Sports - News, Scores, Standings, Rumors, Fantasy Games',\n",
    "    'MLB News',\n",
    "    'ABC News: Sports',\n",
    "    'Latest Headlines- The Athletic',\n",
    "    'NYT > Sports',\n",
    "    'CBSSports.com Headlines',\n",
    "]\n",
    "\n",
    "medium_cred=[\n",
    "    'Sports', #vibe\n",
    "    'NY Post Sports – Latest News, Scores, Stats & Videos' 'ABC News: Sports',\n",
    "]  \n",
    "\n",
    "low_cred=[\n",
    "    'Sports – UPROXX',\n",
    "    'Not the Bee',\n",
    "    'Latest Sports News Today on Fox News', #foxnews.com\n",
    "]\n",
    "\n",
    "no_cred=[\n",
    "    'SportsLogos.Net News',\n",
    "    'Sporting News RSS', \n",
    "    'Sports Weez', \n",
    "    'Sports Brackets',\n",
    "    '21Sports.com', \n",
    "    'Boxing News', \n",
    "    'The Cold Wire', \n",
    "    'Sportsdark',\n",
    "    'EssentiallySports', \n",
    "    'Sportskeeda', \n",
    "    'Sports Collectors Daily',\n",
    "    'FOX Sports Digital', #foxsports.com\n",
    "]\n",
    "\n",
    "def determine_source_credibility(feed_title):\n",
    "    if feed_title in high_cred:\n",
    "        return \"high\"\n",
    "    elif feed_title in medium_cred:\n",
    "        return \"medium\"\n",
    "    elif feed_title in low_cred:\n",
    "        return \"low\"\n",
    "    else:\n",
    "        return \"unknown\"  \n",
    "\n",
    "\n",
    "#Create a new column for source_credibility based on the feed_title\n",
    "all_articles_df[\"source_credibility\"] = all_articles_df[\"feed_title\"].apply(determine_source_credibility)\n",
    "\n",
    "\n",
    "CREDIBLE_VALUE= 0 \n",
    "credibility_scores = []\n",
    "credibility_labels = []\n",
    "for index, row in all_articles_df.iterrows():\n",
    "    #get the headline and source_credibility\n",
    "    headline = row[\"entry_title\"]\n",
    "    source_credibility = row[\"source_credibility\"]\n",
    "    \n",
    "    #Calculate the credibility score \n",
    "    score= calculate_credibility_score(headline, source_credibility)\n",
    "    credibility_scores.append(score) #add to list\n",
    "\n",
    "    if score>= CREDIBLE_VALUE:\n",
    "        credibility_labels.append(\"credible\")\n",
    "    else:\n",
    "        credibility_labels.append(\"uncredible\")\n",
    "\n",
    "\n",
    "all_articles_df[\"credibility_score\"] = credibility_scores\n",
    "all_articles_df[\"credibility_label\"] = credibility_labels\n",
    "\n",
    "all_articles_df.to_csv(\"labeled_RSS_posts.csv\")\n",
    "print(\"saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecb1bf3-293c-4d0c-831c-28a3e5ce68d1",
   "metadata": {},
   "source": [
    "### run Labeling Heuristic Function on EURO Data to evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4f986a19-fcc3-4c64-972c-8464c352019e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "all_articles_df = pd.read_csv('real_european.csv')\n",
    "all_articles_df = all_articles_df[all_articles_df[\"tweet\"].notna()]\n",
    "\n",
    "#Create a new column for source_credibility based on the tweet- always 'unknown' since not news\n",
    "all_articles_df[\"source_credibility\"] = \"unknown\"\n",
    "\n",
    "CREDIBLE_VALUE= 0 \n",
    "credibility_scores = []\n",
    "credibility_labels = []\n",
    "for index, row in all_articles_df.iterrows():\n",
    "    #get the headline and source_credibility\n",
    "    headline = row[\"tweet\"]\n",
    "    source_credibility = row[\"source_credibility\"]\n",
    "    \n",
    "    #Calculate the credibility score \n",
    "    score= calculate_credibility_score(headline, source_credibility)\n",
    "    credibility_scores.append(score) #add to list\n",
    "\n",
    "    if score>= CREDIBLE_VALUE:\n",
    "        credibility_labels.append(\"credible\")\n",
    "    else:\n",
    "        credibility_labels.append(\"uncredible\")\n",
    "\n",
    "\n",
    "all_articles_df[\"credibility_score\"] = credibility_scores\n",
    "all_articles_df[\"credibility_label\"] = credibility_labels\n",
    "\n",
    "all_articles_df.to_csv(\"testing_labeling_on_euro_real.csv\")\n",
    "print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ebd91a53-6281-4aa3-9c46-dff22b629e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "all_articles_df = pd.read_csv('fake_european.csv')\n",
    "all_articles_df = all_articles_df[all_articles_df[\"tweet\"].notna()]\n",
    "\n",
    "#Create a new column for source_credibility based on the tweet- always 'unknown' since not news\n",
    "all_articles_df[\"source_credibility\"] = \"unknown\"\n",
    "\n",
    "CREDIBLE_VALUE= 0 \n",
    "credibility_scores = []\n",
    "credibility_labels = []\n",
    "for index, row in all_articles_df.iterrows():\n",
    "    #get the headline and source_credibility\n",
    "    headline = row[\"tweet\"]\n",
    "    source_credibility = row[\"source_credibility\"]\n",
    "    \n",
    "    #Calculate the credibility score \n",
    "    score= calculate_credibility_score(headline, source_credibility)\n",
    "    credibility_scores.append(score) #add to list\n",
    "\n",
    "    if score>= CREDIBLE_VALUE:\n",
    "        credibility_labels.append(\"credible\")\n",
    "    else:\n",
    "        credibility_labels.append(\"uncredible\")\n",
    "\n",
    "\n",
    "all_articles_df[\"credibility_score\"] = credibility_scores\n",
    "all_articles_df[\"credibility_label\"] = credibility_labels\n",
    "\n",
    "all_articles_df.to_csv(\"testing_labeling_on_euro_fake.csv\")\n",
    "print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cbd2f928-29f3-4573-8b50-f935ae4cb22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 'credible' in Fake Euro dataset: 15033\n",
      "Count of 'uncredible' in Fake Euro dataset (good): 4955\n",
      "\n",
      "Count of 'credible' in real Euro dataset (good): 21062\n",
      "Count of 'uncredible'in real Euro dataset: 801\n"
     ]
    }
   ],
   "source": [
    "#analysis: counts on each labeled euro dataset\n",
    "fake_test_labeled= pd.read_csv('testing_labeling_on_euro_fake.csv')\n",
    "real_test_labeled= pd.read_csv('testing_labeling_on_euro_real.csv')\n",
    "\n",
    "credible_count = fake_test_labeled[fake_test_labeled['credibility_label'] == 'credible'].shape[0]\n",
    "uncredible_count = fake_test_labeled[fake_test_labeled['credibility_label'] == 'uncredible'].shape[0]\n",
    "print(f\"Count of 'credible' in Fake Euro dataset: {credible_count}\")\n",
    "print(f\"Count of 'uncredible' in Fake Euro dataset (good): {uncredible_count}\")\n",
    "print()\n",
    "real_credible_count = real_test_labeled[real_test_labeled['credibility_label'] == 'credible'].shape[0]\n",
    "real_uncredible_count = real_test_labeled[real_test_labeled['credibility_label'] == 'uncredible'].shape[0]\n",
    "print(f\"Count of 'credible' in real Euro dataset (good): {real_credible_count}\")\n",
    "print(f\"Count of 'uncredible'in real Euro dataset: {real_uncredible_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e036be3e-f960-4310-8980-05ced479a092",
   "metadata": {},
   "source": [
    "### run Labeling Heuristic Function on 20 true/false headlines to evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "19ba69ed-bfff-4a90-bedd-a41d89daa15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Lists\n",
    "false_headlines=[\n",
    "    \"BREAKING: Fox Sports Cancels ALL NFL Broadcasts \\\"Until Players Respect The Flag\\\"\",\n",
    "    \"Rugby Safer Than American Football \\\"For Health Reasons\\\": Biden\",\n",
    "    \"First woman to medal in six straight Olympics. Media and sponsors ignore her because she is outspoken pro-2A.\",\n",
    "    \"Novak Djokovic becomes the first professional athlete in history to be banned from a major sporting competition for not taking drugs\",\n",
    "    \"BREAKING: ESPN has fired Shannon Sharpe, per @ESPNNBA\",\n",
    "    \"The Minnesota Vikings have denounced Tim Walz: \\\"We don’t suppᴏrt his values.\\\"\",\n",
    "    \"'BREAKING: WNBA referees disqualify two players under league’s new \\\"no anthem kneeling\\\" rule\",\n",
    "    \"Nike announces termination of contract with Brittney Griner after \\\"strong backlash\\\" from online community: \\\"We need more athletes like Riley Gaines and less woke Brittney Griner!\\\"\",\n",
    "    \"KNEELING: After the University of Texas, all students who knelt during the national anthem were rounded up and REMOVED FROM SCHOLARSHIPS\",\n",
    "    \"Travis Kelce kneels during national anthem fined $10 million and thrown out of the game.\",\n",
    "    \"The NFL will now use facial recognition at every stadium to verify the identity of everyone at the game.\",\n",
    "    \"Mike Tyson says he’s willing to box Olympic DUDE with all proceeds to go to a battered women’s charity.\",\n",
    "    \"After winning silver, Yusef stood emotionless on the Olympic podium and declared, \\\"Sharon, if you’re watching this, I want my dog back.\\\"\",\n",
    "    \"Miami Dolphins QB Tua Tagovailoa will be sitting front row tonight in Doral for the Trump speech\",\n",
    "    \"BREAKING: The WNBA organizers have officially announced an investigation into the referees in all of Caitlin Clark's games for ignoring all dirty actions by her opponents against her\",\n",
    "    \"Chiefs' Coach Andy Reid \\\"fires 3 top players for anthem kneeling.\\\"\",\n",
    "    \"BREAKING: Caitlin Clark Rejects $400 Million Deal From Nike, \\\"Not With That Kaepernick Clown,\\\"\",\n",
    "    \"At Euro 2020, UEFA (European Football Association) ordered all team captains to wear \\\"OneLove\\\" bands. The band was used as a symbol of LGBTQ. But, Portugal captain Cristiano Ronaldo was the only European captain who did not wear the band.\",\n",
    "    \"Golden State Warriors refuse to visit White House after winning NBA title: reports\",\n",
    "    \"Taylor Swift faces a 10-game NFL ban following controversial political involvement - fans in uproar!\"\n",
    "]\n",
    "true_headlines=[\n",
    "    \"Did that really happen? Barbados v Grenada and a deliberate own goal\",\n",
    "    \"Breakdancing Will Not Be Busting A Move In 2028 Olympics\",\n",
    "    \"Breaking Will Not Be in The 2028 Los Angeles Olympics—What’s Next?\",\n",
    "    \"Braves Superstar Sets Atlanta-Era Record in 1st Inning\",\n",
    "    \"Only 20 schools in the Football Bowl Subdivision have athletic departments with revenue exceeding expenses\",\n",
    "    \"LATEST: Mitchell Stadium named America’s Best High School Football Stadium\",\n",
    "    \"Homes of Patrick Mahomes, Travis Kelce burglarized last month\",\n",
    "    \"Gregg Popovich recovering from mild stroke, no timeline for return set\",\n",
    "    \"Patrick Queen: I wasn’t wanted back with Ravens, it was definitely kind of upsetting\",\n",
    "    \"Odell Beckham Jr. boasts about taking 2021 Rams salary in Bitcoin National Football League Odell Beckham Jr. boasts about taking 2021 Rams salary in Bitcoin\",\n",
    "    \"Ecuador international soccer player Marco Angulo dies aged 22 following car crash\",\n",
    "    \"Shohei Ohtani Baseball Worth $4 Million Lands in Globally-Recognized Skyscraper\",\n",
    "    \"Kobe Bryant is the only person to have won both an Olympic medal and an Oscar\",\n",
    "    \"Bucks fan predicted Milwaukee-Phoenix NBA Finals all the way back in 2016\",\n",
    "    \"Chiefs kicker Butker congratulates women graduates and says most are more excited about motherhood\",\n",
    "    \"Kansas City Chiefs player faces backlash for graduation speech criticizing working women, calling Pride a \\\"deadly sin\\\"\", \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d49ccd92-480d-4ddc-a3a7-1790ce381ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "#true headlines - labels\n",
    "#CREDIBLE_VALUE= 0 \n",
    "credibility_scores = []\n",
    "credibility_labels = []\n",
    "for trueline in true_headlines:\n",
    "    #Calculate the credibility score \n",
    "    score= calculate_credibility_score(trueline, 'unknown')\n",
    "    credibility_scores.append(score) #add to list\n",
    "\n",
    "    if score>= CREDIBLE_VALUE:\n",
    "        credibility_labels.append(\"credible\")\n",
    "    else:\n",
    "        credibility_labels.append(\"uncredible\")\n",
    "\n",
    "all_lines_df = pd.DataFrame({\n",
    "    \"headline\": true_headlines,\n",
    "    \"credibility_score\": credibility_scores,\n",
    "    \"credibility_label\": credibility_labels\n",
    "})\n",
    "\n",
    "all_lines_df\n",
    "all_lines_df.to_csv(\"testing_labeling_on_headlines_true.csv\")\n",
    "print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "546c5e49-a567-4967-a314-ef685d6a6ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "#fake headlines - labels\n",
    "CREDIBLE_VALUE= 0 \n",
    "credibility_scores = []\n",
    "credibility_labels = []\n",
    "for fakeline in false_headlines:\n",
    "    #Calculate the credibility score \n",
    "    score= calculate_credibility_score(fakeline, 'unknown')\n",
    "    credibility_scores.append(score) #add to list\n",
    "\n",
    "    if score>= CREDIBLE_VALUE:\n",
    "        credibility_labels.append(\"credible\")\n",
    "    else:\n",
    "        credibility_labels.append(\"uncredible\")\n",
    "\n",
    "all_lines_df = pd.DataFrame({\n",
    "    \"headline\": false_headlines,\n",
    "    \"credibility_score\": credibility_scores,\n",
    "    \"credibility_label\": credibility_labels\n",
    "})\n",
    "\n",
    "all_lines_df\n",
    "all_lines_df.to_csv(\"testing_labeling_on_headlines_fake.csv\")\n",
    "print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "75e56cec-770e-4e9d-9ba3-7da03567217f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 'credible' in fake headline set: 12\n",
      "Count of 'uncredible' in fake headline set (good): 4\n",
      "\n",
      "Count of 'credible' in real headline dataset (good): 17\n",
      "Count of 'uncredible' in real headline dataset: 3\n"
     ]
    }
   ],
   "source": [
    "#analysis: counts on each labeled euro dataset\n",
    "fake_test_labeled= pd.read_csv('testing_labeling_on_headlines_true.csv')\n",
    "real_test_labeled= pd.read_csv('testing_labeling_on_headlines_fake.csv')\n",
    "\n",
    "credible_count = fake_test_labeled[fake_test_labeled['credibility_label'] == 'credible'].shape[0]\n",
    "uncredible_count = fake_test_labeled[fake_test_labeled['credibility_label'] == 'uncredible'].shape[0]\n",
    "print(f\"Count of 'credible' in fake headline set: {credible_count}\")\n",
    "print(f\"Count of 'uncredible' in fake headline set (good): {uncredible_count}\")\n",
    "print()\n",
    "real_credible_count = real_test_labeled[real_test_labeled['credibility_label'] == 'credible'].shape[0]\n",
    "real_uncredible_count = real_test_labeled[real_test_labeled['credibility_label'] == 'uncredible'].shape[0]\n",
    "print(f\"Count of 'credible' in real headline dataset (good): {real_credible_count}\")\n",
    "print(f\"Count of 'uncredible' in real headline dataset: {real_uncredible_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9690bfe4-5ef2-482b-8965-20c571e0bfed",
   "metadata": {},
   "source": [
    "### Run Labeling on summaries instead of headline to compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d8af5ee1-2e2f-433c-b5cd-5a6f6ae62c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "CREDIBLE_VALUE= 0 \n",
    "credibility_scores = []\n",
    "credibility_labels = []\n",
    "for index, row in all_articles_df.iterrows():\n",
    "    if pd.isna(row[\"entry_summary\"]):\n",
    "        credibility_scores.append(0)\n",
    "        credibility_labels.append(\"No summary\")\n",
    "        continue\n",
    "         \n",
    "    #get the headline and source_credibility\n",
    "    summaries = row[\"entry_summary\"]\n",
    "    source_credibility = row[\"source_credibility\"]\n",
    "    \n",
    "    #Calculate the credibility score \n",
    "    score= calculate_credibility_score(summaries, source_credibility)\n",
    "    credibility_scores.append(score) #add  list\n",
    "\n",
    "    if score>= CREDIBLE_VALUE:\n",
    "        credibility_labels.append(\"credible\")\n",
    "    else:\n",
    "        credibility_labels.append(\"uncredible\")\n",
    "\n",
    "\n",
    "all_articles_df[\"summary_credibility_score\"] = credibility_scores\n",
    "all_articles_df[\"summary_credibility_label\"] = credibility_labels\n",
    "\n",
    "all_articles_df.to_csv(\"labeled_RSS_posts_by_summaries.csv\")\n",
    "print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2587a256-ff31-4908-b906-54013a5b3f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feed_title</th>\n",
       "      <th>entry_title</th>\n",
       "      <th>entry_link</th>\n",
       "      <th>entry_published_date</th>\n",
       "      <th>entry_summary</th>\n",
       "      <th>entry_content</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>source_credibility</th>\n",
       "      <th>credibility_score</th>\n",
       "      <th>credibility_label</th>\n",
       "      <th>summary_credibility_score</th>\n",
       "      <th>summary_credibility_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>www.espn.com - TOP</td>\n",
       "      <td>NBA Power Rankings: Where do all 30 teams stac...</td>\n",
       "      <td>https://www.espn.com/nba/story/_/id/42468239/n...</td>\n",
       "      <td>Wed, 20 Nov 2024 18:27:28 EST</td>\n",
       "      <td>Meanwhile, do the Cavs stay on top after a los...</td>\n",
       "      <td>Failed to fetch content</td>\n",
       "      <td>2024-11-20</td>\n",
       "      <td>high</td>\n",
       "      <td>-3.833633</td>\n",
       "      <td>uncredible</td>\n",
       "      <td>-53.371205</td>\n",
       "      <td>uncredible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>www.espn.com - TOP</td>\n",
       "      <td>Braves lefty Sale captures first Cy Young Award</td>\n",
       "      <td>https://www.espn.com/mlb/story/_/id/42512141/b...</td>\n",
       "      <td>Wed, 20 Nov 2024 18:37:45 EST</td>\n",
       "      <td>Braves pitcher Chris Sale, who went 18-3 with ...</td>\n",
       "      <td>Failed to fetch content</td>\n",
       "      <td>2024-11-20</td>\n",
       "      <td>high</td>\n",
       "      <td>39.478000</td>\n",
       "      <td>credible</td>\n",
       "      <td>83.059578</td>\n",
       "      <td>credible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>www.espn.com - TOP</td>\n",
       "      <td>Clark won't play in Unrivaled league, source says</td>\n",
       "      <td>https://www.espn.com/wnba/story/_/id/42510300/...</td>\n",
       "      <td>Wed, 20 Nov 2024 18:37:44 EST</td>\n",
       "      <td>Fever star Caitlin Clark will not play in the ...</td>\n",
       "      <td>Failed to fetch content</td>\n",
       "      <td>2024-11-20</td>\n",
       "      <td>high</td>\n",
       "      <td>39.478000</td>\n",
       "      <td>credible</td>\n",
       "      <td>51.929879</td>\n",
       "      <td>credible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>www.espn.com - TOP</td>\n",
       "      <td>Sources: Jets owner suggested benching Rodgers</td>\n",
       "      <td>https://www.espn.com/nfl/story/_/id/42508628/s...</td>\n",
       "      <td>Wed, 20 Nov 2024 18:37:44 EST</td>\n",
       "      <td>Jets owner Woody Johnson broached the idea of ...</td>\n",
       "      <td>Failed to fetch content</td>\n",
       "      <td>2024-11-20</td>\n",
       "      <td>high</td>\n",
       "      <td>39.478000</td>\n",
       "      <td>credible</td>\n",
       "      <td>45.703940</td>\n",
       "      <td>credible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>www.espn.com - TOP</td>\n",
       "      <td>Man pleads guilty to murder of three UVA players</td>\n",
       "      <td>https://www.espn.com/college-football/story/_/...</td>\n",
       "      <td>Wed, 20 Nov 2024 18:37:44 EST</td>\n",
       "      <td>A former University of Virginia student could ...</td>\n",
       "      <td>Failed to fetch content</td>\n",
       "      <td>2024-11-20</td>\n",
       "      <td>high</td>\n",
       "      <td>39.478000</td>\n",
       "      <td>credible</td>\n",
       "      <td>45.703940</td>\n",
       "      <td>credible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2849</th>\n",
       "      <td>SportsLogos.Net News</td>\n",
       "      <td>NFL Week 9 Uniform Schedule: Giants, Eagles, P...</td>\n",
       "      <td>https://news.sportslogos.net/2024/11/03/nfl-we...</td>\n",
       "      <td>Sun, 03 Nov 2024 16:25:35 +0000</td>\n",
       "      <td>Our weekly series that looks at every uniform ...</td>\n",
       "      <td>The midway point of the 2024-25 season is here...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>35.703940</td>\n",
       "      <td>credible</td>\n",
       "      <td>29.478000</td>\n",
       "      <td>credible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>SportsLogos.Net News</td>\n",
       "      <td>QMJHL’s Quebec Remparts Pay Homage to Beloved ...</td>\n",
       "      <td>https://news.sportslogos.net/2024/11/02/qmjhls...</td>\n",
       "      <td>Sat, 02 Nov 2024 15:00:00 +0000</td>\n",
       "      <td>Jerseys worn Friday vs. Sherbrooke Phoenix ins...</td>\n",
       "      <td>The Quebec Remparts paid homage to one of thei...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>29.478000</td>\n",
       "      <td>credible</td>\n",
       "      <td>35.703940</td>\n",
       "      <td>credible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2851</th>\n",
       "      <td>SportsLogos.Net News</td>\n",
       "      <td>Oregon Ducks Unveil Final “Generation O” Unifo...</td>\n",
       "      <td>https://news.sportslogos.net/2024/11/02/oregon...</td>\n",
       "      <td>Sat, 02 Nov 2024 05:03:33 +0000</td>\n",
       "      <td>Oregon unveiled the fifth and final combinatio...</td>\n",
       "      <td>The Oregon football program unveiled the fifth...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>29.478000</td>\n",
       "      <td>credible</td>\n",
       "      <td>29.478000</td>\n",
       "      <td>credible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852</th>\n",
       "      <td>SportsLogos.Net News</td>\n",
       "      <td>Illinois Fighting Illini Unveil Military Appre...</td>\n",
       "      <td>https://news.sportslogos.net/2024/11/02/illino...</td>\n",
       "      <td>Sat, 02 Nov 2024 04:02:08 +0000</td>\n",
       "      <td>Illinois will wear a special helmet design dur...</td>\n",
       "      <td>The Illinois football program will wear a spec...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>29.478000</td>\n",
       "      <td>credible</td>\n",
       "      <td>29.478000</td>\n",
       "      <td>credible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2853</th>\n",
       "      <td>SportsLogos.Net News</td>\n",
       "      <td>Dodgers Set World Series Sales Records</td>\n",
       "      <td>https://news.sportslogos.net/2024/11/01/dodger...</td>\n",
       "      <td>Fri, 01 Nov 2024 19:22:21 +0000</td>\n",
       "      <td>The Los Angeles Dodgers set a series of mercha...</td>\n",
       "      <td>The Los Angeles Dodgers’ string of success did...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>29.478000</td>\n",
       "      <td>credible</td>\n",
       "      <td>35.703940</td>\n",
       "      <td>credible</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2854 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                feed_title                                        entry_title  \\\n",
       "0       www.espn.com - TOP  NBA Power Rankings: Where do all 30 teams stac...   \n",
       "1       www.espn.com - TOP    Braves lefty Sale captures first Cy Young Award   \n",
       "2       www.espn.com - TOP  Clark won't play in Unrivaled league, source says   \n",
       "3       www.espn.com - TOP     Sources: Jets owner suggested benching Rodgers   \n",
       "4       www.espn.com - TOP   Man pleads guilty to murder of three UVA players   \n",
       "...                    ...                                                ...   \n",
       "2849  SportsLogos.Net News  NFL Week 9 Uniform Schedule: Giants, Eagles, P...   \n",
       "2850  SportsLogos.Net News  QMJHL’s Quebec Remparts Pay Homage to Beloved ...   \n",
       "2851  SportsLogos.Net News  Oregon Ducks Unveil Final “Generation O” Unifo...   \n",
       "2852  SportsLogos.Net News  Illinois Fighting Illini Unveil Military Appre...   \n",
       "2853  SportsLogos.Net News             Dodgers Set World Series Sales Records   \n",
       "\n",
       "                                             entry_link  \\\n",
       "0     https://www.espn.com/nba/story/_/id/42468239/n...   \n",
       "1     https://www.espn.com/mlb/story/_/id/42512141/b...   \n",
       "2     https://www.espn.com/wnba/story/_/id/42510300/...   \n",
       "3     https://www.espn.com/nfl/story/_/id/42508628/s...   \n",
       "4     https://www.espn.com/college-football/story/_/...   \n",
       "...                                                 ...   \n",
       "2849  https://news.sportslogos.net/2024/11/03/nfl-we...   \n",
       "2850  https://news.sportslogos.net/2024/11/02/qmjhls...   \n",
       "2851  https://news.sportslogos.net/2024/11/02/oregon...   \n",
       "2852  https://news.sportslogos.net/2024/11/02/illino...   \n",
       "2853  https://news.sportslogos.net/2024/11/01/dodger...   \n",
       "\n",
       "                 entry_published_date  \\\n",
       "0       Wed, 20 Nov 2024 18:27:28 EST   \n",
       "1       Wed, 20 Nov 2024 18:37:45 EST   \n",
       "2       Wed, 20 Nov 2024 18:37:44 EST   \n",
       "3       Wed, 20 Nov 2024 18:37:44 EST   \n",
       "4       Wed, 20 Nov 2024 18:37:44 EST   \n",
       "...                               ...   \n",
       "2849  Sun, 03 Nov 2024 16:25:35 +0000   \n",
       "2850  Sat, 02 Nov 2024 15:00:00 +0000   \n",
       "2851  Sat, 02 Nov 2024 05:03:33 +0000   \n",
       "2852  Sat, 02 Nov 2024 04:02:08 +0000   \n",
       "2853  Fri, 01 Nov 2024 19:22:21 +0000   \n",
       "\n",
       "                                          entry_summary  \\\n",
       "0     Meanwhile, do the Cavs stay on top after a los...   \n",
       "1     Braves pitcher Chris Sale, who went 18-3 with ...   \n",
       "2     Fever star Caitlin Clark will not play in the ...   \n",
       "3     Jets owner Woody Johnson broached the idea of ...   \n",
       "4     A former University of Virginia student could ...   \n",
       "...                                                 ...   \n",
       "2849  Our weekly series that looks at every uniform ...   \n",
       "2850  Jerseys worn Friday vs. Sherbrooke Phoenix ins...   \n",
       "2851  Oregon unveiled the fifth and final combinatio...   \n",
       "2852  Illinois will wear a special helmet design dur...   \n",
       "2853  The Los Angeles Dodgers set a series of mercha...   \n",
       "\n",
       "                                          entry_content publication_date  \\\n",
       "0                               Failed to fetch content       2024-11-20   \n",
       "1                               Failed to fetch content       2024-11-20   \n",
       "2                               Failed to fetch content       2024-11-20   \n",
       "3                               Failed to fetch content       2024-11-20   \n",
       "4                               Failed to fetch content       2024-11-20   \n",
       "...                                                 ...              ...   \n",
       "2849  The midway point of the 2024-25 season is here...              NaN   \n",
       "2850  The Quebec Remparts paid homage to one of thei...              NaN   \n",
       "2851  The Oregon football program unveiled the fifth...              NaN   \n",
       "2852  The Illinois football program will wear a spec...              NaN   \n",
       "2853  The Los Angeles Dodgers’ string of success did...              NaN   \n",
       "\n",
       "     source_credibility  credibility_score credibility_label  \\\n",
       "0                  high          -3.833633        uncredible   \n",
       "1                  high          39.478000          credible   \n",
       "2                  high          39.478000          credible   \n",
       "3                  high          39.478000          credible   \n",
       "4                  high          39.478000          credible   \n",
       "...                 ...                ...               ...   \n",
       "2849            unknown          35.703940          credible   \n",
       "2850            unknown          29.478000          credible   \n",
       "2851            unknown          29.478000          credible   \n",
       "2852            unknown          29.478000          credible   \n",
       "2853            unknown          29.478000          credible   \n",
       "\n",
       "      summary_credibility_score summary_credibility_label  \n",
       "0                    -53.371205                uncredible  \n",
       "1                     83.059578                  credible  \n",
       "2                     51.929879                  credible  \n",
       "3                     45.703940                  credible  \n",
       "4                     45.703940                  credible  \n",
       "...                         ...                       ...  \n",
       "2849                  29.478000                  credible  \n",
       "2850                  35.703940                  credible  \n",
       "2851                  29.478000                  credible  \n",
       "2852                  29.478000                  credible  \n",
       "2853                  35.703940                  credible  \n",
       "\n",
       "[2854 rows x 12 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles_df #labeled dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0d86b0-433e-4ac5-a6ab-d9387ac9e976",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "-get number of credible labels vs uncredible counts\n",
    "\n",
    "-number of each source (using feed title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8772958-80fe-4806-a4d4-1d1575f98e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dbdf49-1311-4a40-b03b-68f34a343e6e",
   "metadata": {},
   "source": [
    "## Train a model using glove and additional features, source split to eval if model can generalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bd5c00-9607-4b3f-b239-f1bba8c07deb",
   "metadata": {},
   "source": [
    "### Turn each headline into a glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025818fa-d3b3-4ce6-b060-e3211657a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#article vec is mean of token's embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cceebc-80fb-44aa-8851-67736714e38a",
   "metadata": {},
   "source": [
    "### Prep the data into train and test by source split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f301db0a-a60e-4e25-9183-2350b07f4a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(headline): #features not in heuristic for labels \n",
    "    return np.array([\n",
    "        word_count(headline),\n",
    "        char_count(headline),\n",
    "        avg_word_length(headline),\n",
    "        all_capitalized_word_count(headline),\n",
    "        punctuation_count(headline),\n",
    "        readability_score(headline),\n",
    "        count_quotation_pairs(headline),\n",
    "        variety_of_vocabularity(headline),\n",
    "        count_sensational_words(headline),\n",
    "        count_absolute_words(headline),\n",
    "        objective_score(headline),\n",
    "        sentiment(headline)\n",
    "    ])\n",
    "\n",
    "data = pd.read_csv('labeled_RSS_posts.csv') #get labeled data\n",
    "\n",
    "#Split up sources randomly, aka by feed_title\n",
    "sources = data[\"feed_title\"].unique()\n",
    "train_sources, test_sources = train_test_split(sources, test_size=0.2, random_state=42)\n",
    "\n",
    "#Create train and test splits based on sources\n",
    "train_data = data[data[\"feed_title\"].isin(train_sources)]\n",
    "test_data = data[data[\"feed_title\"].isin(test_sources)]   ## ADJUST TEST SIZE BASED ON ENTRIES IN EACH OF THESE??\n",
    "#---------------------------------------\n",
    "\n",
    "# #Apply embeddings and feature calculations to each headline, combine features and glove embeddings\n",
    "# X_train = np.array([\n",
    "#     np.hstack((GLOVE_FUNCTION(row[\"entry_title\"], glove_model), extract_features(row[\"entry_title\"])))\n",
    "#     for index, row in train_data.iterrows()\n",
    "# ])\n",
    "\n",
    "# # Prepare testing data\n",
    "# X_test = np.array([\n",
    "#     np.hstack((GLOVE_FUNCTION(row[\"entry_title\"], glove_model), extract_features(row[\"entry_title\"])))\n",
    "#     for index, row in test_data.iterrows()\n",
    "# ])\n",
    "\n",
    "y_train = (train_data[\"credibility_label\"] == \"credible\").astype(int).values #1=credible, 0= uncredible\n",
    "y_test = (test_data[\"credibility_label\"] == \"credible\").astype(int).values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1130d-4e1e-4741-8d6a-21bfc2aff250",
   "metadata": {},
   "source": [
    "### Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5dce69-3072-465b-872a-b74e062fc0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
