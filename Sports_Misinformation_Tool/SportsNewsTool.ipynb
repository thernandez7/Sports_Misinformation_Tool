{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72517dfa-1720-49e6-b55a-f15cdf181498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: in the working copy of 'Sports_Misinformation_Tool/.ipynb_checkpoints/Combine_RSS_CSVs-checkpoint.ipynb', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Sports_Misinformation_Tool/.ipynb_checkpoints/SportsNewsTool-checkpoint.ipynb', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Sports_Misinformation_Tool/All_RSS_articles.csv', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Sports_Misinformation_Tool/Combine_RSS_CSVs.ipynb', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Sports_Misinformation_Tool/SportsNewsTool.ipynb', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Sports_Misinformation_Tool/.ipynb_checkpoints/Euro_tweets_initial_hueristic_sample-checkpoint.csv', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Sports_Misinformation_Tool/.ipynb_checkpoints/heuristic_analysis-checkpoint.ipynb', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Sports_Misinformation_Tool/.ipynb_checkpoints/real_european-checkpoint.csv', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Sports_Misinformation_Tool/Euro_tweets_initial_hueristic_exploration.csv', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Sports_Misinformation_Tool/Euro_tweets_initial_hueristic_sample.csv', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Sports_Misinformation_Tool/RSS_sports_feeds_11-17.csv', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Sports_Misinformation_Tool/fake_european.csv', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Sports_Misinformation_Tool/heuristic_analysis.ipynb', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Sports_Misinformation_Tool/real_european.csv', LF will be replaced by CRLF the next time Git touches it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main fa9692b] ran heuristic features on euro dataset and updated RSS collection\n",
      " 15 files changed, 169252 insertions(+), 148 deletions(-)\n",
      " create mode 100644 Sports_Misinformation_Tool/.ipynb_checkpoints/Euro_tweets_initial_hueristic_exploration-checkpoint.csv\n",
      " create mode 100644 Sports_Misinformation_Tool/.ipynb_checkpoints/Euro_tweets_initial_hueristic_sample-checkpoint.csv\n",
      " create mode 100644 Sports_Misinformation_Tool/.ipynb_checkpoints/heuristic_analysis-checkpoint.ipynb\n",
      " create mode 100644 Sports_Misinformation_Tool/.ipynb_checkpoints/real_european-checkpoint.csv\n",
      " create mode 100644 Sports_Misinformation_Tool/Euro_tweets_initial_hueristic_exploration.csv\n",
      " create mode 100644 Sports_Misinformation_Tool/Euro_tweets_initial_hueristic_sample.csv\n",
      " create mode 100644 Sports_Misinformation_Tool/RSS_sports_feeds_11-17.csv\n",
      " create mode 100644 Sports_Misinformation_Tool/fake_european.csv\n",
      " create mode 100644 Sports_Misinformation_Tool/heuristic_analysis.ipynb\n",
      " create mode 100644 Sports_Misinformation_Tool/real_european.csv\n",
      "Already up to date.\n",
      "branch 'main' set up to track 'origin/main'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To github.com:thernandez7/Sports_Misinformation_Tool.git\n",
      "   143b2a3..fa9692b  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git commit -m \"ran heuristic features on euro dataset and updated RSS collection\"\n",
    "!git pull\n",
    "!git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83bc5e79-db2d-4764-81e0-c13cc4ca8fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8446acb8-a93e-418e-a0ec-3ef05f7389b8",
   "metadata": {},
   "source": [
    "## Sports Misinformation Classification Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc105918-5fb3-432a-b47f-f3717be38100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82e525a-1cf9-47be-8298-238a25ce2667",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Connect to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fde6c4b-1920-4bb2-846f-61455f869aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database successfully!\n"
     ]
    }
   ],
   "source": [
    "username = 'root'  \n",
    "password = 'password'\n",
    "host = 'localhost' \n",
    "database = 'sports_news_db'\n",
    "\n",
    "connection = mysql.connector.connect(\n",
    "    host=host,\n",
    "    user=username,\n",
    "    password=password,\n",
    "    database=database\n",
    ")\n",
    "\n",
    "if connection.is_connected():\n",
    "    print(\"Connected to the database successfully!\")\n",
    "\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{username}:{password}@{host}/{database}\", echo=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70ed146f-9fe7-4370-a3d2-898811f3e183",
   "metadata": {},
   "source": [
    "#### Use this format to insert into the database\n",
    "\n",
    "INSERT INTO articles (team_or_player, source, publication_date, content, trust_score, classification, link)\n",
    "VALUES\n",
    "('New York Yankees, Los Angeles Lakers', 'ESPN', '2024-10-27', 'Yankees article content example.', 85.00, 'real', 'https://example.com/article1'),\n",
    "('Los Angeles Lakers', 'Twitter', '2024-10-27', 'Lakers article content example.', 60.00, 'fake', 'https://example.com/article2');\n",
    "\n",
    "\n",
    "#### Table fields \n",
    "Table Name: articles\n",
    "\n",
    "Fields: \n",
    "\n",
    "     id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "     \n",
    "     team_or_player VARCHAR(500), (This will be the article title that we can query- usually includes teams or names in it)\n",
    "     \n",
    "     source VARCHAR(200),\n",
    "     \n",
    "     publication_date DATE,\n",
    "     \n",
    "     content TEXT,\n",
    "     \n",
    "     trust_score DECIMAL(5, 2), \n",
    "     \n",
    "     classification ENUM('credible', 'uncredible', 'unknown') DEFAULT 'unknown',\n",
    "\n",
    "     link VARCHAR(255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359168cc-f336-4972-b580-38575b45a13e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Simulate Tool Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9687e8d1-8c66-4df1-80cf-4bcf6f33805c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the team or player's name:  x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No articles found for x.\n"
     ]
    }
   ],
   "source": [
    "team_or_player = input(\"Enter the team or player's name: \")\n",
    "\n",
    "query = f\"SELECT * FROM articles WHERE team_or_player LIKE '%{team_or_player}%'\" #search for entered name/team in the title \n",
    "df_result = pd.read_sql(query, con=engine, params={'team_or_player': team_or_player})\n",
    "\n",
    "#get results\n",
    "if not df_result.empty:\n",
    "    print(f\"Articles related to {team_or_player}:\")\n",
    "    display(df_result)\n",
    "else:\n",
    "    print(f\"No articles found for {team_or_player}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cee353-47af-48c3-a0eb-cf19ddcbc995",
   "metadata": {},
   "source": [
    "### Get Article Entries from RSS Feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06ea0136-a9b1-4fac-94f6-1949deec9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reddit API - collected 1876 posts\n",
    "# import csv\n",
    "# import praw\n",
    "# from datetime import datetime\n",
    "\n",
    "\n",
    "# reddit = praw.Reddit(\n",
    "#     client_id='w-kwRyPigyjYeG9DOiDc8g', \n",
    "#     client_secret='ZeDsvNH2YlpVH7F9wEWPkt5wkjLzqA',  \n",
    "#     user_agent='sports_misinfo_script'  \n",
    "# )\n",
    "\n",
    "# subreddit = reddit.subreddit('sports+fantasyfootball') #2 subreddits \n",
    "\n",
    "# recent_posts = []\n",
    "# for post in subreddit.new(limit= 5000):\n",
    "#     created_date = datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "#     recent_posts.append({\n",
    "#         'title': post.title,\n",
    "#         'score': post.score,\n",
    "#         'url': post.url,\n",
    "#         'id': post.id,\n",
    "#         'author': str(post.author),\n",
    "#         'text': post.selftext,\n",
    "#         'created_date': created_date,\n",
    "#         'num_comments': post.num_comments,\n",
    "#         'subreddit': post.subreddit.display_name,\n",
    "\n",
    "#     })\n",
    "\n",
    "# print(f\"Fetched {len(recent_posts)} posts\")\n",
    "\n",
    "# for i, post in enumerate(recent_posts[:5]):\n",
    "#     print(f\"{i+1}. Title: {post['title']} | Score: {post['score']} | URL: {post['url']}\")\n",
    "\n",
    "\n",
    "# #-------------------------------\n",
    "# #Save the data to a csv file\n",
    "# fieldnames = ['title', 'score', 'url', 'id', 'author', 'text', 'created_date', 'num_comments', 'subreddit']\n",
    "\n",
    "# with open('recent_sports_reddit_posts.csv', mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "#     writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "#     writer.writeheader()\n",
    "\n",
    "#     for post in recent_posts:\n",
    "#         writer.writerow(post)\n",
    "\n",
    "# print(\"Data successfully saved to 'recent_sports_reddit_posts.csv'\")\n",
    "# print(\"Data saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "334d2970-7ee0-47e1-80d9-e01ae15fa4a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape content from https://www.espn.com/college-football/story/_/id/42447741/oregon-remains-unanimous-no-1-ap-top-25-notre-dame-6: 403 Client Error: Forbidden for url: https://www.espn.com/college-football/story/_/id/42447741/oregon-remains-unanimous-no-1-ap-top-25-notre-dame-6\n",
      "Failed to scrape content from https://www.espn.com/nba/story/_/id/42447996/hornets-lamelo-ball-fined-100k-using-anti-gay-term: 403 Client Error: Forbidden for url: https://www.espn.com/nba/story/_/id/42447996/hornets-lamelo-ball-fined-100k-using-anti-gay-term\n",
      "Failed to scrape content from https://www.espn.com/college-football/story/_/id/42447431/sources-top-2025-qb-julian-lewis-decommits-usc: 403 Client Error: Forbidden for url: https://www.espn.com/college-football/story/_/id/42447431/sources-top-2025-qb-julian-lewis-decommits-usc\n",
      "Failed to scrape content from https://www.espn.com/nba/story/_/id/42449490/celtics-celebrate-nba-title-white-house-thursday: 403 Client Error: Forbidden for url: https://www.espn.com/nba/story/_/id/42449490/celtics-celebrate-nba-title-white-house-thursday\n",
      "Failed to scrape content from https://www.espn.com/golf/story/_/id/42448704/new-father-rafael-campos-emotional-first-pga-tour-win: 403 Client Error: Forbidden for url: https://www.espn.com/golf/story/_/id/42448704/new-father-rafael-campos-emotional-first-pga-tour-win\n",
      "Failed to scrape content from https://www.espn.com/nba/story/_/id/42445812/reports-nba-air-espn-abc-next-season: 403 Client Error: Forbidden for url: https://www.espn.com/nba/story/_/id/42445812/reports-nba-air-espn-abc-next-season\n",
      "Failed to scrape content from https://www.espn.com/college-football/story/_/id/42445530/temple-fires-coach-stan-drayton-day-ot-victory: 403 Client Error: Forbidden for url: https://www.espn.com/college-football/story/_/id/42445530/temple-fires-coach-stan-drayton-day-ot-victory\n",
      "Failed to scrape content from https://www.espn.com/college-sports/recruiting/football/story/_/id/42448614/anthony-rogers-no-97-recruit-2025-decommits-alabama: 403 Client Error: Forbidden for url: https://www.espn.com/college-sports/recruiting/football/story/_/id/42448614/anthony-rogers-no-97-recruit-2025-decommits-alabama\n",
      "Failed to scrape content from https://www.espn.com/nfl/story/_/id/42365446/can-chiefs-go-undefeated-record-winning-ugly-roster-flaws-super-bowl-2024-mahomes: 403 Client Error: Forbidden for url: https://www.espn.com/nfl/story/_/id/42365446/can-chiefs-go-undefeated-record-winning-ugly-roster-flaws-super-bowl-2024-mahomes\n",
      "Failed to scrape content from https://www.espn.com/nfl/story/_/id/42368149/buffalo-bills-player-led-playcalling-building-team-confidence-undefeated-kansas-city-chiefs: 403 Client Error: Forbidden for url: https://www.espn.com/nfl/story/_/id/42368149/buffalo-bills-player-led-playcalling-building-team-confidence-undefeated-kansas-city-chiefs\n",
      "Failed to scrape content from https://www.espn.com/college-football/story/_/page/gamedayfinal111624/college-football-week-12-highlights-top-plays-games-takeaways-2024: 403 Client Error: Forbidden for url: https://www.espn.com/college-football/story/_/page/gamedayfinal111624/college-football-week-12-highlights-top-plays-games-takeaways-2024\n",
      "Failed to scrape content from https://www.espn.com/college-football/story/_/id/42393651/2024-college-football-power-rankings-week-12-top-25-teams: 403 Client Error: Forbidden for url: https://www.espn.com/college-football/story/_/id/42393651/2024-college-football-power-rankings-week-12-top-25-teams\n",
      "Failed to scrape content from https://www.espn.com/wnba/story/_/id/42370750/wnba-mock-draft-2025-paige-bueckers-draft-lottery-los-angeles-sparks-dallas-wings-washington-mystics-chicago-sky: 403 Client Error: Forbidden for url: https://www.espn.com/wnba/story/_/id/42370750/wnba-mock-draft-2025-paige-bueckers-draft-lottery-los-angeles-sparks-dallas-wings-washington-mystics-chicago-sky\n",
      "Failed to scrape content from https://www.espn.com/nhl/story/_/id/42390646/nhl-power-rankings-2024-2025-best-teams-standings-fantasy-hockey: 403 Client Error: Forbidden for url: https://www.espn.com/nhl/story/_/id/42390646/nhl-power-rankings-2024-2025-best-teams-standings-fantasy-hockey\n",
      "Failed to scrape content from https://www.espn.com/mens-college-basketball/story/_/id/42391361/judging-5-early-season-mens-college-basketball-overreactions-kentucky-flagg-arkansas: 403 Client Error: Forbidden for url: https://www.espn.com/mens-college-basketball/story/_/id/42391361/judging-5-early-season-mens-college-basketball-overreactions-kentucky-flagg-arkansas\n",
      "Failed to scrape content from https://www.espn.com/mens-college-basketball/story/_/id/42371393/mens-college-basketball-power-rankings-2024-25-kansas-gonzaga-auburn-kentucky: 403 Client Error: Forbidden for url: https://www.espn.com/mens-college-basketball/story/_/id/42371393/mens-college-basketball-power-rankings-2024-25-kansas-gonzaga-auburn-kentucky\n",
      "Failed to scrape content from https://www.espn.com/boxing/story/_/id/42419370/after-mike-tyson-jake-paul-surprised-see-boxing-spectacle-again: 403 Client Error: Forbidden for url: https://www.espn.com/boxing/story/_/id/42419370/after-mike-tyson-jake-paul-surprised-see-boxing-spectacle-again\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42428167/washington-spirit-gotham-fc-shootout-nwsl-semifinal-championship: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42428167/washington-spirit-gotham-fc-shootout-nwsl-semifinal-championship\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42430199/usmnt-tim-ream-tim-weah-back-copa-america-red-card: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42430199/usmnt-tim-ream-tim-weah-back-copa-america-red-card\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42423254/cristiano-ronaldo-makes-retirement-hint-record-portugal-win: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42423254/cristiano-ronaldo-makes-retirement-hint-record-portugal-win\n",
      "Failed to scrape content from https://www.espn.com/soccer/report/_/gameId/699011: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/report/_/gameId/699011\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42433979/uswnt-lily-yohannes-tough-week-netherlands-ajax-coach: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42433979/uswnt-lily-yohannes-tough-week-netherlands-ajax-coach\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42425280/bayern-munich-extend-mala-grohs-contract-cancer-diagnosis: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42425280/bayern-munich-extend-mala-grohs-contract-cancer-diagnosis\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42431109/hungary-assistant-coach-adam-szalai-netherlands-nations-league: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42431109/hungary-assistant-coach-adam-szalai-netherlands-nations-league\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42427478/man-united-confirm-ruben-amorim-coaching-staff: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42427478/man-united-confirm-ruben-amorim-coaching-staff\n",
      "Failed to scrape content from https://www.espn.com/college-football/story/_/id/42430837/college-football-trolls-week-12: 403 Client Error: Forbidden for url: https://www.espn.com/college-football/story/_/id/42430837/college-football-trolls-week-12\n",
      "Failed to scrape content from https://www.espn.com/college-football/story/_/id/42433526/colorado-buffaloes-peggy-coppom-happy-birthday-100-birthday: 403 Client Error: Forbidden for url: https://www.espn.com/college-football/story/_/id/42433526/colorado-buffaloes-peggy-coppom-happy-birthday-100-birthday\n",
      "Failed to scrape content from https://www.espn.com/espn/betting/story/_/id/42376299/2024-nfl-season-betting-odds-daniel-dopp-liz-loza-football-props-pop-week-11: 403 Client Error: Forbidden for url: https://www.espn.com/espn/betting/story/_/id/42376299/2024-nfl-season-betting-odds-daniel-dopp-liz-loza-football-props-pop-week-11\n",
      "Failed to scrape content from https://www.espn.com/espn/betting/story/_/id/42394173/2024-nba-betting-futures-rookie-year-jared-mccain-zaccharie-risacher-zach-edey: 403 Client Error: Forbidden for url: https://www.espn.com/espn/betting/story/_/id/42394173/2024-nba-betting-futures-rookie-year-jared-mccain-zaccharie-risacher-zach-edey\n",
      "Failed to scrape content from https://www.espn.com/boxing/story/_/id/42422294/katie-taylor-amanda-serrano-jake-paul-mike-tyson-boxing: 403 Client Error: Forbidden for url: https://www.espn.com/boxing/story/_/id/42422294/katie-taylor-amanda-serrano-jake-paul-mike-tyson-boxing\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42388116/premier-league-manchester-city-chelsea-arsenal-tottenham-manchester-united-january-transfer: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42388116/premier-league-manchester-city-chelsea-arsenal-tottenham-manchester-united-january-transfer\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42431717/soccer-transfer-rumors-news-lafc-circle-griezmann-eyes-atleti-exit: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42431717/soccer-transfer-rumors-news-lafc-circle-griezmann-eyes-atleti-exit\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#add urls to this list to parse\n",
    "url_list = [\n",
    "        #MBFC High Credibility\n",
    "        \"https://www.espn.com/espn/rss/news\", #ESPN top headlines\n",
    "        \"https://deadspin.com/rss/\", #Deadspin \n",
    "        \"https://feeds.abcnews.com/abcnews/sportsheadlines\" #ABC news- espn sports\n",
    "        \"https://rss.nytimes.com/services/xml/rss/nyt/Sports.xml\", #NYT\n",
    "        \"https://www.nytimes.com/athletic/rss/news/\"#The Athletic- acquired by NYT\n",
    "        \"https://www.mlb.com/feeds/news/rss.xml\", #MLB news \n",
    "        \"https://www.reutersagency.com/feed/?best-topics=sports&post_type=best\", # Reuters\n",
    "        \"https://sports.yahoo.com/rss/\", #Yahoo News\n",
    "        \"https://www.cbssports.com/rss/headlines/\" #CBS sports general headlines \n",
    "\n",
    "        #MBFC questionable sources or medium credibility\n",
    "        \"https://notthebee.com/feed\", #not the bee\n",
    "        \"https://uproxx.com/sports/feed/\", #uproxx   \n",
    "        \"https://www.vibe.com/c/news/sports/feed/\", #The vibe - medium cred \n",
    "        \"https://moxie.foxnews.com/google-publisher/sports.xml\", #fox news \n",
    "        \"https://nypost.com/sports/feed/\" #NY post - medium cred\n",
    "\n",
    "        #NO MBFC RATING\n",
    "        \"https://www.sportscollectorsdaily.com/feed/\", #Sports Collectors Daily \n",
    "        \"https://news.sportslogos.net/feed/\", #SportsLogos.Net\n",
    "        \"https://www.sportingnews.com/us/rss\", #sportingnews.com\n",
    "        \"https://www.sportskeeda.com/feed\",#sportskeeda.com\n",
    "        \"https://sportsweez.com/feed/\", #sportsweez\n",
    "        \"https://sportsbrackets.net/feed/\", #sportsbracket\n",
    "        \"https://21sports.com/feed/\", # 21 sports\n",
    "        \"https://www.essentiallysports.com/feed/\", #essentiallysports\n",
    "        \"https://boxingnewsonline.net/feed/\", #boxing news\n",
    "        \"https://www.thecoldwire.com/feed/\", #the cold wire\n",
    "        \"https://sportsdark.com/feed/\", #sports dark\n",
    "]\n",
    "    \n",
    "entries = [] #list of dictionaries\n",
    "\n",
    "#--------------DEFINE FUNCTION TO SCRAPE-------------------\n",
    "def scrape_article_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        #extract content from common tags\n",
    "        article_body = (\n",
    "            soup.find(\"article\") or\n",
    "            soup.find(\"div\", {\"class\": \"post-content\"}) or\n",
    "            soup.find(\"div\", class_=\"article-body\") or\n",
    "            soup.find(\"div\", class_=\"article-content\") or\n",
    "            soup.find(\"section\", class_=\"article-section\") or\n",
    "            soup.find(\"div\", class_=\"main-content\") or\n",
    "            soup.find(\"div\", class_=\"content-body\")\n",
    "        )\n",
    "        \n",
    "        if article_body:\n",
    "            paragraphs = article_body.find_all(\"p\")\n",
    "        else:\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "\n",
    "        #join all paragraphs into a single string\n",
    "        article_content = \" \".join(p.get_text() for p in paragraphs)\n",
    "        return article_content.strip() if article_content else \"No content found\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape content from {url}: {e}\")\n",
    "        return \"Failed to fetch content\"\n",
    "\n",
    "\n",
    "#run the function to collect feeds and scrape\n",
    "for url in url_list:\n",
    "    feed = feedparser.parse(url)\n",
    "    #feed_title= feed.feed.title\n",
    "    feed_title = getattr(feed.feed, \"title\", None)\n",
    "\n",
    "    \n",
    "    for entry in feed.entries:\n",
    "        entry_title = getattr(entry, \"title\", None)\n",
    "        entry_link = getattr(entry, \"link\", None)\n",
    "        entry_published_date = getattr(entry, \"published\", None)\n",
    "        entry_summary = getattr(entry, \"summary\", None)\n",
    "        entry_content = scrape_article_content(entry_link) #scrape\n",
    "        \n",
    "        entries.append({\n",
    "            \"feed_title\": feed_title,\n",
    "            \"entry_title\": entry_title,\n",
    "            \"entry_link\": entry_link,\n",
    "            \"entry_published_date\": entry_published_date,\n",
    "            \"entry_summary\": entry_summary,\n",
    "            \"entry_content\": entry_content,\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(entries)\n",
    "#print(df)\n",
    "\n",
    "df.to_csv('RSS_sports_feeds_11-17.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b17ce3-4167-4ee2-b75b-0e30f8d21697",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Attempt to Scrape Links without RSS Feeds- not very effective "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f9a3d96-4201-4150-8ecb-b14e133f886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "\n",
    "# uncredible_urls = [\n",
    "#     \"https://www.tellerreport.com/sports\",\n",
    "#     \"https://www.newsbreak.com/mountain-view-ca-sports\",\n",
    "#     \"https://newsrnd.com/sports\",\n",
    "#     \"https://baltimorecitywire.com/stories/tag/53-sports\"\n",
    "# ]\n",
    "\n",
    "# # Function to scrape article details\n",
    "# def scrape_article(url):\n",
    "#     try:\n",
    "#         response = requests.get(url, timeout=10)\n",
    "#         response.raise_for_status()\n",
    "#         soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "#         # Extract the article title\n",
    "#         title = soup.find(\"h1\").get_text() if soup.find(\"h1\") else soup.title.get_text()\n",
    "\n",
    "#         # Extract the publication date (common in <time> or meta tags)\n",
    "#         date = soup.find(\"time\")\n",
    "#         if date:\n",
    "#             publication_date = date.get(\"datetime\") or date.get_text()\n",
    "#         else:\n",
    "#             date_meta = soup.find(\"meta\", {\"name\": \"article:published_time\"})\n",
    "#             publication_date = date_meta[\"content\"] if date_meta else \"No date found\"\n",
    "\n",
    "#         # Extract the article content\n",
    "#         content_container = (\n",
    "#             soup.find(\"article\") or\n",
    "#             soup.find(\"div\", class_=[\"post-content\", \"article-body\", \"article-content\", \"content-body\"])\n",
    "#         )\n",
    "#         if content_container:\n",
    "#             paragraphs = content_container.find_all(\"p\")\n",
    "#         else:\n",
    "#             paragraphs = soup.find_all(\"p\")\n",
    "\n",
    "#         content = \"\\n\".join(p.get_text() for p in paragraphs)\n",
    "\n",
    "#         return {\n",
    "#             \"Title\": title.strip(),\n",
    "#             \"Publication Date\": publication_date.strip(),\n",
    "#             \"Content\": content.strip()[:500] + \"...\",\n",
    "#             \"Link\": url\n",
    "#         }\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to scrape {url}: {e}\")\n",
    "#         return None\n",
    "\n",
    "# articles = []\n",
    "# for url in uncredible_urls:\n",
    "#     print(f\"Scraping: {url}\")\n",
    "#     article_details = scrape_article(url)\n",
    "#     if article_details:\n",
    "#         articles.append(article_details)\n",
    "\n",
    "# for article in articles:\n",
    "#     print(\"\\n--- Article ---\")\n",
    "#     print(f\"Title: {article['Title']}\")\n",
    "#     print(f\"Date: {article['Publication Date']}\")\n",
    "#     print(f\"Content Preview: {article['Content']}\")\n",
    "#     print(f\"Link: {article['Link']}\\n\")\n",
    "\n",
    "# df = pd.DataFrame(articles)\n",
    "# output_file = \"scraped_uncredible_articles.csv\"\n",
    "# df.to_csv(output_file, index=False)\n",
    "# print(f\"\\nResults saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f53448-164e-46d8-a16d-02c3dbaa7d0c",
   "metadata": {},
   "source": [
    "### Format DF to match DB table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b6b1163a-ce9f-47f0-9c15-d7e5cceaf6e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tizia\\Anaconda4\\Lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "df['publication_date'] = df['entry_published_date'].apply(lambda x: parser.parse(x).date()) #parse different date formats to date object format\n",
    "dbdf = pd.read_csv('RSS_sports_feeds.csv')\n",
    "dbdf['team_or_player'] = df['entry_title']\n",
    "dbdf['source'] = df['feed_title']\n",
    "dbdf['publication_date'] = df['publication_date'] \n",
    "dbdf['content'] = df['entry_summary']\n",
    "dbdf['trust_score'] = 0.00  #default\n",
    "dbdf['classification'] = 'unknown' #default\n",
    "dbdf['link'] = df['entry_link']\n",
    "\n",
    "#make a new df in the format of the DB table for easy inserting\n",
    "sports_DB_df = dbdf[['team_or_player', 'source', 'publication_date', 'content', 'trust_score', 'classification', 'link']]\n",
    "#save to a new CSV \n",
    "sports_DB_df.to_csv('formatted_sports_posts_for_DB.csv', index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b923503-bb70-46cc-ad0c-a13fb1dbdd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_euro_df = pd.read_csv('real_european.csv')\n",
    "fake_euro_df = pd.read_csv('fake_european.csv')\n",
    "\n",
    "real_euro_df[\"label\"] = \"True\"\n",
    "fake_euro_df[\"label\"] = \"False\"\n",
    "\n",
    "real_tweets= real_euro_df[\"tweet\"].dropna()\n",
    "fake_tweets= fake_euro_df[\"tweet\"].dropna()\n",
    "\n",
    "# Compute features for true and false tweets\n",
    "euro_true_features_df = compute_features(real_tweets)\n",
    "euro_false_features_df = compute_features(fake_tweets)\n",
    "\n",
    "euro_true_features_df[\"label\"] = \"True\"\n",
    "euro_false_features_df[\"label\"] = \"False\"\n",
    "\n",
    "euro_combined_df = pd.concat([euro_true_features_df, euro_false_features_df], ignore_index=True)\n",
    "euro_combined_df.to_csv(\"Euro_tweets_initial_hueristic_exploration.csv\")\n",
    "print(\"Data saved to 'Euro_tweets_initial_hueristic_exploration.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c282d88e-3a8a-4283-af31-f011ec789135",
   "metadata": {},
   "source": [
    "### Define labeling heuristic and trust score to update sports_DB_df with ground truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2a5014-fff8-4491-a937-cadbd2b697ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
