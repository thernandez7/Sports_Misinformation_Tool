{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "72517dfa-1720-49e6-b55a-f15cdf181498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "# !git add .\n",
    "# !git commit -m \"added sentiment analysis different tool evaluation\"\n",
    "#!git pull\n",
    "# !git push -u origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8446acb8-a93e-418e-a0ec-3ef05f7389b8",
   "metadata": {},
   "source": [
    "## Sports Misinformation Classification Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82e525a-1cf9-47be-8298-238a25ce2667",
   "metadata": {},
   "source": [
    "### Connect to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065e181a-e5fd-4418-8be0-3378790be6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database successfully!\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "username = 'root'  \n",
    "password = 'password'\n",
    "host = 'localhost' \n",
    "database = 'sports_news_db'\n",
    "\n",
    "connection = mysql.connector.connect(\n",
    "    host=host,\n",
    "    user=username,\n",
    "    password=password,\n",
    "    database=database\n",
    ")\n",
    "\n",
    "if connection.is_connected():\n",
    "    print(\"Connected to the database successfully!\")\n",
    "\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{username}:{password}@{host}/{database}\", echo=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70ed146f-9fe7-4370-a3d2-898811f3e183",
   "metadata": {},
   "source": [
    "#### Use this format to insert into the database\n",
    "\n",
    "INSERT INTO articles (team_or_player, source, publication_date, content, trust_score, classification, link)\n",
    "VALUES\n",
    "('New York Yankees, Los Angeles Lakers', 'ESPN', '2024-10-27', 'Yankees article content example.', 85.00, 'real', 'https://example.com/article1'),\n",
    "('Los Angeles Lakers', 'Twitter', '2024-10-27', 'Lakers article content example.', 60.00, 'fake', 'https://example.com/article2');\n",
    "\n",
    "\n",
    "#### Table fields \n",
    "Table Name: articles\n",
    "\n",
    "Fields: \n",
    "\n",
    "     id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "     \n",
    "     team_or_player VARCHAR(500), (This will be the article title that we can query- usually includes teams or names in it)\n",
    "     \n",
    "     source VARCHAR(200),\n",
    "     \n",
    "     publication_date DATE,\n",
    "     \n",
    "     content TEXT,\n",
    "     \n",
    "     trust_score DECIMAL(5, 2), \n",
    "     \n",
    "     classification ENUM('credible', 'uncredible', 'unknown') DEFAULT 'unknown',\n",
    "\n",
    "     link VARCHAR(255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359168cc-f336-4972-b580-38575b45a13e",
   "metadata": {},
   "source": [
    "### Simulate Tool Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9687e8d1-8c66-4df1-80cf-4bcf6f33805c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the team or player's name:  x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No articles found for x.\n"
     ]
    }
   ],
   "source": [
    "team_or_player = input(\"Enter the team or player's name: \")\n",
    "\n",
    "query = f\"SELECT * FROM articles WHERE team_or_player LIKE '%{team_or_player}%'\" #search for entered name/team in the title \n",
    "df_result = pd.read_sql(query, con=engine, params={'team_or_player': team_or_player})\n",
    "\n",
    "#get results\n",
    "if not df_result.empty:\n",
    "    print(f\"Articles related to {team_or_player}:\")\n",
    "    display(df_result)\n",
    "else:\n",
    "    print(f\"No articles found for {team_or_player}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cee353-47af-48c3-a0eb-cf19ddcbc995",
   "metadata": {},
   "source": [
    "### Get Article Entries from RSS Feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "06ea0136-a9b1-4fac-94f6-1949deec9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reddit API - collected 1876 posts\n",
    "# import csv\n",
    "# import praw\n",
    "# from datetime import datetime\n",
    "\n",
    "\n",
    "# reddit = praw.Reddit(\n",
    "#     client_id='w-kwRyPigyjYeG9DOiDc8g', \n",
    "#     client_secret='ZeDsvNH2YlpVH7F9wEWPkt5wkjLzqA',  \n",
    "#     user_agent='sports_misinfo_script'  \n",
    "# )\n",
    "\n",
    "# subreddit = reddit.subreddit('sports+fantasyfootball') #2 subreddits \n",
    "\n",
    "# recent_posts = []\n",
    "# for post in subreddit.new(limit= 5000):\n",
    "#     created_date = datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "#     recent_posts.append({\n",
    "#         'title': post.title,\n",
    "#         'score': post.score,\n",
    "#         'url': post.url,\n",
    "#         'id': post.id,\n",
    "#         'author': str(post.author),\n",
    "#         'text': post.selftext,\n",
    "#         'created_date': created_date,\n",
    "#         'num_comments': post.num_comments,\n",
    "#         'subreddit': post.subreddit.display_name,\n",
    "\n",
    "#     })\n",
    "\n",
    "# print(f\"Fetched {len(recent_posts)} posts\")\n",
    "\n",
    "# for i, post in enumerate(recent_posts[:5]):\n",
    "#     print(f\"{i+1}. Title: {post['title']} | Score: {post['score']} | URL: {post['url']}\")\n",
    "\n",
    "\n",
    "# #-------------------------------\n",
    "# #Save the data to a csv file\n",
    "# fieldnames = ['title', 'score', 'url', 'id', 'author', 'text', 'created_date', 'num_comments', 'subreddit']\n",
    "\n",
    "# with open('recent_sports_reddit_posts.csv', mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "#     writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "#     writer.writeheader()\n",
    "\n",
    "#     for post in recent_posts:\n",
    "#         writer.writerow(post)\n",
    "\n",
    "# print(\"Data successfully saved to 'recent_sports_reddit_posts.csv'\")\n",
    "# print(\"Data saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d2970-7ee0-47e1-80d9-e01ae15fa4a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#add urls to this list to parse\n",
    "url_list = [\n",
    "        \"https://moxie.foxnews.com/google-publisher/sports.xml\", #fox news \n",
    "        \"https://www.sportscollectorsdaily.com/feed/\", #Sports Collectors Daily \n",
    "        \"https://www.espn.com/espn/rss/news\", #ESPN top headlines\n",
    "        \"https://deadspin.com/rss/\", #Deadspin \n",
    "        \"https://news.sportslogos.net/feed/\", #SportsLogos.Net\n",
    "\n",
    "        #questionable sources or medium credibility\n",
    "        \"https://notthebee.com/feed\", #not the bee\n",
    "        \"https://uproxx.com/sports/feed/\", #uproxx   \n",
    "        \"https://www.vibe.com/c/news/sports/feed/\", #The vibe - medium cred \n",
    "]\n",
    "    \n",
    "entries = [] #list of dictionaries\n",
    "\n",
    "#--------------DEFINE FUNCTION TO SCRAPE-------------------\n",
    "def scrape_article_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        #extract content from common tags\n",
    "        article_body = (\n",
    "            soup.find(\"article\") or\n",
    "            soup.find(\"div\", {\"class\": \"post-content\"}) or\n",
    "            soup.find(\"div\", class_=\"article-body\") or\n",
    "            soup.find(\"div\", class_=\"article-content\") or\n",
    "            soup.find(\"section\", class_=\"article-section\") or\n",
    "            soup.find(\"div\", class_=\"main-content\") or\n",
    "            soup.find(\"div\", class_=\"content-body\")\n",
    "        )\n",
    "        \n",
    "        if article_body:\n",
    "            paragraphs = article_body.find_all(\"p\")\n",
    "        else:\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "\n",
    "        #join all paragraphs into a single string\n",
    "        article_content = \" \".join(p.get_text() for p in paragraphs)\n",
    "        return article_content.strip() if article_content else \"No content found\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape content from {url}: {e}\")\n",
    "        return \"Failed to fetch content\"\n",
    "\n",
    "\n",
    "#run the function to collect feeds and scrape\n",
    "for url in url_list:\n",
    "    feed = feedparser.parse(url)\n",
    "    feed_title= feed.feed.title\n",
    "    \n",
    "    for entry in feed.entries:\n",
    "        entry_title= entry.title\n",
    "        entry_link= entry.link\n",
    "        entry_published_date= entry.published\n",
    "        entry_summary= entry.summary\n",
    "        entry_content = scrape_article_content(entry_link) #scrape\n",
    "        \n",
    "        entries.append({\n",
    "            \"feed_title\": feed_title,\n",
    "            \"entry_title\": entry_title,\n",
    "            \"entry_link\": entry_link,\n",
    "            \"entry_published_date\": entry_published_date,\n",
    "            \"entry_summary\": entry_summary,\n",
    "            \"entry_content\": entry_content,\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(entries)\n",
    "#print(df)\n",
    "\n",
    "df.to_csv('RSS_sports_feeds_11-13.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6387fe68-4eca-4499-8f0c-c3356f1f53e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for line in df[\"entry_content\"].tolist():\n",
    "#     print(line)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f9a3d96-4201-4150-8ecb-b14e133f886d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://www.tellerreport.com/sports\n",
      "Scraping: https://www.newsbreak.com/mountain-view-ca-sports\n",
      "Scraping: https://newsrnd.com/sports\n",
      "Scraping: https://baltimorecitywire.com/stories/tag/53-sports\n",
      "Failed to scrape https://baltimorecitywire.com/stories/tag/53-sports: 403 Client Error: Forbidden for url: https://baltimorecitywire.com/stories/tag/53-sports\n",
      "\n",
      "--- Article ---\n",
      "Title: Sports - Teller Report\n",
      "Date: No date found\n",
      "Content Preview: Now you can see non-English news...\n",
      "© Communities 2019 - Privacy...\n",
      "Link: https://www.tellerreport.com/sports\n",
      "\n",
      "\n",
      "--- Article ---\n",
      "Title: Mountain View, CA Sports and More | NewsBreak\n",
      "Date: No date found\n",
      "Content Preview: Mountain View\n",
      "This weekend saw a flurry of action in Bay Area high school football, with notable performances shaping the playoff picture. Among the highlights, De La Salle and Pittsburg secured dominant wins, while some ranked teams faced setbacks. The matchups across various leagues showcased high-scoring games and surprising results, particularly for the Bay Area News Group Top 25 teams. The landscape is adjusting as the season heads into its final weeks with playoff positions at stake.\n",
      "San J...\n",
      "Link: https://www.newsbreak.com/mountain-view-ca-sports\n",
      "\n",
      "\n",
      "--- Article ---\n",
      "Title: Sports - The Limited Times\n",
      "Date: No date found\n",
      "Content Preview: Now you can see non-English news...\n",
      " Champions League: UEFA confirms and explains its new competitions from next season2024-03-05T09:29:32.508Z\n",
      " Bayern fans are completely freaking out about Mathys Tel2024-03-05T09:19:33.179Z\n",
      " Bayern fans will listen carefully: Zidane talks about a possible coaching comeback2024-03-05T09:18:28.770Z\n",
      " Watzke is raging after the Bundesliga's investor deal collapsed2024-03-05T09:18:04.011Z\n",
      " Tennis: Rafael Nadal, in Indian Wells it's off to a comeback2024-03-05T09:12...\n",
      "Link: https://newsrnd.com/sports\n",
      "\n",
      "\n",
      "Results saved to scraped_uncredible_articles.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "uncredible_urls = [\n",
    "    \"https://www.tellerreport.com/sports\",\n",
    "    \"https://www.newsbreak.com/mountain-view-ca-sports\",\n",
    "    \"https://newsrnd.com/sports\",\n",
    "    \"https://baltimorecitywire.com/stories/tag/53-sports\"\n",
    "]\n",
    "\n",
    "# Function to scrape article details\n",
    "def scrape_article(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Extract the article title\n",
    "        title = soup.find(\"h1\").get_text() if soup.find(\"h1\") else soup.title.get_text()\n",
    "\n",
    "        # Extract the publication date (common in <time> or meta tags)\n",
    "        date = soup.find(\"time\")\n",
    "        if date:\n",
    "            publication_date = date.get(\"datetime\") or date.get_text()\n",
    "        else:\n",
    "            date_meta = soup.find(\"meta\", {\"name\": \"article:published_time\"})\n",
    "            publication_date = date_meta[\"content\"] if date_meta else \"No date found\"\n",
    "\n",
    "        # Extract the article content\n",
    "        content_container = (\n",
    "            soup.find(\"article\") or\n",
    "            soup.find(\"div\", class_=[\"post-content\", \"article-body\", \"article-content\", \"content-body\"])\n",
    "        )\n",
    "        if content_container:\n",
    "            paragraphs = content_container.find_all(\"p\")\n",
    "        else:\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "\n",
    "        content = \"\\n\".join(p.get_text() for p in paragraphs)\n",
    "\n",
    "        return {\n",
    "            \"Title\": title.strip(),\n",
    "            \"Publication Date\": publication_date.strip(),\n",
    "            \"Content\": content.strip()[:500] + \"...\",\n",
    "            \"Link\": url\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "articles = []\n",
    "for url in uncredible_urls:\n",
    "    print(f\"Scraping: {url}\")\n",
    "    article_details = scrape_article(url)\n",
    "    if article_details:\n",
    "        articles.append(article_details)\n",
    "\n",
    "for article in articles:\n",
    "    print(\"\\n--- Article ---\")\n",
    "    print(f\"Title: {article['Title']}\")\n",
    "    print(f\"Date: {article['Publication Date']}\")\n",
    "    print(f\"Content Preview: {article['Content']}\")\n",
    "    print(f\"Link: {article['Link']}\\n\")\n",
    "\n",
    "df = pd.DataFrame(articles)\n",
    "output_file = \"scraped_uncredible_articles.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\nResults saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f53448-164e-46d8-a16d-02c3dbaa7d0c",
   "metadata": {},
   "source": [
    "### Format DF to match DB table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b6b1163a-ce9f-47f0-9c15-d7e5cceaf6e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tizia\\Anaconda4\\Lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "df['publication_date'] = df['entry_published_date'].apply(lambda x: parser.parse(x).date()) #parse different date formats to date object format\n",
    "dbdf = pd.read_csv('RSS_sports_feeds.csv')\n",
    "dbdf['team_or_player'] = df['entry_title']\n",
    "dbdf['source'] = df['feed_title']\n",
    "dbdf['publication_date'] = df['publication_date'] \n",
    "dbdf['content'] = df['entry_summary']\n",
    "dbdf['trust_score'] = 0.00  #default\n",
    "dbdf['classification'] = 'unknown' #default\n",
    "dbdf['link'] = df['entry_link']\n",
    "\n",
    "#make a new df in the format of the DB table for easy inserting\n",
    "sports_DB_df = dbdf[['team_or_player', 'source', 'publication_date', 'content', 'trust_score', 'classification', 'link']]\n",
    "#save to a new CSV \n",
    "sports_DB_df.to_csv('formatted_sports_posts_for_DB.csv', index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c282d88e-3a8a-4283-af31-f011ec789135",
   "metadata": {},
   "source": [
    "## Define labeling approach to get classification and trust score to update sports_DB_df with ground truths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a2e79d-f469-4781-9f74-8bdd5e666204",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Tool analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d6c14acc-35e7-431f-833b-55ef48aca1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_bert_finnews: [{'label': 'negative', 'score': 0.9906446933746338}]\n"
     ]
    }
   ],
   "source": [
    "#sentiment tool test\n",
    "from transformers import pipeline\n",
    "\n",
    "df = pd.read_csv('RSS_sports_feeds_11-12.csv') #scraped feeds\n",
    "\n",
    "pipe_finnews = pipeline(\"text-classification\", model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
    "pipe_emotions = pipeline(\"text-classification\", model=\"bhadresh-savani/distilbert-base-uncased-emotion\")\n",
    "\n",
    "headlines=  random.sample( df[\"entry_title\"].tolist(), 15) \n",
    "# print(headlines)\n",
    "\n",
    "# for headline in headlines: \n",
    "#     result_bert_finnews = pipe_finnews(headline)\n",
    "#     result_bert_emotions = pipe_emotions(headline)\n",
    "#     print(headline)\n",
    "#     print(f\"result_bert_finnews: {result_bert_finnews}\")\n",
    "#     # #print(f\"result_bert_emotions: {result_bert_emotions}\")\n",
    "#     print()\n",
    "\n",
    "#check labels\n",
    "result_bert_finnews = pipe_finnews(\"Lionel Messi & Inter Miami SHOCKINGLY lose to Atlanta United 3-2 | SOTU\")\n",
    "print(f\"result_bert_finnews: {result_bert_finnews}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7727b548-8c86-4e77-a37e-aafd8fccffb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adidas Created An Incredible Halloween Ad For Anthony Edwards\n",
      "result_bert_finnews: [{'label': 'neutral', 'score': 0.9930527806282043}]\n",
      "result_bert_fin2: [{'label': 'NEUTRAL', 'score': 0.9766274690628052}]\n",
      "result_bert_twitter: [{'label': 'positive', 'score': 0.9170230627059937}]\n",
      "\n",
      "49ers coach Kyle Shanahan says players' sideline spat has been 'squashed'\n",
      "result_bert_finnews: [{'label': 'negative', 'score': 0.9799481630325317}]\n",
      "result_bert_fin2: [{'label': 'NEGATIVE', 'score': 0.995461642742157}]\n",
      "result_bert_twitter: [{'label': 'neutral', 'score': 0.7507781982421875}]\n",
      "\n",
      "NHL Awards Watch: Players the voters are backing after the first month of the season\n",
      "result_bert_finnews: [{'label': 'neutral', 'score': 0.9998824596405029}]\n",
      "result_bert_fin2: [{'label': 'NEUTRAL', 'score': 0.9994582533836365}]\n",
      "result_bert_twitter: [{'label': 'neutral', 'score': 0.8696771860122681}]\n",
      "\n",
      "Bears fire offensive coordinator Shane Waldron amid QB Caleb Williams' struggles\n",
      "result_bert_finnews: [{'label': 'negative', 'score': 0.9714420437812805}]\n",
      "result_bert_fin2: [{'label': 'NEGATIVE', 'score': 0.9404820203781128}]\n",
      "result_bert_twitter: [{'label': 'neutral', 'score': 0.5283016562461853}]\n",
      "\n",
      "Lou Williams Called Drake’s Comments About DeMar DeRozan ‘Selfish’\n",
      "result_bert_finnews: [{'label': 'neutral', 'score': 0.9763056039810181}]\n",
      "result_bert_fin2: [{'label': 'NEGATIVE', 'score': 0.6506056785583496}]\n",
      "result_bert_twitter: [{'label': 'neutral', 'score': 0.6449485421180725}]\n",
      "\n",
      "Bee Forum News: Some Voters Did Something\n",
      "result_bert_finnews: [{'label': 'neutral', 'score': 0.9998337030410767}]\n",
      "result_bert_fin2: [{'label': 'NEUTRAL', 'score': 0.9986600875854492}]\n",
      "result_bert_twitter: [{'label': 'neutral', 'score': 0.8702095746994019}]\n",
      "\n",
      "Islanders struggling to close out games, tangle with Oilers next\n",
      "result_bert_finnews: [{'label': 'negative', 'score': 0.9938458800315857}]\n",
      "result_bert_fin2: [{'label': 'NEGATIVE', 'score': 0.9908638596534729}]\n",
      "result_bert_twitter: [{'label': 'negative', 'score': 0.6971492767333984}]\n",
      "\n",
      "Georgia shows deep scoring attack to cruise past Texas Southern\n",
      "result_bert_finnews: [{'label': 'positive', 'score': 0.9985087513923645}]\n",
      "result_bert_fin2: [{'label': 'NEUTRAL', 'score': 0.9155462980270386}]\n",
      "result_bert_twitter: [{'label': 'neutral', 'score': 0.6750205159187317}]\n",
      "\n",
      "Giants to evaluate potential QB change over bye week\n",
      "result_bert_finnews: [{'label': 'neutral', 'score': 0.9997783303260803}]\n",
      "result_bert_fin2: [{'label': 'NEUTRAL', 'score': 0.9989570379257202}]\n",
      "result_bert_twitter: [{'label': 'neutral', 'score': 0.942303478717804}]\n",
      "\n",
      "Robertson to critics: 'I'll fight for Liverpool spot'\n",
      "result_bert_finnews: [{'label': 'neutral', 'score': 0.9994902610778809}]\n",
      "result_bert_fin2: [{'label': 'NEUTRAL', 'score': 0.9982014894485474}]\n",
      "result_bert_twitter: [{'label': 'neutral', 'score': 0.6674142479896545}]\n",
      "\n",
      "Surgery ends season for Dak Prescott, Cowboys shift to QB triage mode\n",
      "result_bert_finnews: [{'label': 'neutral', 'score': 0.9939262270927429}]\n",
      "result_bert_fin2: [{'label': 'NEGATIVE', 'score': 0.5581539273262024}]\n",
      "result_bert_twitter: [{'label': 'neutral', 'score': 0.7369771003723145}]\n",
      "\n",
      "Surprising DePaul hopes hot shooting continues vs. Mercer\n",
      "result_bert_finnews: [{'label': 'positive', 'score': 0.9962813258171082}]\n",
      "result_bert_fin2: [{'label': 'NEUTRAL', 'score': 0.9794981479644775}]\n",
      "result_bert_twitter: [{'label': 'positive', 'score': 0.543778121471405}]\n",
      "\n",
      "Howard University Ushers In A New Era With A Just Blaze-Inspired Halftime Show\n",
      "result_bert_finnews: [{'label': 'neutral', 'score': 0.9995008707046509}]\n",
      "result_bert_fin2: [{'label': 'NEUTRAL', 'score': 0.996658444404602}]\n",
      "result_bert_twitter: [{'label': 'neutral', 'score': 0.6611554622650146}]\n",
      "\n",
      "Cowboys QB Dak Prescott to undergo season-ending surgery on torn hamstring\n",
      "result_bert_finnews: [{'label': 'neutral', 'score': 0.9960153698921204}]\n",
      "result_bert_fin2: [{'label': 'NEUTRAL', 'score': 0.5239162445068359}]\n",
      "result_bert_twitter: [{'label': 'neutral', 'score': 0.5967209935188293}]\n",
      "\n",
      "No. 1 Kansas, Michigan State center stage at Champions Classic\n",
      "result_bert_finnews: [{'label': 'neutral', 'score': 0.9998782873153687}]\n",
      "result_bert_fin2: [{'label': 'NEUTRAL', 'score': 0.9982008934020996}]\n",
      "result_bert_twitter: [{'label': 'neutral', 'score': 0.8463523983955383}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sentiment tool test- trying another twitter post fine-tuned model to compare to the Financial News tool \n",
    "#examine differences- which more accurate?\n",
    "from transformers import pipeline\n",
    "\n",
    "df = pd.read_csv('RSS_sports_feeds_11-12.csv') #scraped feeds\n",
    "\n",
    "#pipe_newsart = pipeline(\"text-classification\", model=\"fhamborg/roberta-targeted-sentiment-classification-newsarticles\")\n",
    "pipe_twitter = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\") \n",
    "pipe_finnews = pipeline(\"text-classification\", model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
    "pipe_fin2 = pipe = pipeline(\"text-classification\", model=\"AnkitAI/distilbert-base-uncased-financial-news-sentiment-analysis\")     \n",
    "\n",
    "headlines=  random.sample( df[\"entry_title\"].tolist(), 15) \n",
    "\n",
    "for headline in headlines: \n",
    "    result_bert_twitter = pipe_twitter(headline)\n",
    "    result_bert_finnews = pipe_finnews(headline)\n",
    "    #result_bert_newsart = pipe_newsart(headline)\n",
    "    result_bert_fin2 = pipe_fin2(headline)\n",
    "    \n",
    "    print(headline)\n",
    "    print(f\"result_bert_finnews: {result_bert_finnews}\")\n",
    "    print(f\"result_bert_fin2: {result_bert_fin2}\")\n",
    "    print(f\"result_bert_twitter: {result_bert_twitter}\")\n",
    "    #print(f\"result_bert_newsart: {result_bert_newsart}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# #check labels\n",
    "# result_bert_fin2 = pipe_fin2(\"Prem ref Coote suspended over Klopp outburst\")\n",
    "# print(f\"result_bert_fin2: {result_bert_fin2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1fdeee32-d9c3-45e1-8aaa-21bbe85ac915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset labels\t: [0, -1, 0, -1, 1, 0, 0, 0, 0, 0, 1, 1, -1, 1, -1]\n",
      "My labels\t: [0, -1, 0, 0, 1, 0, 0, 0, 0, -1, 1, 1, -1, -1, -1]\n",
      "Pearson Correlation Coefficient: 0.6356845346445097\n"
     ]
    }
   ],
   "source": [
    "#labels from original random sample\n",
    "sample_actual_labels=[0,-1,0,-1,1,0,0,0,0,0,1,1,-1,1,-1]\n",
    "print(f\"Dataset labels\\t: {sample_actual_labels}\")\n",
    "\n",
    "#my labels were the exact same as dataset labels   \n",
    "my_sample_labels= [0,-1,0,0,1,0,0,0,0,-1,1,1,-1,-1,-1]\n",
    "print(f\"My labels\\t: {my_sample_labels}\")\n",
    "\n",
    "#3/15 differing labels \n",
    "\n",
    "import numpy as np\n",
    "correlation = np.corrcoef(sample_actual_labels, my_sample_labels)[0, 1]\n",
    "print(f\"Pearson Correlation Coefficient: {correlation}\") \n",
    "#strong positive correlation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
