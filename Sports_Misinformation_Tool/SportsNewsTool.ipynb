{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "72517dfa-1720-49e6-b55a-f15cdf181498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: in the working copy of 'Sports_Misinformation_Tool/Analysis_for Heuristic.ipynb', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Sports_Misinformation_Tool/SportsNewsTool.ipynb', LF will be replaced by CRLF the next time Git touches it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main d5d8b52] organized some false claims in Analysis_for Heuristic.ipynb\n",
      " 2 files changed, 38 insertions(+), 9 deletions(-)\n",
      "Already up to date.\n",
      "branch 'main' set up to track 'origin/main'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To github.com:thernandez7/Sports_Misinformation_Tool.git\n",
      "   cc8190b..d5d8b52  main -> main\n"
     ]
    }
   ],
   "source": [
    "# !git add .\n",
    "# !git commit -m \"organized some false claims in Analysis_for Heuristic.ipynb and added  \"\n",
    "# !git pull\n",
    "# !git push -u origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8446acb8-a93e-418e-a0ec-3ef05f7389b8",
   "metadata": {},
   "source": [
    "## Sports Misinformation Classification Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82e525a-1cf9-47be-8298-238a25ce2667",
   "metadata": {},
   "source": [
    "### Connect to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065e181a-e5fd-4418-8be0-3378790be6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database successfully!\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "username = 'root'  \n",
    "password = 'password'\n",
    "host = 'localhost' \n",
    "database = 'sports_news_db'\n",
    "\n",
    "connection = mysql.connector.connect(\n",
    "    host=host,\n",
    "    user=username,\n",
    "    password=password,\n",
    "    database=database\n",
    ")\n",
    "\n",
    "if connection.is_connected():\n",
    "    print(\"Connected to the database successfully!\")\n",
    "\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{username}:{password}@{host}/{database}\", echo=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70ed146f-9fe7-4370-a3d2-898811f3e183",
   "metadata": {},
   "source": [
    "#### Use this format to insert into the database\n",
    "\n",
    "INSERT INTO articles (team_or_player, source, publication_date, content, trust_score, classification, link)\n",
    "VALUES\n",
    "('New York Yankees, Los Angeles Lakers', 'ESPN', '2024-10-27', 'Yankees article content example.', 85.00, 'real', 'https://example.com/article1'),\n",
    "('Los Angeles Lakers', 'Twitter', '2024-10-27', 'Lakers article content example.', 60.00, 'fake', 'https://example.com/article2');\n",
    "\n",
    "\n",
    "#### Table fields \n",
    "Table Name: articles\n",
    "\n",
    "Fields: \n",
    "\n",
    "     id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "     \n",
    "     team_or_player VARCHAR(500), (This will be the article title that we can query- usually includes teams or names in it)\n",
    "     \n",
    "     source VARCHAR(200),\n",
    "     \n",
    "     publication_date DATE,\n",
    "     \n",
    "     content TEXT,\n",
    "     \n",
    "     trust_score DECIMAL(5, 2), \n",
    "     \n",
    "     classification ENUM('credible', 'uncredible', 'unknown') DEFAULT 'unknown',\n",
    "\n",
    "     link VARCHAR(255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359168cc-f336-4972-b580-38575b45a13e",
   "metadata": {},
   "source": [
    "### Simulate Tool Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9687e8d1-8c66-4df1-80cf-4bcf6f33805c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the team or player's name:  x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No articles found for x.\n"
     ]
    }
   ],
   "source": [
    "team_or_player = input(\"Enter the team or player's name: \")\n",
    "\n",
    "query = f\"SELECT * FROM articles WHERE team_or_player LIKE '%{team_or_player}%'\" #search for entered name/team in the title \n",
    "df_result = pd.read_sql(query, con=engine, params={'team_or_player': team_or_player})\n",
    "\n",
    "#get results\n",
    "if not df_result.empty:\n",
    "    print(f\"Articles related to {team_or_player}:\")\n",
    "    display(df_result)\n",
    "else:\n",
    "    print(f\"No articles found for {team_or_player}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cee353-47af-48c3-a0eb-cf19ddcbc995",
   "metadata": {},
   "source": [
    "### Get Article Entries from RSS Feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "06ea0136-a9b1-4fac-94f6-1949deec9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reddit API - collected 1876 posts\n",
    "# import csv\n",
    "# import praw\n",
    "# from datetime import datetime\n",
    "\n",
    "\n",
    "# reddit = praw.Reddit(\n",
    "#     client_id='w-kwRyPigyjYeG9DOiDc8g', \n",
    "#     client_secret='ZeDsvNH2YlpVH7F9wEWPkt5wkjLzqA',  \n",
    "#     user_agent='sports_misinfo_script'  \n",
    "# )\n",
    "\n",
    "# subreddit = reddit.subreddit('sports+fantasyfootball') #2 subreddits \n",
    "\n",
    "# recent_posts = []\n",
    "# for post in subreddit.new(limit= 5000):\n",
    "#     created_date = datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "#     recent_posts.append({\n",
    "#         'title': post.title,\n",
    "#         'score': post.score,\n",
    "#         'url': post.url,\n",
    "#         'id': post.id,\n",
    "#         'author': str(post.author),\n",
    "#         'text': post.selftext,\n",
    "#         'created_date': created_date,\n",
    "#         'num_comments': post.num_comments,\n",
    "#         'subreddit': post.subreddit.display_name,\n",
    "\n",
    "#     })\n",
    "\n",
    "# print(f\"Fetched {len(recent_posts)} posts\")\n",
    "\n",
    "# for i, post in enumerate(recent_posts[:5]):\n",
    "#     print(f\"{i+1}. Title: {post['title']} | Score: {post['score']} | URL: {post['url']}\")\n",
    "\n",
    "\n",
    "# #-------------------------------\n",
    "# #Save the data to a csv file\n",
    "# fieldnames = ['title', 'score', 'url', 'id', 'author', 'text', 'created_date', 'num_comments', 'subreddit']\n",
    "\n",
    "# with open('recent_sports_reddit_posts.csv', mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "#     writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "#     writer.writeheader()\n",
    "\n",
    "#     for post in recent_posts:\n",
    "#         writer.writerow(post)\n",
    "\n",
    "# print(\"Data successfully saved to 'recent_sports_reddit_posts.csv'\")\n",
    "# print(\"Data saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "334d2970-7ee0-47e1-80d9-e01ae15fa4a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape content from https://www.espn.com/nfl/story/_/id/42348968/aaron-rodgers-new-york-jets-complicated-struggle: 403 Client Error: Forbidden for url: https://www.espn.com/nfl/story/_/id/42348968/aaron-rodgers-new-york-jets-complicated-struggle\n",
      "Failed to scrape content from https://www.espn.com/mens-college-basketball/story/_/id/42360055/jon-scheyer-take-hard-look-duke-cramping-issues: 403 Client Error: Forbidden for url: https://www.espn.com/mens-college-basketball/story/_/id/42360055/jon-scheyer-take-hard-look-duke-cramping-issues\n",
      "Failed to scrape content from https://www.espn.com/wnba/story/_/id/42366828/dream-hire-florida-gulf-coast-karl-smesko-head-coach: 403 Client Error: Forbidden for url: https://www.espn.com/wnba/story/_/id/42366828/dream-hire-florida-gulf-coast-karl-smesko-head-coach\n",
      "Failed to scrape content from https://www.espn.com/nba/story/_/id/42362342/mavericks-klay-thompson-celebrated-warriors-surreal-return-bay-area: 403 Client Error: Forbidden for url: https://www.espn.com/nba/story/_/id/42362342/mavericks-klay-thompson-celebrated-warriors-surreal-return-bay-area\n",
      "Failed to scrape content from https://www.espn.com/nba/story/_/id/42359359/spoelstra-takes-blame-horrendous-gaffe-heat-loss: 403 Client Error: Forbidden for url: https://www.espn.com/nba/story/_/id/42359359/spoelstra-takes-blame-horrendous-gaffe-heat-loss\n",
      "Failed to scrape content from https://www.espn.com/college-football/story/_/id/42355474/oregon-ohio-state-texas-penn-state-atop-cfp-rankings: 403 Client Error: Forbidden for url: https://www.espn.com/college-football/story/_/id/42355474/oregon-ohio-state-texas-penn-state-atop-cfp-rankings\n",
      "Failed to scrape content from https://www.espn.com/boxing/story/_/id/42360734/mike-tyson-vows-bring-devil-ring-vs-jake-paul: 403 Client Error: Forbidden for url: https://www.espn.com/boxing/story/_/id/42360734/mike-tyson-vows-bring-devil-ring-vs-jake-paul\n",
      "Failed to scrape content from https://www.espn.com/college-football/story/_/id/42358262/oklahoma-president-ad-back-brent-venables-amid-struggles: 403 Client Error: Forbidden for url: https://www.espn.com/college-football/story/_/id/42358262/oklahoma-president-ad-back-brent-venables-amid-struggles\n",
      "Failed to scrape content from https://www.espn.com/nhl/story/_/id/42357126/winnipeg-jets-become-first-team-nhl-history-15-1: 403 Client Error: Forbidden for url: https://www.espn.com/nhl/story/_/id/42357126/winnipeg-jets-become-first-team-nhl-history-15-1\n",
      "Failed to scrape content from https://www.espn.com/college-football/story/_/id/42350167/college-football-playoff-anger-index-week-12-2024: 403 Client Error: Forbidden for url: https://www.espn.com/college-football/story/_/id/42350167/college-football-playoff-anger-index-week-12-2024\n",
      "Failed to scrape content from https://www.espn.com/nba/story/_/id/40544747/2024-nba-season-tournament-format-schedule-groups: 403 Client Error: Forbidden for url: https://www.espn.com/nba/story/_/id/40544747/2024-nba-season-tournament-format-schedule-groups\n",
      "Failed to scrape content from https://www.espn.com/nba/story/_/id/42347029/how-jj-redick-changed-los-angeles-lakers-same-roster: 403 Client Error: Forbidden for url: https://www.espn.com/nba/story/_/id/42347029/how-jj-redick-changed-los-angeles-lakers-same-roster\n",
      "Failed to scrape content from https://www.espn.com/nba/story/_/id/42333682/nba-power-rankings-most-important-role-player-all-30-teams: 403 Client Error: Forbidden for url: https://www.espn.com/nba/story/_/id/42333682/nba-power-rankings-most-important-role-player-all-30-teams\n",
      "Failed to scrape content from https://www.espn.com/mens-college-basketball/story/_/id/42353890/2024-champions-classic-kansas-michigan-state-duke-kentucky-played-out: 403 Client Error: Forbidden for url: https://www.espn.com/mens-college-basketball/story/_/id/42353890/2024-champions-classic-kansas-michigan-state-duke-kentucky-played-out\n",
      "Failed to scrape content from https://www.espn.com/nfl/story/_/id/42331551/2024-nfl-rookies-ranking-top-10-daniels-bowers-verse: 403 Client Error: Forbidden for url: https://www.espn.com/nfl/story/_/id/42331551/2024-nfl-rookies-ranking-top-10-daniels-bowers-verse\n",
      "Failed to scrape content from https://www.espn.com/boxing/story/_/id/39772221/what-boxing-stats-tell-us-mike-tyson-vs-jake-paul-fight-netflix-friday: 403 Client Error: Forbidden for url: https://www.espn.com/boxing/story/_/id/39772221/what-boxing-stats-tell-us-mike-tyson-vs-jake-paul-fight-netflix-friday\n",
      "Failed to scrape content from https://www.espn.com/nhl/story/_/id/42347912/nhl-2024-25-calder-trophy-goaltender-rookie-year-wolf-blomqvist-annunen: 403 Client Error: Forbidden for url: https://www.espn.com/nhl/story/_/id/42347912/nhl-2024-25-calder-trophy-goaltender-rookie-year-wolf-blomqvist-annunen\n",
      "Failed to scrape content from https://www.espn.com/nhl/story/_/id/42328557/nhl-awards-ranking-ballots-hart-norris-calder-vezina-selke: 403 Client Error: Forbidden for url: https://www.espn.com/nhl/story/_/id/42328557/nhl-awards-ranking-ballots-hart-norris-calder-vezina-selke\n",
      "Failed to scrape content from https://www.espn.com/womens-college-basketball/story/_/id/42357887/12-best-womens-college-basketball-high-school-recruits-regardless-class-betts-davidson-hall: 403 Client Error: Forbidden for url: https://www.espn.com/womens-college-basketball/story/_/id/42357887/12-best-womens-college-basketball-high-school-recruits-regardless-class-betts-davidson-hall\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42345733/manchester-united-ruben-amorim-do-list-new-manager-rashford-hojlund-old-trafford: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42345733/manchester-united-ruben-amorim-do-list-new-manager-rashford-hojlund-old-trafford\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42352383/ricardo-pepi-seizing-chance-shed-super-sub-label-psv-eindhoven-usmnt: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42352383/ricardo-pepi-seizing-chance-shed-super-sub-label-psv-eindhoven-usmnt\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42350165/premier-league-admits-var-error-ten-hag-sacking: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42350165/premier-league-admits-var-error-ten-hag-sacking\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42364885/real-madrid-transfers-club-steps-florian-wirtz-pursuit-source: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42364885/real-madrid-transfers-club-steps-florian-wirtz-pursuit-source\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42363668/barcelona-chief-seeks-clarity-lewandowski-disallowed-goal: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42363668/barcelona-chief-seeks-clarity-lewandowski-disallowed-goal\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42365026/liverpool-mohamed-salah-aiming-win-all-season: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42365026/liverpool-mohamed-salah-aiming-win-all-season\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42363759/cole-palmer-frustrated-lack-game-england: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42363759/cole-palmer-frustrated-lack-game-england\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42364311/real-sociedad-director-roberto-olabe-leave-end-season: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42364311/real-sociedad-director-roberto-olabe-leave-end-season\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42362632/cristiano-ronaldo-not-score-1000-goals: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42362632/cristiano-ronaldo-not-score-1000-goals\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42361468/man-united-bruno-fernandes-helps-unwell-passenger-flight: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42361468/man-united-bruno-fernandes-helps-unwell-passenger-flight\n",
      "Failed to scrape content from https://www.espn.com/fantasy/football/story/_/page/FFWeeklyPlayerRank24main-41028715/nfl-fantasy-football-rankings-2024-qb-rb-wr-te-dst: 403 Client Error: Forbidden for url: https://www.espn.com/fantasy/football/story/_/page/FFWeeklyPlayerRank24main-41028715/nfl-fantasy-football-rankings-2024-qb-rb-wr-te-dst\n",
      "Failed to scrape content from https://www.espn.com/mma/story/_/id/42307556/ufc-309-daniel-cormier-breaks-jon-jones-vs-stipe-miocic-heavyweight-title-fight: 403 Client Error: Forbidden for url: https://www.espn.com/mma/story/_/id/42307556/ufc-309-daniel-cormier-breaks-jon-jones-vs-stipe-miocic-heavyweight-title-fight\n",
      "Failed to scrape content from https://www.espn.com/mma/story/_/id/42277592/ufc-309-michael-chandler-conor-mcgregor-moving-betting-odds: 403 Client Error: Forbidden for url: https://www.espn.com/mma/story/_/id/42277592/ufc-309-michael-chandler-conor-mcgregor-moving-betting-odds\n",
      "Failed to scrape content from https://www.espn.com/soccer/story/_/id/42353731/soccer-transfer-rumors-news-clubs-circling-salah-january-looms: 403 Client Error: Forbidden for url: https://www.espn.com/soccer/story/_/id/42353731/soccer-transfer-rumors-news-clubs-circling-salah-january-looms\n",
      "Failed to scrape content from https://www.espn.com/espn/betting/story/_/id/41194969/2024-college-football-betting-heisman-trophy-picks-odds: 403 Client Error: Forbidden for url: https://www.espn.com/espn/betting/story/_/id/41194969/2024-college-football-betting-heisman-trophy-picks-odds\n",
      "Failed to scrape content from https://www.espn.com/espn/betting/story/_/id/42346757/2024-nba-betting-futures-bets-make-red-hot-golden-state-warriors: 403 Client Error: Forbidden for url: https://www.espn.com/espn/betting/story/_/id/42346757/2024-nba-betting-futures-bets-make-red-hot-golden-state-warriors\n",
      "Failed to scrape content from https://www.espn.com/espn/feature/story/_/page/bracketology/ncaa-bracketology-2025-march-madness-men-field-predictions: 403 Client Error: Forbidden for url: https://www.espn.com/espn/feature/story/_/page/bracketology/ncaa-bracketology-2025-march-madness-men-field-predictions\n",
      "Failed to scrape content from https://www.espn.com/espn/feature/story/_/id/30423107/ncaa-women-bracketology-2025-women-college-basketball-projections: 403 Client Error: Forbidden for url: https://www.espn.com/espn/feature/story/_/id/30423107/ncaa-women-bracketology-2025-women-college-basketball-projections\n",
      "Failed to scrape content from https://www.espn.com/nba/story/_/id/42350515/oklahoma-city-thunder-city-edition-uniforms-2024-25: 403 Client Error: Forbidden for url: https://www.espn.com/nba/story/_/id/42350515/oklahoma-city-thunder-city-edition-uniforms-2024-25\n",
      "Failed to scrape content from https://www.espn.com/wnba/story/_/id/42353221/angel-reese-old-photo-uconn-merch-geno-auriemma: 403 Client Error: Forbidden for url: https://www.espn.com/wnba/story/_/id/42353221/angel-reese-old-photo-uconn-merch-geno-auriemma\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "object has no attribute 'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Anaconda4\\Lib\\site-packages\\feedparser\\util.py:156\u001b[0m, in \u001b[0;36mFeedParserDict.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(key)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Anaconda4\\Lib\\site-packages\\feedparser\\util.py:113\u001b[0m, in \u001b[0;36mFeedParserDict.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, realkey)\n\u001b[1;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'title'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[175], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m url_list:\n\u001b[0;32m     55\u001b[0m     feed \u001b[38;5;241m=\u001b[39m feedparser\u001b[38;5;241m.\u001b[39mparse(url)\n\u001b[1;32m---> 56\u001b[0m     feed_title\u001b[38;5;241m=\u001b[39m feed\u001b[38;5;241m.\u001b[39mfeed\u001b[38;5;241m.\u001b[39mtitle\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m feed\u001b[38;5;241m.\u001b[39mentries:\n\u001b[0;32m     59\u001b[0m         entry_title\u001b[38;5;241m=\u001b[39m entry\u001b[38;5;241m.\u001b[39mtitle\n",
      "File \u001b[1;32m~\\Anaconda4\\Lib\\site-packages\\feedparser\\util.py:158\u001b[0m, in \u001b[0;36mFeedParserDict.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(key)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m key)\n",
      "\u001b[1;31mAttributeError\u001b[0m: object has no attribute 'title'"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#add urls to this list to parse\n",
    "url_list = [\n",
    "        #MBFC High Credibility\n",
    "        \"https://www.espn.com/espn/rss/news\", #ESPN top headlines\n",
    "        \"https://deadspin.com/rss/\", #Deadspin \n",
    "        \"https://feeds.abcnews.com/abcnews/sportsheadlines\" #ABC news- espn sports\n",
    "        \"https://rss.nytimes.com/services/xml/rss/nyt/Sports.xml\" #NYT\n",
    "\n",
    "        #MBFC questionable sources or medium credibility\n",
    "        \"https://notthebee.com/feed\", #not the bee\n",
    "        \"https://uproxx.com/sports/feed/\", #uproxx   \n",
    "        \"https://www.vibe.com/c/news/sports/feed/\", #The vibe - medium cred \n",
    "        \"https://moxie.foxnews.com/google-publisher/sports.xml\", #fox news \n",
    "        \"https://nypost.com/sports/feed/\" #NY post - medium cred\n",
    "\n",
    "        #NO MBFC RATING\n",
    "        \"https://www.sportscollectorsdaily.com/feed/\", #Sports Collectors Daily \n",
    "        \"https://news.sportslogos.net/feed/\", #SportsLogos.Net\n",
    "]\n",
    "    \n",
    "entries = [] #list of dictionaries\n",
    "\n",
    "#--------------DEFINE FUNCTION TO SCRAPE-------------------\n",
    "def scrape_article_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        #extract content from common tags\n",
    "        article_body = (\n",
    "            soup.find(\"article\") or\n",
    "            soup.find(\"div\", {\"class\": \"post-content\"}) or\n",
    "            soup.find(\"div\", class_=\"article-body\") or\n",
    "            soup.find(\"div\", class_=\"article-content\") or\n",
    "            soup.find(\"section\", class_=\"article-section\") or\n",
    "            soup.find(\"div\", class_=\"main-content\") or\n",
    "            soup.find(\"div\", class_=\"content-body\")\n",
    "        )\n",
    "        \n",
    "        if article_body:\n",
    "            paragraphs = article_body.find_all(\"p\")\n",
    "        else:\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "\n",
    "        #join all paragraphs into a single string\n",
    "        article_content = \" \".join(p.get_text() for p in paragraphs)\n",
    "        return article_content.strip() if article_content else \"No content found\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape content from {url}: {e}\")\n",
    "        return \"Failed to fetch content\"\n",
    "\n",
    "\n",
    "#run the function to collect feeds and scrape\n",
    "for url in url_list:\n",
    "    feed = feedparser.parse(url)\n",
    "    feed_title= feed.feed.title\n",
    "    \n",
    "    for entry in feed.entries:\n",
    "        entry_title= entry.title\n",
    "        entry_link= entry.link\n",
    "        entry_published_date= entry.published\n",
    "        entry_summary= entry.summary\n",
    "        entry_content = scrape_article_content(entry_link) #scrape\n",
    "        \n",
    "        entries.append({\n",
    "            \"feed_title\": feed_title,\n",
    "            \"entry_title\": entry_title,\n",
    "            \"entry_link\": entry_link,\n",
    "            \"entry_published_date\": entry_published_date,\n",
    "            \"entry_summary\": entry_summary,\n",
    "            \"entry_content\": entry_content,\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(entries)\n",
    "#print(df)\n",
    "\n",
    "df.to_csv('RSS_sports_feeds_11-13.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6387fe68-4eca-4499-8f0c-c3356f1f53e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for line in df[\"entry_content\"].tolist():\n",
    "#     print(line)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f9a3d96-4201-4150-8ecb-b14e133f886d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://www.tellerreport.com/sports\n",
      "Scraping: https://www.newsbreak.com/mountain-view-ca-sports\n",
      "Scraping: https://newsrnd.com/sports\n",
      "Scraping: https://baltimorecitywire.com/stories/tag/53-sports\n",
      "Failed to scrape https://baltimorecitywire.com/stories/tag/53-sports: 403 Client Error: Forbidden for url: https://baltimorecitywire.com/stories/tag/53-sports\n",
      "\n",
      "--- Article ---\n",
      "Title: Sports - Teller Report\n",
      "Date: No date found\n",
      "Content Preview: Now you can see non-English news...\n",
      "Â© Communities 2019 - Privacy...\n",
      "Link: https://www.tellerreport.com/sports\n",
      "\n",
      "\n",
      "--- Article ---\n",
      "Title: Mountain View, CA Sports and More | NewsBreak\n",
      "Date: No date found\n",
      "Content Preview: Mountain View\n",
      "This weekend saw a flurry of action in Bay Area high school football, with notable performances shaping the playoff picture. Among the highlights, De La Salle and Pittsburg secured dominant wins, while some ranked teams faced setbacks. The matchups across various leagues showcased high-scoring games and surprising results, particularly for the Bay Area News Group Top 25 teams. The landscape is adjusting as the season heads into its final weeks with playoff positions at stake.\n",
      "San J...\n",
      "Link: https://www.newsbreak.com/mountain-view-ca-sports\n",
      "\n",
      "\n",
      "--- Article ---\n",
      "Title: Sports - The Limited Times\n",
      "Date: No date found\n",
      "Content Preview: Now you can see non-English news...\n",
      " Champions League: UEFA confirms and explains its new competitions from next season2024-03-05T09:29:32.508Z\n",
      " Bayern fans are completely freaking out about Mathys Tel2024-03-05T09:19:33.179Z\n",
      " Bayern fans will listen carefully: Zidane talks about a possible coaching comeback2024-03-05T09:18:28.770Z\n",
      " Watzke is raging after the Bundesliga's investor deal collapsed2024-03-05T09:18:04.011Z\n",
      " Tennis: Rafael Nadal, in Indian Wells it's off to a comeback2024-03-05T09:12...\n",
      "Link: https://newsrnd.com/sports\n",
      "\n",
      "\n",
      "Results saved to scraped_uncredible_articles.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "uncredible_urls = [\n",
    "    \"https://www.tellerreport.com/sports\",\n",
    "    \"https://www.newsbreak.com/mountain-view-ca-sports\",\n",
    "    \"https://newsrnd.com/sports\",\n",
    "    \"https://baltimorecitywire.com/stories/tag/53-sports\"\n",
    "]\n",
    "\n",
    "# Function to scrape article details\n",
    "def scrape_article(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Extract the article title\n",
    "        title = soup.find(\"h1\").get_text() if soup.find(\"h1\") else soup.title.get_text()\n",
    "\n",
    "        # Extract the publication date (common in <time> or meta tags)\n",
    "        date = soup.find(\"time\")\n",
    "        if date:\n",
    "            publication_date = date.get(\"datetime\") or date.get_text()\n",
    "        else:\n",
    "            date_meta = soup.find(\"meta\", {\"name\": \"article:published_time\"})\n",
    "            publication_date = date_meta[\"content\"] if date_meta else \"No date found\"\n",
    "\n",
    "        # Extract the article content\n",
    "        content_container = (\n",
    "            soup.find(\"article\") or\n",
    "            soup.find(\"div\", class_=[\"post-content\", \"article-body\", \"article-content\", \"content-body\"])\n",
    "        )\n",
    "        if content_container:\n",
    "            paragraphs = content_container.find_all(\"p\")\n",
    "        else:\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "\n",
    "        content = \"\\n\".join(p.get_text() for p in paragraphs)\n",
    "\n",
    "        return {\n",
    "            \"Title\": title.strip(),\n",
    "            \"Publication Date\": publication_date.strip(),\n",
    "            \"Content\": content.strip()[:500] + \"...\",\n",
    "            \"Link\": url\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "articles = []\n",
    "for url in uncredible_urls:\n",
    "    print(f\"Scraping: {url}\")\n",
    "    article_details = scrape_article(url)\n",
    "    if article_details:\n",
    "        articles.append(article_details)\n",
    "\n",
    "for article in articles:\n",
    "    print(\"\\n--- Article ---\")\n",
    "    print(f\"Title: {article['Title']}\")\n",
    "    print(f\"Date: {article['Publication Date']}\")\n",
    "    print(f\"Content Preview: {article['Content']}\")\n",
    "    print(f\"Link: {article['Link']}\\n\")\n",
    "\n",
    "df = pd.DataFrame(articles)\n",
    "output_file = \"scraped_uncredible_articles.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\nResults saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f53448-164e-46d8-a16d-02c3dbaa7d0c",
   "metadata": {},
   "source": [
    "### Format DF to match DB table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b6b1163a-ce9f-47f0-9c15-d7e5cceaf6e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tizia\\Anaconda4\\Lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "df['publication_date'] = df['entry_published_date'].apply(lambda x: parser.parse(x).date()) #parse different date formats to date object format\n",
    "dbdf = pd.read_csv('RSS_sports_feeds.csv')\n",
    "dbdf['team_or_player'] = df['entry_title']\n",
    "dbdf['source'] = df['feed_title']\n",
    "dbdf['publication_date'] = df['publication_date'] \n",
    "dbdf['content'] = df['entry_summary']\n",
    "dbdf['trust_score'] = 0.00  #default\n",
    "dbdf['classification'] = 'unknown' #default\n",
    "dbdf['link'] = df['entry_link']\n",
    "\n",
    "#make a new df in the format of the DB table for easy inserting\n",
    "sports_DB_df = dbdf[['team_or_player', 'source', 'publication_date', 'content', 'trust_score', 'classification', 'link']]\n",
    "#save to a new CSV \n",
    "sports_DB_df.to_csv('formatted_sports_posts_for_DB.csv', index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c282d88e-3a8a-4283-af31-f011ec789135",
   "metadata": {},
   "source": [
    "## Define labeling approach to get classification and trust score to update sports_DB_df with ground truths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
