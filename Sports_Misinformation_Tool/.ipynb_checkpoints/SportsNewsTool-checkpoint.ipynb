{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "72517dfa-1720-49e6-b55a-f15cdf181498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: in the working copy of 'Sports_Misinformation_Tool/.ipynb_checkpoints/SportsNewsTool-checkpoint.ipynb', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Sports_Misinformation_Tool/SportsNewsTool.ipynb', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Sports_Misinformation_Tool/.ipynb_checkpoints/Analysis_for Heuristic-checkpoint.ipynb', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Sports_Misinformation_Tool/.ipynb_checkpoints/RSS_sports_feeds_11-13-checkpoint.csv', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Sports_Misinformation_Tool/.ipynb_checkpoints/Sentiment Analysis Tool Analysis-checkpoint.ipynb', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Sports_Misinformation_Tool/Analysis_for Heuristic.ipynb', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Sports_Misinformation_Tool/RSS_sports_feeds_11-13.csv', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'Sports_Misinformation_Tool/Sentiment Analysis Tool Analysis.ipynb', LF will be replaced by CRLF the next time Git touches it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 1e7281f] added sentiment analysis different tool evaluation\n",
      " 10 files changed, 2778 insertions(+), 436 deletions(-)\n",
      " create mode 100644 Sports_Misinformation_Tool/.ipynb_checkpoints/Analysis_for Heuristic-checkpoint.ipynb\n",
      " create mode 100644 Sports_Misinformation_Tool/.ipynb_checkpoints/RSS_sports_feeds_11-13-checkpoint.csv\n",
      " create mode 100644 Sports_Misinformation_Tool/.ipynb_checkpoints/Sentiment Analysis Tool Analysis-checkpoint.ipynb\n",
      " delete mode 100644 Sports_Misinformation_Tool/.ipynb_checkpoints/scraped_uncredible_articles-checkpoint.csv\n",
      " create mode 100644 Sports_Misinformation_Tool/Analysis_for Heuristic.ipynb\n",
      " create mode 100644 Sports_Misinformation_Tool/RSS_sports_feeds_11-13.csv\n",
      " create mode 100644 Sports_Misinformation_Tool/Sentiment Analysis Tool Analysis.ipynb\n",
      " delete mode 100644 Sports_Misinformation_Tool/scraped_uncredible_articles.csv\n",
      "Already up to date.\n",
      "branch 'main' set up to track 'origin/main'."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To github.com:thernandez7/Sports_Misinformation_Tool.git\n",
      "   624697c..1e7281f  main -> main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git commit -m \"added sentiment analysis different tool evaluation\"\n",
    "!git pull\n",
    "!git push -u origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8446acb8-a93e-418e-a0ec-3ef05f7389b8",
   "metadata": {},
   "source": [
    "## Sports Misinformation Classification Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82e525a-1cf9-47be-8298-238a25ce2667",
   "metadata": {},
   "source": [
    "### Connect to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065e181a-e5fd-4418-8be0-3378790be6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database successfully!\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "username = 'root'  \n",
    "password = 'password'\n",
    "host = 'localhost' \n",
    "database = 'sports_news_db'\n",
    "\n",
    "connection = mysql.connector.connect(\n",
    "    host=host,\n",
    "    user=username,\n",
    "    password=password,\n",
    "    database=database\n",
    ")\n",
    "\n",
    "if connection.is_connected():\n",
    "    print(\"Connected to the database successfully!\")\n",
    "\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{username}:{password}@{host}/{database}\", echo=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70ed146f-9fe7-4370-a3d2-898811f3e183",
   "metadata": {},
   "source": [
    "#### Use this format to insert into the database\n",
    "\n",
    "INSERT INTO articles (team_or_player, source, publication_date, content, trust_score, classification, link)\n",
    "VALUES\n",
    "('New York Yankees, Los Angeles Lakers', 'ESPN', '2024-10-27', 'Yankees article content example.', 85.00, 'real', 'https://example.com/article1'),\n",
    "('Los Angeles Lakers', 'Twitter', '2024-10-27', 'Lakers article content example.', 60.00, 'fake', 'https://example.com/article2');\n",
    "\n",
    "\n",
    "#### Table fields \n",
    "Table Name: articles\n",
    "\n",
    "Fields: \n",
    "\n",
    "     id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "     \n",
    "     team_or_player VARCHAR(500), (This will be the article title that we can query- usually includes teams or names in it)\n",
    "     \n",
    "     source VARCHAR(200),\n",
    "     \n",
    "     publication_date DATE,\n",
    "     \n",
    "     content TEXT,\n",
    "     \n",
    "     trust_score DECIMAL(5, 2), \n",
    "     \n",
    "     classification ENUM('credible', 'uncredible', 'unknown') DEFAULT 'unknown',\n",
    "\n",
    "     link VARCHAR(255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359168cc-f336-4972-b580-38575b45a13e",
   "metadata": {},
   "source": [
    "### Simulate Tool Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9687e8d1-8c66-4df1-80cf-4bcf6f33805c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the team or player's name:  x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No articles found for x.\n"
     ]
    }
   ],
   "source": [
    "team_or_player = input(\"Enter the team or player's name: \")\n",
    "\n",
    "query = f\"SELECT * FROM articles WHERE team_or_player LIKE '%{team_or_player}%'\" #search for entered name/team in the title \n",
    "df_result = pd.read_sql(query, con=engine, params={'team_or_player': team_or_player})\n",
    "\n",
    "#get results\n",
    "if not df_result.empty:\n",
    "    print(f\"Articles related to {team_or_player}:\")\n",
    "    display(df_result)\n",
    "else:\n",
    "    print(f\"No articles found for {team_or_player}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cee353-47af-48c3-a0eb-cf19ddcbc995",
   "metadata": {},
   "source": [
    "### Get Article Entries from RSS Feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "06ea0136-a9b1-4fac-94f6-1949deec9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reddit API - collected 1876 posts\n",
    "# import csv\n",
    "# import praw\n",
    "# from datetime import datetime\n",
    "\n",
    "\n",
    "# reddit = praw.Reddit(\n",
    "#     client_id='w-kwRyPigyjYeG9DOiDc8g', \n",
    "#     client_secret='ZeDsvNH2YlpVH7F9wEWPkt5wkjLzqA',  \n",
    "#     user_agent='sports_misinfo_script'  \n",
    "# )\n",
    "\n",
    "# subreddit = reddit.subreddit('sports+fantasyfootball') #2 subreddits \n",
    "\n",
    "# recent_posts = []\n",
    "# for post in subreddit.new(limit= 5000):\n",
    "#     created_date = datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "#     recent_posts.append({\n",
    "#         'title': post.title,\n",
    "#         'score': post.score,\n",
    "#         'url': post.url,\n",
    "#         'id': post.id,\n",
    "#         'author': str(post.author),\n",
    "#         'text': post.selftext,\n",
    "#         'created_date': created_date,\n",
    "#         'num_comments': post.num_comments,\n",
    "#         'subreddit': post.subreddit.display_name,\n",
    "\n",
    "#     })\n",
    "\n",
    "# print(f\"Fetched {len(recent_posts)} posts\")\n",
    "\n",
    "# for i, post in enumerate(recent_posts[:5]):\n",
    "#     print(f\"{i+1}. Title: {post['title']} | Score: {post['score']} | URL: {post['url']}\")\n",
    "\n",
    "\n",
    "# #-------------------------------\n",
    "# #Save the data to a csv file\n",
    "# fieldnames = ['title', 'score', 'url', 'id', 'author', 'text', 'created_date', 'num_comments', 'subreddit']\n",
    "\n",
    "# with open('recent_sports_reddit_posts.csv', mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "#     writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "#     writer.writeheader()\n",
    "\n",
    "#     for post in recent_posts:\n",
    "#         writer.writerow(post)\n",
    "\n",
    "# print(\"Data successfully saved to 'recent_sports_reddit_posts.csv'\")\n",
    "# print(\"Data saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d2970-7ee0-47e1-80d9-e01ae15fa4a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#add urls to this list to parse\n",
    "url_list = [\n",
    "        \"https://www.sportscollectorsdaily.com/feed/\", #Sports Collectors Daily - No MBFC\n",
    "        \"https://www.espn.com/espn/rss/news\", #ESPN top headlines\n",
    "        \"https://deadspin.com/rss/\", #Deadspin \n",
    "        \"https://news.sportslogos.net/feed/\", #SportsLogos.Net -No MBFC\n",
    "        #CNN\n",
    "        #AP News\n",
    "\n",
    "        #questionable sources or medium credibility according to MBFC\n",
    "        \"https://notthebee.com/feed\", #not the bee\n",
    "        \"https://uproxx.com/sports/feed/\", #uproxx   \n",
    "        \"https://www.vibe.com/c/news/sports/feed/\", #The vibe - medium cred \n",
    "        \"https://moxie.foxnews.com/google-publisher/sports.xml\", #fox news \n",
    "]\n",
    "    \n",
    "entries = [] #list of dictionaries\n",
    "\n",
    "#--------------DEFINE FUNCTION TO SCRAPE-------------------\n",
    "def scrape_article_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        #extract content from common tags\n",
    "        article_body = (\n",
    "            soup.find(\"article\") or\n",
    "            soup.find(\"div\", {\"class\": \"post-content\"}) or\n",
    "            soup.find(\"div\", class_=\"article-body\") or\n",
    "            soup.find(\"div\", class_=\"article-content\") or\n",
    "            soup.find(\"section\", class_=\"article-section\") or\n",
    "            soup.find(\"div\", class_=\"main-content\") or\n",
    "            soup.find(\"div\", class_=\"content-body\")\n",
    "        )\n",
    "        \n",
    "        if article_body:\n",
    "            paragraphs = article_body.find_all(\"p\")\n",
    "        else:\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "\n",
    "        #join all paragraphs into a single string\n",
    "        article_content = \" \".join(p.get_text() for p in paragraphs)\n",
    "        return article_content.strip() if article_content else \"No content found\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape content from {url}: {e}\")\n",
    "        return \"Failed to fetch content\"\n",
    "\n",
    "\n",
    "#run the function to collect feeds and scrape\n",
    "for url in url_list:\n",
    "    feed = feedparser.parse(url)\n",
    "    feed_title= feed.feed.title\n",
    "    \n",
    "    for entry in feed.entries:\n",
    "        entry_title= entry.title\n",
    "        entry_link= entry.link\n",
    "        entry_published_date= entry.published\n",
    "        entry_summary= entry.summary\n",
    "        entry_content = scrape_article_content(entry_link) #scrape\n",
    "        \n",
    "        entries.append({\n",
    "            \"feed_title\": feed_title,\n",
    "            \"entry_title\": entry_title,\n",
    "            \"entry_link\": entry_link,\n",
    "            \"entry_published_date\": entry_published_date,\n",
    "            \"entry_summary\": entry_summary,\n",
    "            \"entry_content\": entry_content,\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(entries)\n",
    "#print(df)\n",
    "\n",
    "df.to_csv('RSS_sports_feeds_11-13.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6387fe68-4eca-4499-8f0c-c3356f1f53e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for line in df[\"entry_content\"].tolist():\n",
    "#     print(line)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f9a3d96-4201-4150-8ecb-b14e133f886d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://www.tellerreport.com/sports\n",
      "Scraping: https://www.newsbreak.com/mountain-view-ca-sports\n",
      "Scraping: https://newsrnd.com/sports\n",
      "Scraping: https://baltimorecitywire.com/stories/tag/53-sports\n",
      "Failed to scrape https://baltimorecitywire.com/stories/tag/53-sports: 403 Client Error: Forbidden for url: https://baltimorecitywire.com/stories/tag/53-sports\n",
      "\n",
      "--- Article ---\n",
      "Title: Sports - Teller Report\n",
      "Date: No date found\n",
      "Content Preview: Now you can see non-English news...\n",
      "© Communities 2019 - Privacy...\n",
      "Link: https://www.tellerreport.com/sports\n",
      "\n",
      "\n",
      "--- Article ---\n",
      "Title: Mountain View, CA Sports and More | NewsBreak\n",
      "Date: No date found\n",
      "Content Preview: Mountain View\n",
      "This weekend saw a flurry of action in Bay Area high school football, with notable performances shaping the playoff picture. Among the highlights, De La Salle and Pittsburg secured dominant wins, while some ranked teams faced setbacks. The matchups across various leagues showcased high-scoring games and surprising results, particularly for the Bay Area News Group Top 25 teams. The landscape is adjusting as the season heads into its final weeks with playoff positions at stake.\n",
      "San J...\n",
      "Link: https://www.newsbreak.com/mountain-view-ca-sports\n",
      "\n",
      "\n",
      "--- Article ---\n",
      "Title: Sports - The Limited Times\n",
      "Date: No date found\n",
      "Content Preview: Now you can see non-English news...\n",
      " Champions League: UEFA confirms and explains its new competitions from next season2024-03-05T09:29:32.508Z\n",
      " Bayern fans are completely freaking out about Mathys Tel2024-03-05T09:19:33.179Z\n",
      " Bayern fans will listen carefully: Zidane talks about a possible coaching comeback2024-03-05T09:18:28.770Z\n",
      " Watzke is raging after the Bundesliga's investor deal collapsed2024-03-05T09:18:04.011Z\n",
      " Tennis: Rafael Nadal, in Indian Wells it's off to a comeback2024-03-05T09:12...\n",
      "Link: https://newsrnd.com/sports\n",
      "\n",
      "\n",
      "Results saved to scraped_uncredible_articles.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "uncredible_urls = [\n",
    "    \"https://www.tellerreport.com/sports\",\n",
    "    \"https://www.newsbreak.com/mountain-view-ca-sports\",\n",
    "    \"https://newsrnd.com/sports\",\n",
    "    \"https://baltimorecitywire.com/stories/tag/53-sports\"\n",
    "]\n",
    "\n",
    "# Function to scrape article details\n",
    "def scrape_article(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Extract the article title\n",
    "        title = soup.find(\"h1\").get_text() if soup.find(\"h1\") else soup.title.get_text()\n",
    "\n",
    "        # Extract the publication date (common in <time> or meta tags)\n",
    "        date = soup.find(\"time\")\n",
    "        if date:\n",
    "            publication_date = date.get(\"datetime\") or date.get_text()\n",
    "        else:\n",
    "            date_meta = soup.find(\"meta\", {\"name\": \"article:published_time\"})\n",
    "            publication_date = date_meta[\"content\"] if date_meta else \"No date found\"\n",
    "\n",
    "        # Extract the article content\n",
    "        content_container = (\n",
    "            soup.find(\"article\") or\n",
    "            soup.find(\"div\", class_=[\"post-content\", \"article-body\", \"article-content\", \"content-body\"])\n",
    "        )\n",
    "        if content_container:\n",
    "            paragraphs = content_container.find_all(\"p\")\n",
    "        else:\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "\n",
    "        content = \"\\n\".join(p.get_text() for p in paragraphs)\n",
    "\n",
    "        return {\n",
    "            \"Title\": title.strip(),\n",
    "            \"Publication Date\": publication_date.strip(),\n",
    "            \"Content\": content.strip()[:500] + \"...\",\n",
    "            \"Link\": url\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "articles = []\n",
    "for url in uncredible_urls:\n",
    "    print(f\"Scraping: {url}\")\n",
    "    article_details = scrape_article(url)\n",
    "    if article_details:\n",
    "        articles.append(article_details)\n",
    "\n",
    "for article in articles:\n",
    "    print(\"\\n--- Article ---\")\n",
    "    print(f\"Title: {article['Title']}\")\n",
    "    print(f\"Date: {article['Publication Date']}\")\n",
    "    print(f\"Content Preview: {article['Content']}\")\n",
    "    print(f\"Link: {article['Link']}\\n\")\n",
    "\n",
    "df = pd.DataFrame(articles)\n",
    "output_file = \"scraped_uncredible_articles.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\nResults saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f53448-164e-46d8-a16d-02c3dbaa7d0c",
   "metadata": {},
   "source": [
    "### Format DF to match DB table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b6b1163a-ce9f-47f0-9c15-d7e5cceaf6e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tizia\\Anaconda4\\Lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "df['publication_date'] = df['entry_published_date'].apply(lambda x: parser.parse(x).date()) #parse different date formats to date object format\n",
    "dbdf = pd.read_csv('RSS_sports_feeds.csv')\n",
    "dbdf['team_or_player'] = df['entry_title']\n",
    "dbdf['source'] = df['feed_title']\n",
    "dbdf['publication_date'] = df['publication_date'] \n",
    "dbdf['content'] = df['entry_summary']\n",
    "dbdf['trust_score'] = 0.00  #default\n",
    "dbdf['classification'] = 'unknown' #default\n",
    "dbdf['link'] = df['entry_link']\n",
    "\n",
    "#make a new df in the format of the DB table for easy inserting\n",
    "sports_DB_df = dbdf[['team_or_player', 'source', 'publication_date', 'content', 'trust_score', 'classification', 'link']]\n",
    "#save to a new CSV \n",
    "sports_DB_df.to_csv('formatted_sports_posts_for_DB.csv', index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c282d88e-3a8a-4283-af31-f011ec789135",
   "metadata": {},
   "source": [
    "## Define labeling approach to get classification and trust score to update sports_DB_df with ground truths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
