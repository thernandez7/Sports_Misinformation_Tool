{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e570b09b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cd962a0-9c5c-47c5-82c9-618b80b923c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Lists\n",
    "false_headlines=[\n",
    "    \"BREAKING: Fox Sports Cancels ALL NFL Broadcasts \\\"Until Players Respect The Flag\\\"\",\n",
    "    \"Rugby Safer Than American Football \\\"For Health Reasons\\\": Biden\",\n",
    "    \"First woman to medal in six straight Olympics. Media and sponsors ignore her because she is outspoken pro-2A.\",\n",
    "    \"Novak Djokovic becomes the first professional athlete in history to be banned from a major sporting competition for not taking drugs\",\n",
    "    \"BREAKING: ESPN has fired Shannon Sharpe, per @ESPNNBA\",\n",
    "    \"The Minnesota Vikings have denounced Tim Walz: \\\"We don’t suppᴏrt his values.\\\"\",\n",
    "    \"'BREAKING: WNBA referees disqualify two players under league’s new \\\"no anthem kneeling\\\" rule\",\n",
    "    \"Nike announces termination of contract with Brittney Griner after \\\"strong backlash\\\" from online community: \\\"We need more athletes like Riley Gaines and less woke Brittney Griner!\\\"\",\n",
    "    \"KNEELING: After the University of Texas, all students who knelt during the national anthem were rounded up and REMOVED FROM SCHOLARSHIPS\",\n",
    "    \"Travis Kelce kneels during national anthem fined $10 million and thrown out of the game.\",\n",
    "    \"The NFL will now use facial recognition at every stadium to verify the identity of everyone at the game.\",\n",
    "    \"Mike Tyson says he’s willing to box Olympic DUDE with all proceeds to go to a battered women’s charity.\",\n",
    "    \"After winning silver, Yusef stood emotionless on the Olympic podium and declared, \\\"Sharon, if you’re watching this, I want my dog back.\\\"\",\n",
    "    \"Miami Dolphins QB Tua Tagovailoa will be sitting front row tonight in Doral for the Trump speech\",\n",
    "    \"BREAKING: The WNBA organizers have officially announced an investigation into the referees in all of Caitlin Clark's games for ignoring all dirty actions by her opponents against her\",\n",
    "    \"Chiefs' Coach Andy Reid \\\"fires 3 top players for anthem kneeling.\\\"\",\n",
    "    \"BREAKING: Caitlin Clark Rejects $400 Million Deal From Nike, \\\"Not With That Kaepernick Clown,\\\"\",\n",
    "    \"At Euro 2020, UEFA (European Football Association) ordered all team captains to wear \\\"OneLove\\\" bands. The band was used as a symbol of LGBTQ. But, Portugal captain Cristiano Ronaldo was the only European captain who did not wear the band.\",\n",
    "    \"Golden State Warriors refuse to visit White House after winning NBA title: reports\",\n",
    "    \"Taylor Swift faces a 10-game NFL ban following controversial political involvement - fans in uproar!\"\n",
    "]\n",
    "true_headlines=[\n",
    "    \"Did that really happen? Barbados v Grenada and a deliberate own goal\",\n",
    "    \"Breakdancing Will Not Be Busting A Move In 2028 Olympics\",\n",
    "    \"Breaking Will Not Be in The 2028 Los Angeles Olympics—What’s Next?\",\n",
    "    \"Braves Superstar Sets Atlanta-Era Record in 1st Inning\",\n",
    "    \"Only 20 schools in the Football Bowl Subdivision have athletic departments with revenue exceeding expenses\",\n",
    "    \"LATEST: Mitchell Stadium named America’s Best High School Football Stadium\",\n",
    "    \"Homes of Patrick Mahomes, Travis Kelce burglarized last month\",\n",
    "    \"Gregg Popovich recovering from mild stroke, no timeline for return set\",\n",
    "    \"Patrick Queen: I wasn’t wanted back with Ravens, it was definitely kind of upsetting\",\n",
    "    \"Odell Beckham Jr. boasts about taking 2021 Rams salary in Bitcoin National Football League Odell Beckham Jr. boasts about taking 2021 Rams salary in Bitcoin\",\n",
    "    \"Ecuador international soccer player Marco Angulo dies aged 22 following car crash\",\n",
    "    \"Shohei Ohtani Baseball Worth $4 Million Lands in Globally-Recognized Skyscraper\",\n",
    "    \"Kobe Bryant is the only person to have won both an Olympic medal and an Oscar\",\n",
    "    \"Bucks fan predicted Milwaukee-Phoenix NBA Finals all the way back in 2016\",\n",
    "    \"Chiefs kicker Butker congratulates women graduates and says most are more excited about motherhood\",\n",
    "    \"Kansas City Chiefs player faces backlash for graduation speech criticizing working women, calling Pride a \\\"deadly sin\\\"\", \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b2cb704-6e6b-4528-946e-0a06fd2c7239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\capuzb\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# !pip install textstat\n",
    "from transformers import pipeline\n",
    "import string\n",
    "from textstat import flesch_reading_ease\n",
    "import textstat\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "#functions for each feature we are considering\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def char_count(text):\n",
    "    return len(text)\n",
    "\n",
    "def avg_word_length(text):\n",
    "    words = text.split()\n",
    "    return sum(len(word) for word in words) / len(words) if words else 0\n",
    "\n",
    "def all_capitalized_word_count(text):\n",
    "    return sum(1 for word in text.split() if word.isupper())\n",
    "\n",
    "def punctuation_count(text):\n",
    "    return sum(1 for char in text if char in string.punctuation)\n",
    "\n",
    "def exclamation_count(text):\n",
    "    return text.count(\"!\")\n",
    "\n",
    "def question_mark_count(text):\n",
    "    return text.count(\"?\")\n",
    "\n",
    "def readability_score(text): #high score = easier readability\n",
    "    return flesch_reading_ease(text)\n",
    "\n",
    "def smog_index(text): #readability based on the number of complex words (3+ syllables)\n",
    "    return textstat.smog_index(text)\n",
    "\n",
    "def count_quotation_pairs(text): \n",
    "    double_quotes = re.findall(r'\"[^\"]+\"', text) #Finds all sections of text in double quotes\n",
    "    return len(double_quotes)\n",
    "\n",
    "def variety_of_vocabularity(text):\n",
    "    words = text.split()\n",
    "    unique_words = set(words)\n",
    "    return len(unique_words) / len(words) if words else 0\n",
    "\n",
    "\n",
    "sensational_words = [\"shocking\",\"shock\", \"amazing\", \"unbelievable\", \"you won’t believe\", \"incredible\", \"stunning\",\\\n",
    "                    \"astounding\", \"breathtaking\", \"outstanding\", \"thrilling\", \"must see\", \"terrible\", \"awful\",\\\n",
    "                     \"fantastic\", \"horrible\", \"remarkable\"]\n",
    "def count_sensational_words(text):\n",
    "    text = text.lower()\n",
    "    return sum(1 for word in sensational_words if word in text)\n",
    "\n",
    "\n",
    "absolute_words = [\"always\", \"never\", \"clearly\", \"obviously\", \"definitely\", \"everyone\", \"nobody\", \"all\",\\\n",
    "              \"none\", \"blatantly\", \"undoubtedly\", \"must\", \"should\"]\n",
    "def count_absolute_words(text):\n",
    "    words = text.lower().split()\n",
    "    return sum(1 for word in words if word in absolute_words)\n",
    "\n",
    "\n",
    "factuality_words = [\"proven\", \"irrefutable\", \"unarguable\", \"unquestionably\", \"certainly\", \"definitely\",\\\n",
    "                    \"undeniable\"]\n",
    "def count_factuality_words(text):\n",
    "    words = text.lower().split()\n",
    "    return sum(1 for word in words if word in factuality_words)\n",
    "\n",
    "\n",
    "subjective_words = [\"believe\", \"think\", \"feel\", \"prefer\", \"seems\", \"wish\"]\n",
    "def subjectivity_score(text):\n",
    "    words = text.lower().split()\n",
    "    subjective_count = sum(1 for word in words if word in subjective_words)\n",
    "    return subjective_count / len(words) if len(words) > 0 else 0\n",
    "\n",
    "objective_words = [\"reported\", \"measured\", \"confirmed\", \"analyzed\", \"observed\", \"recorded\",\\\n",
    "                   \"found\", \"documented\", \"verified\", \"tested\", \"studied\", \"calculated\", \"noted\",\\\n",
    "                   \"established\", \"evidence\", \"fact\", \"data\", \"statistics\", \"demonstrated\", \"shown\",\\\n",
    "                   \"results\", \"result\", \"evidence-based\", \"peer-reviewed\", \"sampled\", \"quantified\",\\\n",
    "                   \"evaluated\", \"experimented\", \"investigated\"]\n",
    "def objective_score(text):\n",
    "    words = text.lower().split()\n",
    "    objective_count = sum(1 for word in words if word in objective_words)\n",
    "    return objective_count / len(words) if len(words) > 0 else 0\n",
    "\n",
    "\n",
    "def count_numerics(text):\n",
    "    numerical_count = len(re.findall(r'\\d+', text)) \n",
    "    return numerical_count\n",
    "\n",
    "\n",
    "pipe_finnews = pipeline(\"text-classification\", model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
    "def sentiment(text):\n",
    "    result = pipe_finnews(text)\n",
    "    return result[0][\"label\"] \n",
    "\n",
    "    \n",
    "#add in source credibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e113df98-a3ba-4de0-bc7e-83fb21aba9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'headlines_initial_hueristic_exploration.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Apply these feature functions to the headlines to compare\n",
    "features = [\n",
    "    word_count, char_count, avg_word_length, all_capitalized_word_count,\n",
    "    punctuation_count, exclamation_count, question_mark_count, readability_score,\n",
    "    smog_index, count_quotation_pairs, variety_of_vocabularity,\n",
    "    count_sensational_words, count_absolute_words, count_factuality_words,\n",
    "    subjectivity_score, objective_score, count_numerics, sentiment,\n",
    "]\n",
    "\n",
    "def compute_features(headlines):\n",
    "    results = []\n",
    "    for headline in headlines:\n",
    "        data = {\n",
    "            \"headline\": headline,\n",
    "            \"word_count\": word_count(headline),\n",
    "            \"char_count\": char_count(headline),\n",
    "            \"avg_word_length\": avg_word_length(headline),\n",
    "            \"all_capitalized_word_count\": all_capitalized_word_count(headline),\n",
    "            \"punctuation_count\": punctuation_count(headline),\n",
    "            \"exclamation_count\": exclamation_count(headline),\n",
    "            \"question_mark_count\": question_mark_count(headline),\n",
    "            \"readability_score\": readability_score(headline),\n",
    "            \"smog_index\": smog_index(headline),\n",
    "            \"quotation_count\": count_quotation_pairs(headline),\n",
    "            \"vocab_variety\": variety_of_vocabularity(headline),\n",
    "            \"sensational_words_count\": count_sensational_words(headline),\n",
    "            \"absolute_words_count\": count_absolute_words(headline),\n",
    "            \"factuality_words_count\": count_factuality_words(headline),\n",
    "            \"subjectivity_score\": subjectivity_score(headline),\n",
    "            \"objective_score\": objective_score(headline),\n",
    "            \"count_numerics\": count_numerics(headline),\n",
    "            \"sentiment\": sentiment(headline),\n",
    "        }\n",
    "        results.append(data)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "true_features_df = compute_features(true_headlines)\n",
    "false_features_df = compute_features(false_headlines)\n",
    "\n",
    "true_features_df[\"label\"] = \"True\"\n",
    "false_features_df[\"label\"] = \"False\"\n",
    "\n",
    "combined_df = pd.concat([true_features_df, false_features_df], ignore_index=True)\n",
    "combined_df.to_csv(\"headlines_initial_hueristic_exploration.csv\")\n",
    "print(\"Data saved to 'headlines_initial_hueristic_exploration.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0fcfb20-7975-4269-8ce6-e476deef6b74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Feature Comparison:\n",
      "                             True_Avg   False_Avg  Difference\n",
      "exclamation_count            0.000000    0.100000         inf\n",
      "quotation_count              0.062500    0.500000    8.000000\n",
      "all_capitalized_word_count   0.250000    1.150000    4.600000\n",
      "punctuation_count            1.000000    3.200000    3.200000\n",
      "absolute_words_count         0.125000    0.350000    2.800000\n",
      "count_numerics               0.562500    0.300000    1.875000\n",
      "word_count                  12.875000   17.650000    1.370874\n",
      "char_count                  82.562500  110.450000    1.337774\n",
      "smog_index                   0.400000    0.485000    1.212500\n",
      "avg_word_length              5.543158    5.370848    1.032083\n",
      "readability_score           60.378750   61.347000    1.016036\n",
      "vocab_variety                0.962344    0.976598    1.014812\n",
      "question_mark_count          0.125000    0.000000    0.125000\n",
      "factuality_words_count       0.062500    0.000000    0.062500\n",
      "sensational_words_count      0.000000    0.000000    0.000000\n",
      "subjectivity_score           0.000000    0.000000    0.000000\n",
      "objective_score              0.000000    0.000000    0.000000\n",
      "Sentiment Comparison:\n",
      "           True_Percentages  False_Percentages\n",
      "sentiment                                     \n",
      "negative               12.5               20.0\n",
      "neutral                75.0               80.0\n",
      "positive               12.5                0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#compare the averages of the numeric features \n",
    "true_avg_features = true_features_df.mean(numeric_only=True).to_dict()\n",
    "false_avg_features = false_features_df.mean(numeric_only=True).to_dict()\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"True_Avg\": true_avg_features,\n",
    "    \"False_Avg\": false_avg_features\n",
    "})\n",
    "\n",
    "comparison_df['Difference'] = np.where(\n",
    "    comparison_df['False_Avg'] != 0,\n",
    "    np.maximum(comparison_df['True_Avg'] / comparison_df['False_Avg'], comparison_df['False_Avg'] / comparison_df['True_Avg']),\n",
    "    comparison_df['True_Avg']\n",
    ")\n",
    "print(\"Numeric Feature Comparison:\")\n",
    "sorted_df = comparison_df.sort_values(by='Difference', ascending=False)\n",
    "print(sorted_df)\n",
    "\n",
    "#---------------------------------------------\n",
    "#compare sentiment feature\n",
    "sentiment_counts_true = true_features_df['sentiment'].value_counts()\n",
    "sentiment_counts_false = false_features_df['sentiment'].value_counts()\n",
    "#sentiment_counts_true\n",
    "\n",
    "total_sentiments_true = sentiment_counts_true.sum()\n",
    "total_sentiments_false = sentiment_counts_false.sum()\n",
    "\n",
    "sentiment_percentages_true = (sentiment_counts_true / total_sentiments_true) * 100\n",
    "sentiment_percentages_false = (sentiment_counts_false / total_sentiments_false) * 100\n",
    "\n",
    "sentiment_comparison_df = pd.DataFrame({\n",
    "    \"True_Percentages\": sentiment_percentages_true,\n",
    "    \"False_Percentages\": sentiment_percentages_false\n",
    "}).fillna(0)  #NaN values become 0 for sentiments not present\n",
    "\n",
    "print(\"Sentiment Comparison:\")\n",
    "print(sentiment_comparison_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848f80a1-a68d-41ee-aa77-3b1d9547aea0",
   "metadata": {},
   "source": [
    "### Run the Heuristics on the European Soccer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e30794b-66c6-4268-9990-cbda5ebbe110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'Euro_tweets_initial_hueristic_exploration.csv'\n"
     ]
    }
   ],
   "source": [
    "real_euro_df = pd.read_csv('real_european.csv')\n",
    "fake_euro_df = pd.read_csv('fake_european.csv')\n",
    "\n",
    "real_euro_df[\"label\"] = \"True\"\n",
    "fake_euro_df[\"label\"] = \"False\"\n",
    "\n",
    "real_tweets= real_euro_df[\"tweet\"].dropna()\n",
    "fake_tweets= fake_euro_df[\"tweet\"].dropna()\n",
    "\n",
    "euro_true_features_df = compute_features(real_tweets)\n",
    "euro_false_features_df = compute_features(fake_tweets)\n",
    "\n",
    "euro_true_features_df[\"label\"] = \"True\"\n",
    "euro_false_features_df[\"label\"] = \"False\"\n",
    "\n",
    "euro_combined_df = pd.concat([euro_true_features_df, euro_false_features_df], ignore_index=True)\n",
    "euro_combined_df.to_csv(\"Euro_tweets_initial_hueristic_exploration.csv\")\n",
    "print(\"Data saved to 'Euro_tweets_initial_hueristic_exploration.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59268638-6dc4-4711-b252-c9a935994377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random sample\n",
    "df = pd.read_csv(\"Euro_tweets_initial_hueristic_exploration.csv\")\n",
    "\n",
    "true_samples = df[df[\"label\"] == True].sample(n=19000,random_state=42 )\n",
    "false_samples = df[df[\"label\"] == False].sample(n=19000,random_state=42)\n",
    "\n",
    "sampled_df = pd.concat([true_samples, false_samples])\n",
    "\n",
    "sampled_df.to_csv(\"Euro_tweets_initial_hueristic_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7898dad9-7344-4fe3-9bd1-17074c629158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                True_Avg     False_Avg  Difference\n",
      "smog_index                      0.123711      1.168768    9.447607\n",
      "factuality_words_count          0.000263      0.002158    8.200000\n",
      "exclamation_count               0.019211      0.121526    6.326027\n",
      "subjectivity_score              0.000373      0.001606    4.307706\n",
      "question_mark_count             0.032737      0.140263    4.284566\n",
      "count_numerics                  0.749579      0.242684    3.088701\n",
      "Unnamed: 0                  10903.316895  31865.267211    2.922530\n",
      "absolute_words_count            0.043737      0.113579    2.596871\n",
      "quotation_count                 0.065684      0.025842    2.541752\n",
      "sensational_words_count         0.002053      0.005105    2.487179\n",
      "objective_score                 0.001472      0.000825    1.784904\n",
      "readability_score              51.706206     65.815311    1.272871\n",
      "avg_word_length                 5.386473      4.805250    1.120956\n",
      "char_count                    167.898421    150.996842    1.111933\n",
      "punctuation_count               5.561158      5.086947    1.093221\n",
      "vocab_variety                   0.865081      0.876789    1.013535\n",
      "word_count                     26.967789     27.234421    1.009887\n",
      "label                           1.000000      0.000000    1.000000\n",
      "all_capitalized_word_count      0.000000      0.000000    0.000000\n",
      "Sentiment Comparison:\n",
      "           True_Percentages  False_Percentages\n",
      "sentiment                                     \n",
      "neutral           82.078947          88.768421\n",
      "positive          14.094737           6.994737\n",
      "negative           3.826316           4.236842\n"
     ]
    }
   ],
   "source": [
    "#compare the averages of the numeric features \n",
    "\n",
    "euro_true_avg_features = true_samples.mean(numeric_only=True).to_dict()\n",
    "euro_false_avg_features = false_samples.mean(numeric_only=True).to_dict()\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"True_Avg\": euro_true_avg_features,\n",
    "    \"False_Avg\": euro_false_avg_features\n",
    "})\n",
    "\n",
    "comparison_df['Difference'] = np.where(\n",
    "    comparison_df['False_Avg'] != 0,\n",
    "    np.maximum(comparison_df['True_Avg'] / comparison_df['False_Avg'], comparison_df['False_Avg'] / comparison_df['True_Avg']),\n",
    "    comparison_df['True_Avg']\n",
    ")\n",
    "sorted_df = comparison_df.sort_values(by='Difference', ascending=False)\n",
    "print(sorted_df)\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "#compare sentiment feature\n",
    "sentiment_counts_true = true_samples['sentiment'].value_counts()\n",
    "sentiment_counts_false = false_samples['sentiment'].value_counts()\n",
    "#sentiment_counts_true\n",
    "\n",
    "total_sentiments_true = sentiment_counts_true.sum()\n",
    "total_sentiments_false = sentiment_counts_false.sum()\n",
    "\n",
    "sentiment_percentages_true = (sentiment_counts_true / total_sentiments_true) * 100\n",
    "sentiment_percentages_false = (sentiment_counts_false / total_sentiments_false) * 100\n",
    "\n",
    "sentiment_comparison_df = pd.DataFrame({\n",
    "    \"True_Percentages\": sentiment_percentages_true,\n",
    "    \"False_Percentages\": sentiment_percentages_false\n",
    "}).fillna(0)  #NaN values become 0 for sentiments not present\n",
    "\n",
    "print(\"Sentiment Comparison:\")\n",
    "print(sentiment_comparison_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb00339-a6a2-4fab-bae7-7a63392dbc68",
   "metadata": {},
   "source": [
    "### Create heuristic for labeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9eead4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_credibility_score(headline, source_credibility=None):\n",
    "    # Source credibility weights\n",
    "    SOURCE_CREDIBILITY = {\n",
    "        'high': 1.0,\n",
    "        'unknown': 0,\n",
    "        'medium': -0.5,\n",
    "        'low': -1.0\n",
    "    }\n",
    "    \n",
    "    # Top heuristics and their weights (based on the difference column)\n",
    "    heuristics = {\n",
    "        'smog_index': {\n",
    "            'weight': 9.448,  # From the Difference column\n",
    "            'true_avg': 0.124, # From the True_Avg column\n",
    "            'false_avg': 1.169 # From the False_Avg column\n",
    "        },\n",
    "        'factuality_words_count': {\n",
    "            'weight': 8.200,  \n",
    "            'true_avg': 0.000263,\n",
    "            'false_avg': 0.002158\n",
    "        },\n",
    "        'exclamation_count': {\n",
    "            'weight': 6.326, \n",
    "            'true_avg': 0.0192,\n",
    "            'false_avg': 0.1215\n",
    "        },\n",
    "        'subjectivity_score': {\n",
    "            'weight': 4.308,  \n",
    "            'true_avg': 0.000373,\n",
    "            'false_avg': 0.001606\n",
    "        },\n",
    "        'question_mark_count': {\n",
    "            'weight': 4.285,  \n",
    "            'true_avg': 0.0327,\n",
    "            'false_avg': 0.1403\n",
    "        },\n",
    "        'count_numerics': {\n",
    "            'weight': 3.089,  \n",
    "            'true_avg': 0.7496,\n",
    "            'false_avg': 0.2427\n",
    "        }\n",
    "    }   \n",
    "    \n",
    "    # Calculate metrics for each of the heuristics we're using\n",
    "    metrics = {\n",
    "        'smog_index': smog_index(headline),\n",
    "        'factuality_words_count': count_factuality_words(headline),\n",
    "        'exclamation_count': exclamation_count(headline),\n",
    "        'subjectivity_score': subjectivity_score(headline),\n",
    "        'question_mark_count': question_mark_count(headline),\n",
    "        'count_numerics': count_numerics(headline)\n",
    "    }\n",
    "\n",
    "    # Initialize the credibility score\n",
    "    credibility_score = 0\n",
    "    \n",
    "    # Calculate scores for each heuristic\n",
    "    for metric, values in heuristics.items():\n",
    "        true_avg = values['true_avg']\n",
    "        false_avg = values['false_avg']\n",
    "        weight = values['weight']\n",
    "        current_value = metrics[metric]\n",
    "        \n",
    "        # Calculate which average the current value is closer to\n",
    "        dist_to_true = abs(current_value - true_avg)\n",
    "        dist_to_false = abs(current_value - false_avg)\n",
    "        \n",
    "        # Calculate percentage difference from the midpoint\n",
    "        midpoint = (true_avg + false_avg) / 2            \n",
    "        percent_diff = abs(current_value - midpoint) / midpoint\n",
    "        \n",
    "        # Add or subtract from score based on which average it's closer to\n",
    "        if dist_to_true < dist_to_false:\n",
    "            credibility_score += weight * percent_diff\n",
    "        else:\n",
    "            credibility_score -= weight * percent_diff\n",
    "        \n",
    "        # print(f\"{metric}: {current_value:.4f}\\n\\tWeight: {weight:.2f}\\n\\tPercent Difference: {percent_diff:.4f}\\n\\tCredibility Score: {credibility_score:.2f}\")\n",
    "    \n",
    "    # Add source credibility modifier\n",
    "    if source_credibility:\n",
    "        source_credibility_score = SOURCE_CREDIBILITY.get(source_credibility.lower(), SOURCE_CREDIBILITY['unknown'])\n",
    "        weight = 10\n",
    "        credibility_score += (source_credibility_score * weight)\n",
    "        # print(f\"source_credibility: {source_credibility_score:.2f}\\n\\tWeight: {weight:.2f}\\n\\tCredibility Score: {credibility_score:.2f}\")\n",
    "\n",
    "    return credibility_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ec3a793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Credible Headline:  BREAKING: Major upset in championship game! \n",
      "\n",
      "Credibility Score (credible source): -50.44\n",
      "\n",
      "Credibility Score (unknown source): -60.44\n",
      "\n",
      "Credibility Score (medium credibility): -65.44\n",
      "\n",
      "Credible Headline:  NBA All-Star Joel Embiid to undergo knee surgery, expected to miss 6-8 weeks \n",
      "\n",
      "Credibility Score (credible source): 51.93\n",
      "\n",
      "Credibility Score (unknown source): 41.93\n",
      "\n",
      "Credibility Score (medium credibility): 36.93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Case - Non-Credible\n",
    "headline_noncredible = \"BREAKING: Major upset in championship game!\"\n",
    "\n",
    "print(\"Non-Credible Headline: \", headline_noncredible, \"\\n\")\n",
    "\n",
    "# With a credible source\n",
    "score1 = calculate_credibility_score(headline_noncredible, source_credibility=\"high\")\n",
    "print(f\"Credibility Score (credible source): {score1:.2f}\\n\")\n",
    "\n",
    "# With an unknown source\n",
    "score2 = calculate_credibility_score(headline_noncredible, source_credibility=\"unknown\")\n",
    "print(f\"Credibility Score (unknown source): {score2:.2f}\\n\")\n",
    "\n",
    "# With a medium credibility source\n",
    "score3 = calculate_credibility_score(headline_noncredible, source_credibility=\"medium\")\n",
    "print(f\"Credibility Score (medium credibility): {score3:.2f}\\n\")\n",
    "\n",
    "\n",
    "# Test Case - Credible\n",
    "headline_credible = \"NBA All-Star Joel Embiid to undergo knee surgery, expected to miss 6-8 weeks\"\n",
    "\n",
    "print(\"Credible Headline: \", headline_credible, \"\\n\")\n",
    "\n",
    "# With a credible source\n",
    "score1 = calculate_credibility_score(headline_credible, source_credibility=\"high\")\n",
    "print(f\"Credibility Score (credible source): {score1:.2f}\\n\")\n",
    "\n",
    "# With an unknown source\n",
    "score2 = calculate_credibility_score(headline_credible, source_credibility=\"unknown\")\n",
    "print(f\"Credibility Score (unknown source): {score2:.2f}\\n\")\n",
    "\n",
    "# With a medium credibility source\n",
    "score3 = calculate_credibility_score(headline_credible, source_credibility=\"medium\")\n",
    "print(f\"Credibility Score (medium credibility): {score3:.2f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
